# encoding=GBK


"""
Q-Learning
这里简化了算法： Q(s,a) = r(s,a) + gama*(max(Q(s1,a1))
. s: 当前的状态
. a: 当前状态下要采取的动作
. r:　在给定状态下采取给定行动能够得到的奖励
. s1:　当前动作后要到达的新状态
. a1:　在新状态下要采取的动作
. max(Q(s1,a1)): 找出在s1状态下所有动作ａ中最大Q的，认为应该采用这样的动作a1
. gama:　折扣因素。表示牺牲当前收益，换取长远收益的程度
"""

#               房型图
##################################################
#
#    -------------+------------+
#   |             |                  5
#   |     0       |      1     +------------+
#   |             |            |            |
#   +-------+   +-+--+  +------+            |
#   |             |            |      2     |
#   |     4       |      3                  |
#   |                          |            |
#   |             |            |            |
#   +--+   +------+------------+------------+
#         5
#
#              　状态／行动图
##################################################
#                    Action
#     state    0   1   2   3   4   5
#       0    |                 0      |
#       1    |             0      100 |
#   R = 2    |             0          |
#       3    |     0   0       0      |
#       4    | 0           0      100 |
#       5    |     0           0  100 |
#
##################################################


import numpy as np

q = np.zeros((6, 6))            # 这就是要求的ｑ矩阵
rewards = np.zeros((6, 6))      # 回报矩阵
rewards[:, 5] = 500             # 最终到房间 5 的回报最高
actions = [[4],                 # 房间 0 只能进入房间 4
           [3, 5],              # 房间 1 只能进入房间 3, 5
           [3],                 # 房间 2 只能进入房间 3
           [1, 2, 4],           # 房间 3 只能进入房间 1, 2, 4
           [0, 3, 5],           # 房间 4 只能进入房间 0, 3, 5
           [1, 4, 5]]           # 房间 5 只能进入房间 1, 4, 5
gama = 0.8                      # 折现系数为0.8
EPOCH = 100


def train():                                            # 训练模型
    s = np.random.randint(6)                            # 开始时候随机给出一个房间号

    while s < 5:
        a = np.random.choice(actions[s])                # 从对应的房间号里随机选一个动作 a
        s1 = a                                          # 要去的房间就是下一个状态 s1
        r = rewards[s, a]                               # 得到状态 s 下的动作 a 将会得到 的回报 r
        q[s, a] = r + gama * q[s1].max()                # q[s,a] = 在s状态下回报＋通过a到达s1状态后的最大的q的那个动作
        s = s1                                          # 状态转移到了s1


def test(s):
    print(q)
    print("=> {}".format(s))
    while s < 5:
        s = q[s].argmax()                               # 核心就是根据Q表来决定下一次的行动
        print("=> {}".format(s))

for _ in range(EPOCH):         # 迭代训练
    train()

test(2)                 # 看看测试的路径结果
