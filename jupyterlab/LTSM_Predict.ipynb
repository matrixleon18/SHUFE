{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12c3277-a234-4007-8bf4-cb7ea2560565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 123)\n"
     ]
    }
   ],
   "source": [
    "# GPU -- 准备来做一个GPU版本的RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date','prediction'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset['future'] = dataset['future']\n",
    "print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "# 将需要预测的数据\"future\"缩放在[-1,1]之间\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# y_scaled = scaler.fit_transform(np.array(dataset['future']).reshape(-1,1))\n",
    "# dataset['future'] = y_scaled\n",
    "# y_rev = scaler.inverse_transform(np.array(dataset['future']).reshape(-1,1))\n",
    "# print(y_rev)\n",
    "\n",
    "# 这样写更快.（如果第0列就是 future的话）\n",
    "# dataset['future'] = scaler.fit_transform(dataset)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a953d8a-5f64-4bb2-8b9f-a2e50fb34193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling_data shape: (209, 42, 123)\n",
      "seq count: 209\n",
      "seq length: 42\n",
      "total batch count: 209\n",
      "batch size: 1\n",
      "rolling_data: torch.Size([209, 1, 42, 123])\n",
      "train_x: torch.Size([208, 1, 42, 122])\n",
      "train_y: torch.Size([208, 1, 42, 1])\n",
      "test_x:  torch.Size([1, 1, 42, 122])\n",
      "test_y:  torch.Size([1, 1, 42, 1])\n",
      "train_batch_count: 208\n",
      "test_batch_count:  1\n"
     ]
    }
   ],
   "source": [
    "# 方法2，将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 42\n",
    "BATCH_SIZE = 1                                                    # 注意：BATCH_SIZE是要能够整除seq_count的\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, dataset.shape[1])                 # 数据一共是 seq_count x seq_len x (in_dim+1)\n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(SEQ_LENGTH))\n",
    "\n",
    "\n",
    "total_batch_count = int(rolling_data.shape[0]/BATCH_SIZE)                                   # 把数据规划成 batch_count 个 batch\n",
    "\n",
    "print(\"total batch count: {}\".format(total_batch_count))\n",
    "print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "rolling_data = rolling_data.reshape(total_batch_count, BATCH_SIZE, SEQ_LENGTH, dataset.shape[1])  # 把数据转成 total_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "rolling_data = torch.tensor(rolling_data)\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "\n",
    "\n",
    "train_batch_count = total_batch_count - 1\n",
    "test_batch_count = total_batch_count - train_batch_count\n",
    "\n",
    "train = rolling_data[:train_batch_count, :, :, :]\n",
    "test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,1:], train[:,:,:,0:1]\n",
    "test_x,  test_y  = test[:,:,:, 1:],  test[:,:,:,0:1]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train_batch_count))\n",
    "print(\"test_batch_count:  {}\".format(test_batch_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3033e44d-4dfa-4a7f-8fa6-b96783b5ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始定义 LSTM 模型\n",
    "TIME_STEP = SEQ_LENGTH                                        # 一般这个单独设定，这里为了简单，还是直接就等于seq_len的方便。其实也就是等于最长的那个sequence length\n",
    "INPUT_SIZE = dataset.shape[1]-1\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "np.random.seed(3)\n",
    "torch.manual_seed(3)\n",
    "torch.cuda.manual_seed(3)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(input_size=hidden_layer_size, hidden_size=hidden_layer_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.h0 = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "        self.c0 = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "        \n",
    "        self.init_weights3()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "\n",
    "    def init_weights2(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)    \n",
    "    \n",
    "    def init_weights3(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "                \n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # layer 1\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 去掉以后更拟合，但没有验证泛化能力如何\n",
    "        # x = self.relu(x)\n",
    "        \n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch        \n",
    "        # LSTM layer\n",
    "        # lstm_out, (h_n, c_n) = self.lstm(x, (self.h0.detach(), self.c0.detach()))\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, (self.h0, self.c0))\n",
    "\n",
    "        # lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        predictions = self.linear_2(lstm_out)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "927d866d-1743-432b-bcab-79879da807ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 of 1000 epoch loss: 1.024512\n",
      "learning rate: 1e-06\n",
      "19 of 1000 epoch loss: 0.996447\n",
      "learning rate: 1e-06\n",
      "29 of 1000 epoch loss: 0.984793\n",
      "learning rate: 1e-06\n",
      "39 of 1000 epoch loss: 0.969996\n",
      "learning rate: 1e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(pred, train_y[step])\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, norm_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\python38\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "torch.manual_seed(3)\n",
    "torch.cuda.manual_seed(3)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# 初始化 LSTM 模型\n",
    "model = LSTMModel(input_size=INPUT_SIZE, hidden_layer_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1)\n",
    "model = model.to(device)\n",
    "LR = 1e-6\n",
    "loss_func = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-3)\n",
    "\n",
    "\n",
    "# 训练 LSTM 模型; \n",
    "epoches = 1000\n",
    "epoch_loss = 0\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        # print(train_x[i])\n",
    "        # print(train_x[i].shape)\n",
    "        model = model.double()\n",
    "        pred = model(train_x[step])\n",
    "        loss = loss_func(pred, train_y[step])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data\n",
    "        \n",
    "    # print(loss.item())\n",
    "    \n",
    "    if loss.item() < 1e-4:\n",
    "        print('Epoch [{}/{}], Loss: {:.5f}'.format(epoch+1, epoches, loss.item()))\n",
    "        print(\"The loss value is reached\")\n",
    "        break\n",
    "    elif (epoch+1)%10 == 0:\n",
    "        print(\"{} of {} epoch loss: {:.6f}\".format(epoch, epoches, epoch_loss.item()))\n",
    "        print(\"learning rate: {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        # for p in optimizer.param_groups:\n",
    "            # p['lr'] *= 0.95\n",
    "    \n",
    "    epoch_loss = 0\n",
    "\n",
    "plt.plot(epoch_loss_list)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c299f19a-459b-4ee0-9290-063b98fdaad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(test_x[0])\n",
    "\n",
    "test_y=test_y.cpu()\n",
    "pred = pred.cpu()\n",
    "\n",
    "loss = loss_func(pred, test_y[0])\n",
    "print(\"loss: {:.6f}\".format(loss.item()))\n",
    "\n",
    "# print(scaler.inverse_transform(pred))\n",
    "\n",
    "print(\"预测明天涨跌: {}\".format(pred[-1][-1].data*100))\n",
    "\n",
    "# actual_line = np.array(test_y[0]).flatten().tolist()\n",
    "# pred_line = pred.data.numpy().flatten().tolist()\n",
    "\n",
    "scaledl_line = np.array(test_y[0][-1]).flatten().tolist()\n",
    "pred_line = pred[-1].data.numpy().flatten().tolist()\n",
    "\n",
    "plt.plot(scaled_line, 'r--')\n",
    "plt.plot(pred_line, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f368c8-a350-4c17-89b8-646fe710ac2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
