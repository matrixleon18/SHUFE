{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df055ebf-71d4-4fdd-95a3-b391fdfc40bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 有两层 LSTM 的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39540de0-f3f7-49d4-85fc-8e5b795f8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b46b98e-ca92-437c-baf0-ff836e490e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1027)\n",
    "torch.manual_seed(1027)\n",
    "torch.cuda.manual_seed(1027)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ede11b1-7293-4e58-a48c-d5ccc006a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "dataset = dataset.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4fad15-01d6-4065-ab36-aa952f39cef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling_data shape: (601, 60, 139)\n",
      "seq count: 601\n",
      "seq length: 60\n",
      "train_x: torch.Size([6, 100, 60, 138])\n",
      "train_y: torch.Size([6, 100, 1, 1])\n",
      "test_x:  torch.Size([1, 1, 60, 138])\n",
      "test_y:  torch.Size([1, 1, 1, 1])\n",
      "train_batch_count: 6\n",
      "test_batch_count:  1\n"
     ]
    }
   ],
   "source": [
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 60\n",
    "TRAIN_BATCH_SIZE = 100                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_SIZE = 1                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_COUNT = 1\n",
    "Y_SEQ_LEN = 1                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "Y_DIM = 1\n",
    "X_DIM = dataset.shape[1]-Y_SEQ_LEN                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                   # 数据一共是 seq_count x seq_len x (x_in_dim+Y_SEQ_LEN) \n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(SEQ_LENGTH))\n",
    "# print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "test_seq_count = TEST_BATCH_COUNT * TEST_BATCH_SIZE\n",
    "\n",
    "\n",
    "# train = rolling_data[:-test_seq_count].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "# test  = rolling_data[-test_seq_count:].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "train = rolling_data[:-test_seq_count].reshape(-1, TRAIN_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                    # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "test  = rolling_data[-test_seq_count:].reshape(-1, TEST_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)      # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "TRAIN_BATCH_SIZE = train.shape[1]\n",
    "TRAIN_BATCH_COUNT = train.shape[0]\n",
    "TEST_BATCH_SIZE = test.shape[1]\n",
    "TEST_BATCH_COUNT = test.shape[0]\n",
    "\n",
    "train = torch.tensor(train)\n",
    "test  = torch.tensor(test)\n",
    "\n",
    "# train = rolling_data[:train_batch_count, :, :, :]\n",
    "# test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_SEQ_LEN:], train[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_SEQ_LEN:],  test[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y_seq_len]  to  [test_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train.shape[0]))\n",
    "print(\"test_batch_count:  {}\".format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f5b7bce-a6af-4fc4-bada-7dd20985f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 2-lyaers LSTM class\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=input_size,        hidden_size=hidden_layer_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_layer_size, hidden_size=hidden_layer_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "\n",
    "        self.linear_1 = nn.Linear(hidden_layer_size, int(hidden_layer_size/4))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(int(hidden_layer_size/4), output_size)\n",
    "\n",
    "        # self.h10 = torch.zeros(NUM_LAYERS, BATCH_SIZE, int(hidden_layer_size/2)).double().to(device)\n",
    "        # self.c10 = torch.zeros(NUM_LAYERS, BATCH_SIZE, int(hidden_layer_size/2)).double().to(device)\n",
    "        # self.h20 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size).double().to(device)\n",
    "        # self.c20 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size).double().to(device)\n",
    "        \n",
    "        self.init_weights2()\n",
    "\n",
    "    def init_weights1(self):\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "        for name, param in self.lstm2.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "    def init_weights2(self):\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)    \n",
    "        for name, param in self.lstm2.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)    \n",
    "                \n",
    "    def init_weights3(self):\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "        for name, param in self.lstm2.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "    def forward(self, x, hidden, cell):\n",
    "\n",
    "        # layer 1\n",
    "        # x = self.linear_1(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch        \n",
    "        # LSTM layer\n",
    "        # lstm_out, (h_n, c_n) = self.lstm(x, (self.h0.detach(), self.c0.detach()))\n",
    "        \n",
    "        lstm1_out, (h1_n, c1_n) = self.lstm1(x, (hidden, cell))\n",
    "        \n",
    "        lstm1_out = self.dropout(lstm1_out)\n",
    "        \n",
    "        lstm_out, (h2_n, c2_n) = self.lstm2(lstm1_out, (h1_n, c1_n))\n",
    "\n",
    "        # lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        Linear_out = self.linear_1(lstm_out)\n",
    "        self.relu(Linear_out)\n",
    "        predictions = self.linear_2(Linear_out)\n",
    "        \n",
    "        return predictions, h2_n, c2_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33bcc6e-17bd-45ef-a37c-0c4fc5266e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "model = LSTMModel(input_size=X_DIM, hidden_layer_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf7392c7-42e8-4345-83a5-1c392716f26e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 200 epoch loss: 0.1145 with lr: 0.0001\n",
      "1 of 200 epoch loss: 0.0907 with lr: 0.0001\n",
      "2 of 200 epoch loss: 0.0672 with lr: 0.0001\n",
      "3 of 200 epoch loss: 0.0581 with lr: 0.0001\n",
      "4 of 200 epoch loss: 0.0487 with lr: 0.0001\n",
      "5 of 200 epoch loss: 0.0491 with lr: 0.0001\n",
      "6 of 200 epoch loss: 0.0457 with lr: 0.0001\n",
      "7 of 200 epoch loss: 0.0458 with lr: 0.0001\n",
      "8 of 200 epoch loss: 0.0449 with lr: 0.0001\n",
      "9 of 200 epoch loss: 0.0436 with lr: 0.0001\n",
      "10 of 200 epoch loss: 0.0431 with lr: 0.0001\n",
      "11 of 200 epoch loss: 0.0427 with lr: 0.0001\n",
      "12 of 200 epoch loss: 0.0426 with lr: 0.0001\n",
      "13 of 200 epoch loss: 0.0416 with lr: 0.0001\n",
      "14 of 200 epoch loss: 0.0413 with lr: 0.0001\n",
      "15 of 200 epoch loss: 0.0417 with lr: 0.0001\n",
      "16 of 200 epoch loss: 0.0406 with lr: 0.0001\n",
      "17 of 200 epoch loss: 0.0403 with lr: 0.0001\n",
      "18 of 200 epoch loss: 0.0392 with lr: 0.0001\n",
      "19 of 200 epoch loss: 0.0401 with lr: 0.0001\n",
      "20 of 200 epoch loss: 0.0391 with lr: 0.0001\n",
      "21 of 200 epoch loss: 0.0401 with lr: 0.0001\n",
      "22 of 200 epoch loss: 0.0388 with lr: 0.0001\n",
      "23 of 200 epoch loss: 0.0397 with lr: 0.0001\n",
      "24 of 200 epoch loss: 0.0397 with lr: 0.0001\n",
      "25 of 200 epoch loss: 0.0392 with lr: 0.0001\n",
      "26 of 200 epoch loss: 0.0412 with lr: 0.0001\n",
      "27 of 200 epoch loss: 0.0455 with lr: 0.0001\n",
      "28 of 200 epoch loss: 0.0406 with lr: 0.0001\n",
      "29 of 200 epoch loss: 0.0378 with lr: 0.0001\n",
      "30 of 200 epoch loss: 0.0385 with lr: 0.0001\n",
      "31 of 200 epoch loss: 0.0373 with lr: 0.0001\n",
      "32 of 200 epoch loss: 0.0379 with lr: 0.0001\n",
      "33 of 200 epoch loss: 0.0362 with lr: 0.0001\n",
      "34 of 200 epoch loss: 0.0367 with lr: 0.0001\n",
      "35 of 200 epoch loss: 0.0370 with lr: 0.0001\n",
      "36 of 200 epoch loss: 0.0364 with lr: 0.0001\n",
      "37 of 200 epoch loss: 0.0349 with lr: 0.0001\n",
      "38 of 200 epoch loss: 0.0348 with lr: 0.0001\n",
      "39 of 200 epoch loss: 0.0360 with lr: 0.0001\n",
      "40 of 200 epoch loss: 0.0365 with lr: 0.0001\n",
      "41 of 200 epoch loss: 0.0354 with lr: 0.0001\n",
      "42 of 200 epoch loss: 0.0361 with lr: 0.0001\n",
      "43 of 200 epoch loss: 0.0360 with lr: 0.0001\n",
      "44 of 200 epoch loss: 0.0351 with lr: 0.0001\n",
      "45 of 200 epoch loss: 0.0359 with lr: 0.0001\n",
      "46 of 200 epoch loss: 0.0344 with lr: 0.0001\n",
      "47 of 200 epoch loss: 0.0341 with lr: 0.0001\n",
      "48 of 200 epoch loss: 0.0339 with lr: 0.0001\n",
      "49 of 200 epoch loss: 0.0332 with lr: 0.0001\n",
      "50 of 200 epoch loss: 0.0335 with lr: 0.0001\n",
      "51 of 200 epoch loss: 0.0354 with lr: 0.0001\n",
      "52 of 200 epoch loss: 0.0376 with lr: 0.0001\n",
      "53 of 200 epoch loss: 0.0347 with lr: 0.0001\n",
      "54 of 200 epoch loss: 0.0338 with lr: 0.0001\n",
      "55 of 200 epoch loss: 0.0317 with lr: 0.0001\n",
      "56 of 200 epoch loss: 0.0348 with lr: 0.0001\n",
      "57 of 200 epoch loss: 0.0315 with lr: 0.0001\n",
      "58 of 200 epoch loss: 0.0316 with lr: 0.0001\n",
      "59 of 200 epoch loss: 0.0306 with lr: 0.0001\n",
      "60 of 200 epoch loss: 0.0304 with lr: 0.0001\n",
      "61 of 200 epoch loss: 0.0312 with lr: 0.0001\n",
      "62 of 200 epoch loss: 0.0317 with lr: 0.0001\n",
      "63 of 200 epoch loss: 0.0307 with lr: 0.0001\n",
      "64 of 200 epoch loss: 0.0299 with lr: 0.0001\n",
      "65 of 200 epoch loss: 0.0339 with lr: 0.0001\n",
      "66 of 200 epoch loss: 0.0331 with lr: 0.0001\n",
      "67 of 200 epoch loss: 0.0327 with lr: 0.0001\n",
      "68 of 200 epoch loss: 0.0297 with lr: 0.0001\n",
      "69 of 200 epoch loss: 0.0291 with lr: 0.0001\n",
      "70 of 200 epoch loss: 0.0307 with lr: 0.0001\n",
      "71 of 200 epoch loss: 0.0306 with lr: 0.0001\n",
      "72 of 200 epoch loss: 0.0303 with lr: 0.0001\n",
      "73 of 200 epoch loss: 0.0317 with lr: 0.0001\n",
      "74 of 200 epoch loss: 0.0313 with lr: 0.0001\n",
      "75 of 200 epoch loss: 0.0304 with lr: 0.0001\n",
      "76 of 200 epoch loss: 0.0286 with lr: 0.0001\n",
      "77 of 200 epoch loss: 0.0280 with lr: 0.0001\n",
      "78 of 200 epoch loss: 0.0283 with lr: 0.0001\n",
      "79 of 200 epoch loss: 0.0284 with lr: 0.0001\n",
      "80 of 200 epoch loss: 0.0304 with lr: 0.0001\n",
      "81 of 200 epoch loss: 0.0295 with lr: 0.0001\n",
      "82 of 200 epoch loss: 0.0315 with lr: 0.0001\n",
      "83 of 200 epoch loss: 0.0290 with lr: 0.0001\n",
      "84 of 200 epoch loss: 0.0273 with lr: 0.0001\n",
      "85 of 200 epoch loss: 0.0285 with lr: 0.0001\n",
      "86 of 200 epoch loss: 0.0271 with lr: 0.0001\n",
      "87 of 200 epoch loss: 0.0267 with lr: 0.0001\n",
      "88 of 200 epoch loss: 0.0260 with lr: 0.0001\n",
      "89 of 200 epoch loss: 0.0247 with lr: 0.0001\n",
      "90 of 200 epoch loss: 0.0266 with lr: 0.0001\n",
      "91 of 200 epoch loss: 0.0272 with lr: 0.0001\n",
      "92 of 200 epoch loss: 0.0265 with lr: 0.0001\n",
      "93 of 200 epoch loss: 0.0281 with lr: 0.0001\n",
      "94 of 200 epoch loss: 0.0280 with lr: 0.0001\n",
      "95 of 200 epoch loss: 0.0295 with lr: 0.0001\n",
      "96 of 200 epoch loss: 0.0283 with lr: 0.0001\n",
      "97 of 200 epoch loss: 0.0265 with lr: 0.0001\n",
      "98 of 200 epoch loss: 0.0256 with lr: 0.0001\n",
      "99 of 200 epoch loss: 0.0276 with lr: 0.0001\n",
      "100 of 200 epoch loss: 0.0246 with lr: 0.0001\n",
      "101 of 200 epoch loss: 0.0252 with lr: 0.0001\n",
      "102 of 200 epoch loss: 0.0255 with lr: 0.0001\n",
      "103 of 200 epoch loss: 0.0247 with lr: 0.0001\n",
      "104 of 200 epoch loss: 0.0257 with lr: 0.0001\n",
      "105 of 200 epoch loss: 0.0281 with lr: 0.0001\n",
      "106 of 200 epoch loss: 0.0268 with lr: 0.0001\n",
      "107 of 200 epoch loss: 0.0247 with lr: 0.0001\n",
      "108 of 200 epoch loss: 0.0237 with lr: 0.0001\n",
      "109 of 200 epoch loss: 0.0244 with lr: 0.0001\n",
      "110 of 200 epoch loss: 0.0257 with lr: 0.0001\n",
      "111 of 200 epoch loss: 0.0243 with lr: 0.0001\n",
      "112 of 200 epoch loss: 0.0250 with lr: 0.0001\n",
      "113 of 200 epoch loss: 0.0256 with lr: 0.0001\n",
      "114 of 200 epoch loss: 0.0255 with lr: 0.0001\n",
      "115 of 200 epoch loss: 0.0242 with lr: 0.0001\n",
      "116 of 200 epoch loss: 0.0251 with lr: 0.0001\n",
      "117 of 200 epoch loss: 0.0266 with lr: 0.0001\n",
      "118 of 200 epoch loss: 0.0256 with lr: 0.0001\n",
      "119 of 200 epoch loss: 0.0272 with lr: 0.0001\n",
      "120 of 200 epoch loss: 0.0236 with lr: 0.0001\n",
      "121 of 200 epoch loss: 0.0237 with lr: 0.0001\n",
      "122 of 200 epoch loss: 0.0233 with lr: 0.0001\n",
      "123 of 200 epoch loss: 0.0232 with lr: 0.0001\n",
      "124 of 200 epoch loss: 0.0231 with lr: 0.0001\n",
      "125 of 200 epoch loss: 0.0236 with lr: 0.0001\n",
      "126 of 200 epoch loss: 0.0232 with lr: 0.0001\n",
      "127 of 200 epoch loss: 0.0222 with lr: 0.0001\n",
      "128 of 200 epoch loss: 0.0226 with lr: 0.0001\n",
      "129 of 200 epoch loss: 0.0209 with lr: 0.0001\n",
      "130 of 200 epoch loss: 0.0232 with lr: 0.0001\n",
      "131 of 200 epoch loss: 0.0230 with lr: 0.0001\n",
      "132 of 200 epoch loss: 0.0225 with lr: 0.0001\n",
      "133 of 200 epoch loss: 0.0227 with lr: 0.0001\n",
      "134 of 200 epoch loss: 0.0209 with lr: 0.0001\n",
      "135 of 200 epoch loss: 0.0240 with lr: 0.0001\n",
      "136 of 200 epoch loss: 0.0253 with lr: 0.0001\n",
      "137 of 200 epoch loss: 0.0248 with lr: 0.0001\n",
      "138 of 200 epoch loss: 0.0239 with lr: 0.0001\n",
      "139 of 200 epoch loss: 0.0224 with lr: 0.0001\n",
      "140 of 200 epoch loss: 0.0230 with lr: 0.0001\n",
      "141 of 200 epoch loss: 0.0202 with lr: 0.0001\n",
      "142 of 200 epoch loss: 0.0207 with lr: 0.0001\n",
      "143 of 200 epoch loss: 0.0204 with lr: 0.0001\n",
      "144 of 200 epoch loss: 0.0213 with lr: 0.0001\n",
      "145 of 200 epoch loss: 0.0218 with lr: 0.0001\n",
      "146 of 200 epoch loss: 0.0233 with lr: 0.0001\n",
      "147 of 200 epoch loss: 0.0224 with lr: 0.0001\n",
      "148 of 200 epoch loss: 0.0240 with lr: 0.0001\n",
      "149 of 200 epoch loss: 0.0217 with lr: 0.0001\n",
      "150 of 200 epoch loss: 0.0208 with lr: 0.0001\n",
      "151 of 200 epoch loss: 0.0216 with lr: 0.0001\n",
      "152 of 200 epoch loss: 0.0205 with lr: 0.0001\n",
      "153 of 200 epoch loss: 0.0195 with lr: 0.0001\n",
      "154 of 200 epoch loss: 0.0191 with lr: 0.0001\n",
      "155 of 200 epoch loss: 0.0202 with lr: 0.0001\n",
      "156 of 200 epoch loss: 0.0205 with lr: 0.0001\n",
      "157 of 200 epoch loss: 0.0207 with lr: 0.0001\n",
      "158 of 200 epoch loss: 0.0215 with lr: 0.0001\n",
      "159 of 200 epoch loss: 0.0195 with lr: 0.0001\n",
      "160 of 200 epoch loss: 0.0197 with lr: 0.0001\n",
      "161 of 200 epoch loss: 0.0203 with lr: 0.0001\n",
      "162 of 200 epoch loss: 0.0191 with lr: 0.0001\n",
      "163 of 200 epoch loss: 0.0203 with lr: 0.0001\n",
      "164 of 200 epoch loss: 0.0193 with lr: 0.0001\n",
      "165 of 200 epoch loss: 0.0186 with lr: 0.0001\n",
      "166 of 200 epoch loss: 0.0197 with lr: 0.0001\n",
      "167 of 200 epoch loss: 0.0218 with lr: 0.0001\n",
      "168 of 200 epoch loss: 0.0209 with lr: 0.0001\n",
      "169 of 200 epoch loss: 0.0217 with lr: 0.0001\n",
      "170 of 200 epoch loss: 0.0187 with lr: 0.0001\n",
      "171 of 200 epoch loss: 0.0189 with lr: 0.0001\n",
      "172 of 200 epoch loss: 0.0200 with lr: 0.0001\n",
      "173 of 200 epoch loss: 0.0200 with lr: 0.0001\n",
      "174 of 200 epoch loss: 0.0172 with lr: 0.0001\n",
      "175 of 200 epoch loss: 0.0196 with lr: 0.0001\n",
      "176 of 200 epoch loss: 0.0193 with lr: 0.0001\n",
      "177 of 200 epoch loss: 0.0206 with lr: 0.0001\n",
      "178 of 200 epoch loss: 0.0174 with lr: 0.0001\n",
      "179 of 200 epoch loss: 0.0215 with lr: 0.0001\n",
      "180 of 200 epoch loss: 0.0179 with lr: 0.0001\n",
      "181 of 200 epoch loss: 0.0173 with lr: 0.0001\n",
      "182 of 200 epoch loss: 0.0181 with lr: 0.0001\n",
      "183 of 200 epoch loss: 0.0196 with lr: 0.0001\n",
      "184 of 200 epoch loss: 0.0189 with lr: 0.0001\n",
      "185 of 200 epoch loss: 0.0183 with lr: 0.0001\n",
      "186 of 200 epoch loss: 0.0179 with lr: 0.0001\n",
      "187 of 200 epoch loss: 0.0191 with lr: 0.0001\n",
      "188 of 200 epoch loss: 0.0183 with lr: 0.0001\n",
      "189 of 200 epoch loss: 0.0162 with lr: 0.0001\n",
      "190 of 200 epoch loss: 0.0170 with lr: 0.0001\n",
      "191 of 200 epoch loss: 0.0167 with lr: 0.0001\n",
      "192 of 200 epoch loss: 0.0181 with lr: 0.0001\n",
      "193 of 200 epoch loss: 0.0163 with lr: 0.0001\n",
      "194 of 200 epoch loss: 0.0193 with lr: 0.0001\n",
      "195 of 200 epoch loss: 0.0151 with lr: 0.0001\n",
      "196 of 200 epoch loss: 0.0171 with lr: 0.0001\n",
      "197 of 200 epoch loss: 0.0169 with lr: 0.0001\n",
      "198 of 200 epoch loss: 0.0154 with lr: 0.0001\n",
      "199 of 200 epoch loss: 0.0160 with lr: 0.0001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练 LSTM 模型;  ---- 这里的损失函数是计算Sequence最后一个元素的预测数据和真实数据差异\n",
    "model.train()\n",
    "LR = 1e-4\n",
    "loss_func = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1, last_epoch=-1)\n",
    "\n",
    "epoches = 200\n",
    "epoch_loss = 0\n",
    "epoch_loss_list = []\n",
    "train_batch_count = train_x.shape[0]\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, TRAIN_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, TRAIN_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        pred, hn, cn = model(train_x[step], h0, c0)\n",
    "        h0, c0 = hn.detach(), cn.detach()\n",
    "        loss = loss_func(pred[:,-1], train_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data.cpu()\n",
    "        \n",
    "    if epoch_loss.item() < 1e-4:\n",
    "        print('Epoch [{}/{}], Loss: {:.5f}'.format(epoch+1, epoches, loss.item()))\n",
    "        print(\"The loss value is reached\")\n",
    "        break\n",
    "\n",
    "    print(\"{} of {} epoch loss: {:.4f} with lr: {}\".format(epoch, epoches, epoch_loss.item(), optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    if (epoch+1) % 2000 ==0:\n",
    "        scheduler.step()\n",
    "    # print(\"learning rate: {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    # for p in optimizer.param_groups:\n",
    "    #     p['lr'] *= 0.99\n",
    "    \n",
    "plt.plot(epoch_loss_list)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d50c0b44-f62f-4c56-90cf-0a55d1fb6d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model, 'e:\\\\Model_LSTM2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b617d8d5-f726-4a0a-9ad8-46f2af1e1a12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model = torch.load('e:\\\\Model_LSTM2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c7e16af-8034-49d5-a300-fc2bfd644947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Loss average:0.001182\n",
      "Prediction: -0.03\n",
      "Actual:     0.00\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAT90lEQVR4nO3df6zdd33f8eersciaoSV2uDVenM0ULFUBtUE7Sjb1nwwc21CB00IR+6eXLVlatQh1qNJcZZKzhEpO1ioIbSNyDZo7aRCWaYopXa0bQ6T9A81xlqTJSHbdALI9J7mNLShDgLK998f5mB675+Nr33Ovr6/zfEhfne/n831/v+fzsaX7ut8f555UFZIkTfJTqz0ASdLly5CQJHUZEpKkLkNCktRlSEiSutat9gCW01ve8pbasmXLag9DktaUI0eO/GVVzUzadkWFxJYtWxgOh6s9DElaU5J8p7fNy02SpC5DQpLUZUhIkroMCUlSlyEhSepalpBIsjPJi0mOJtk9YfvVSR5p27+RZMvYtt9t/S8m2XGhx5QkrbypQyLJVcC/A94H3AT8kyQ3nVN2J3C6qt4BPAQ80Pa9Cfgo8E5gJ/Dvk1x1gceUJK2w5TiTuAU4WlUvVdWPgS8Cu86p2QUcaOuPAu9Nktb/xar6UVV9Czjajnchx5QkrbDlCIkbgGNj7eOtb2JNVb0OfBe4/jz7XsgxAUhyd5JhkuHCwsIU05AknWvN37iuqn1VNaiqwczMxE+VS5KWaDlC4gRw41h7c+ubWJNkHXAt8Np59r2QY0qSVthyhMSTwNYkb0vyJkY3og+eU3MQmG3rHwa+WqPvTT0IfLQ9/fQ2YCvwZxd4TEnSCpv6D/xV1etJPg4cAq4CPl9Vzye5DxhW1UHgc8B/THIUOMXohz6t7kvA/wReB36rqv4vwKRjTjtWSdLFyegX+ivDYDAo/wqsJF2cJEeqajBp25q/cS1JWjmGhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqSuqUIiyYYkc0nm2+v6Tt1sq5lPMjvW/w+S/HmSo0k+kySt/94kJ5I83Zb3TzNOSdLSTHsmsRs4XFVbgcOtfZYkG4A9wK3ALcCesTD5LPDPga1t2Tm260NVdXNb/mTKcUqSlmDakNgFHGjrB4A7JtTsAOaq6lRVnQbmgJ1JNgF/p6q+XlUF/FFnf0nSKpk2JDZW1cm2/jKwcULNDcCxsfbx1ndDWz+3/4yPJ3k2yed7l7EAktydZJhkuLCwsKRJSJImWzQkkjye5LkJy67xunY2UMs0rs8CbwduBk4Cf9ArrKp9VTWoqsHMzMwyvb0kCWDdYgVVta23LckrSTZV1cl2+ejVCWUngNvG2puBJ1r/5nP6T7T3fGXsPf4Q+OPFxilJWn7TXm46CJx5WmkWeGxCzSFge5L17bLRduBQu0z1vST/sD3V9Gtn9m+Bc8YvA89NOU5J0hIseiaxiL3Al5LcCXwH+AhAkgHwG1V1V1WdSnI/8GTb576qOtXWfxP4D8BPA/+tLQAPJrmZ0eWrbwO/PuU4JUlLkNGthCvDYDCo4XC42sOQpDUlyZGqGkza5ieuJUldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkrqlCIsmGJHNJ5tvr+k7dbKuZTzI71v97SY4l+f459VcneSTJ0STfSLJlmnFKkpZm2jOJ3cDhqtoKHG7tsyTZAOwBbgVuAfaMhcmXW9+57gROV9U7gIeAB6YcpyRpCaYNiV3AgbZ+ALhjQs0OYK6qTlXVaWAO2AlQVV+vqpOLHPdR4L1JMuVYJUkXadqQ2Dj2Q/5lYOOEmhuAY2Pt463vfH6yT1W9DnwXuH5SYZK7kwyTDBcWFi5m7JKkRaxbrCDJ48BbJ2y6Z7xRVZWklmtgF6qq9gH7AAaDwSV/f0m6ki0aElW1rbctyStJNlXVySSbgFcnlJ0AbhtrbwaeWORtTwA3AseTrAOuBV5bbKySpOU17eWmg8CZp5Vmgccm1BwCtidZ325Yb299F3rcDwNfrSrPEiTpEps2JPYCtyeZB7a1NkkGSfYDVNUp4H7gybbc1/pI8mCS48A1SY4nubcd93PA9UmOAp9kwlNTkqSVlyvpF/TBYFDD4XC1hyFJa0qSI1U1mLTNT1xLkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6poqJJJsSDKXZL69ru/Uzbaa+SSzY/2/l+RYku+fU/+xJAtJnm7LXdOMU5K0NNOeSewGDlfVVuBwa58lyQZgD3ArcAuwZyxMvtz6Jnmkqm5uy/4pxylJWoJpQ2IXcKCtHwDumFCzA5irqlNVdRqYA3YCVNXXq+rklGOQJK2QaUNi49gP+ZeBjRNqbgCOjbWPt77FfCjJs0keTXJjryjJ3UmGSYYLCwsXPHBJ0uIWDYkkjyd5bsKya7yuqgqoZRrXl4EtVfXzjM48DvQKq2pfVQ2qajAzM7NMby9JAli3WEFVbettS/JKkk1VdTLJJuDVCWUngNvG2puBJxZ5z9fGmvuBBxcbpyRp+U17uekgcOZppVngsQk1h4DtSda3G9bbW19XC5wzPgh8c8pxSpKWYNqQ2AvcnmQe2NbaJBkk2Q9QVaeA+4En23Jf6yPJg0mOA9ckOZ7k3nbcTyR5PskzwCeAj005TknSEmR0K+HKMBgMajgcrvYwJGlNSXKkqgaTtvmJa0lSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqWuqkEiyIclckvn2ur5TN9tq5pPMtr5rknwlyQtJnk+yd6z+6iSPJDma5BtJtkwzTknS0kx7JrEbOFxVW4HDrX2WJBuAPcCtwC3AnrEw+f2q+jng3cAvJnlf678TOF1V7wAeAh6YcpySpCWYNiR2AQfa+gHgjgk1O4C5qjpVVaeBOWBnVf2gqr4GUFU/Bp4CNk847qPAe5NkyrFKki7StCGxsapOtvWXgY0Tam4Ajo21j7e+n0hyHfABRmcjZ+1TVa8D3wWunzSAJHcnGSYZLiwsLHEakqRJ1i1WkORx4K0TNt0z3qiqSlIXO4Ak64AvAJ+pqpcudv+q2gfsAxgMBhf9/pKkvkVDoqq29bYleSXJpqo6mWQT8OqEshPAbWPtzcATY+19wHxVffqcfW4EjrcQuRZ4bbGxSpKW17SXmw4Cs219FnhsQs0hYHuS9e2G9fbWR5JPMQqA3z7PcT8MfLWqPEuQpEts2pDYC9yeZB7Y1tokGSTZD1BVp4D7gSfbcl9VnUqymdElq5uAp5I8neSudtzPAdcnOQp8kglPTUmSVl6upF/QB4NBDYfD1R6GJK0pSY5U1WDSNj9xLUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdU0VEkk2JJlLMt9e13fqZlvNfJLZ1ndNkq8keSHJ80n2jtV/LMlCkqfbctc045QkLc20ZxK7gcNVtRU43NpnSbIB2APcCtwC7BkLk9+vqp8D3g38YpL3je36SFXd3Jb9U45TkrQE04bELuBAWz8A3DGhZgcwV1Wnquo0MAfsrKofVNXXAKrqx8BTwOYpxyNJWkbThsTGqjrZ1l8GNk6ouQE4NtY+3vp+Isl1wAcYnY2c8aEkzyZ5NMmNvQEkuTvJMMlwYWFhKXOQJHUsGhJJHk/y3IRl13hdVRVQFzuAJOuALwCfqaqXWveXgS1V9fOMzjwO9Pavqn1VNaiqwczMzMW+vSTpPNYtVlBV23rbkrySZFNVnUyyCXh1QtkJ4Lax9mbgibH2PmC+qj499p6vjW3fDzy42DglSctv2stNB4HZtj4LPDah5hCwPcn6dsN6e+sjyaeAa4HfHt+hBc4ZHwS+OeU4JUlLMG1I7AVuTzIPbGttkgyS7AeoqlPA/cCTbbmvqk4l2QzcA9wEPHXOo66faI/FPgN8AvjYlOOUJC1BRrcSrgyDwaCGw+FqD0OS1pQkR6pqMGmbn7iWJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6pg6JJBuSzCWZb6/rO3WzrWY+yexY/58meSbJ80keTnLVxRxXkrRyluNMYjdwuKq2Aodb+yxJNgB7gFuBW4A9Yz/0P1JVvwC8C5gBfvVCjytJWlnLERK7gANt/QBwx4SaHcBcVZ2qqtPAHLAToKq+12rWAW8C6iKOK0laQcsREhur6mRbfxnYOKHmBuDYWPt46wMgySHgVeCvgEcv4rgkuTvJMMlwYWFh6bOQJP0NFxQSSR5P8tyEZdd4XVUVf30mcMGqagewCbgaeM+E7d3jVtW+qhpU1WBmZuZi31qSdB7rLqSoqrb1tiV5JcmmqjqZZBOjM4JznQBuG2tvBp445z1+mOQxRpeZ5oALOa4kaQUtx+Wmg8CZp5Vmgccm1BwCtidZ325YbwcOJXlzCwCSrAN+CXjhIo4rSVpByxESe4Hbk8wD21qbJIMk+wGq6hRwP/BkW+5rfX8bOJjkWeBpRmcLD5/vuJKkSyejy/1XhsFgUMPhcLWHIUlrSpIjVTWYtM1PXEuSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqmiokkmxIMpdkvr2u79TNtpr5JLNj/X+a5Jkkzyd5OMlVrf/eJCeSPN2W908zTknS0kx7JrEbOFxVW4HDrX2WJBuAPcCtwC3AnrEw+UhV/QLwLmAG+NWxXR+qqpvb8idTjlOStATThsQu4EBbPwDcMaFmBzBXVaeq6jQwB+wEqKrvtZp1wJuAmnI8kqRlNG1IbKyqk239ZWDjhJobgGNj7eOtD4Akh4BXgb8CHh2r+3iSZ5N8vncZq+1/d5JhkuHCwsJS5yFJmmDRkEjyeJLnJiy7xuuqqljCmUBV7QA2AVcD72ndnwXeDtwMnAT+4Dz776uqQVUNZmZmLvbtJUnnsW6xgqra1tuW5JUkm6rqZJJNjM4IznUCuG2svRl44pz3+GGSxxhdvpqrqlfG3uMPgT9ebJySpOU37eWmg8CZp5Vmgccm1BwCtidZ3y4bbQcOJXlzCxaSrAN+CXihtTeN7f/LwHNTjlOStASLnkksYi/wpSR3At8BPgKQZAD8RlXdVVWnktwPPNn2ua/1bQQOJrmaUVh9DXi41TyY5GZGl6++Dfz6lOOUJC1BRrcSrgyDwaCGw+FqD0OS1pQkR6pqMGmbn7iWJHVdUWcSSRYYXfZaa94C/OVqD+ISe6PN+Y02X3DOa8nfr6qJj4deUSGxViUZ9k71rlRvtDm/0eYLzvlK4eUmSVKXISFJ6jIkLg/7VnsAq+CNNuc32nzBOV8RvCchSeryTEKS1GVISJK6DIlLZNpv8RvbfjDJZf+3rKaZb5JrknwlyQvtWwv3XtrRX5wkO5O8mORokklfvHV1kkfa9m8k2TK27Xdb/4tJdlzSgU9hqXNOcnuSI0n+vL2+528c/DI1zf9z2/73knw/ye9cskEvh6pyuQQL8CCwu63vBh6YULMBeKm9rm/r68e2/wrwn4DnVns+Kzlf4BrgH7eaNwH/HXjfas+pM8+rgL8AfraN9RngpnNqfhN4uK1/FHikrd/U6q8G3taOc9Vqz2mF5/xu4O+29XcBJ1Z7Pis957HtjwL/Gfid1Z7PxSyeSVw6U32LX5I3A58EPrXyQ10WS55vVf2gqr4GUFU/Bp5i9CfmL0e3AEer6qU21i8ymvu48X+LR4H3Jknr/2JV/aiqvgUcbce73C15zlX1P6rqf7f+54Gfbn/k83I3zf8zSe4AvsVozmuKIXHpTPstfvcz+vKlH6zYCJfX1N9aCJDkOuADjL5D/XK06BzGa6rqdeC7wPUXuO/laJo5j/sQ8FRV/WiFxrmcljzn9gvevwT+9SUY57Kb9k+Fa0ySx4G3Tth0z3ijqirJBT973P5s+tur6l+ce51zNa3UfMeOvw74AvCZqnppaaPU5SjJO4EHGH2/zJXuXuChqvp+O7FYUwyJZVQr9y1+/wgYJPk2o/+zn0nyRFXdxipawfmesQ+Yr6pPTz/aFXMCuHGsvbn1Tao53oLvWuC1C9z3cjTNnEmyGfivwK9V1V+s/HCXxTRzvhX4cJIHgeuA/5fkh1X1b1d81MthtW+KvFEW4N9w9o3cByfUbGB03XJ9W74FbDinZgtr48b1VPNldO/lvwA/tdpzWWSe6xjdcH8bf31D853n1PwWZ9/Q/FJbfydn37h+ibVx43qaOV/X6n9ltedxqeZ8Ts29rLEb16s+gDfKwuh67GFgHnh87IfhANg/VvfPGN3APAr80wnHWSshseT5MvotrYBvAk+35a7VntN55vp+4H8xevrlntZ3H/DBtv63GD3VchT4M+Bnx/a9p+33IpfpE1zLOWfgXwH/Z+z/9WngZ1Z7Piv9/zx2jDUXEv5ZDklSl083SZK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkrv8PBK+awSjpxDMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 用模型预测数据\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_batch_count = test_x.shape[0]\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "actual_line=[]\n",
    "pred_line=[]\n",
    "\n",
    "for step in range(test_batch_count):\n",
    "    pred, hn, cn = model(test_x[step], h0, c0)\n",
    "    \n",
    "    h0, c0 = hn.detach(), cn.detach()\n",
    "\n",
    "    loss = loss_func(pred[:,-1], test_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "    \n",
    "    test_loss += loss.cpu()\n",
    "    \n",
    "    actual_line.append(test_y[step][-1,-1].item())\n",
    "    pred_line.append(pred[-1,-1].item())\n",
    "        \n",
    "print(\"Prediction Loss average:{:.6f}\".format(test_loss.data/(step+1)))\n",
    "print(\"Prediction: {:.2f}\".format(float(pred[-1,-1].data)))\n",
    "print(\"Actual:     {:.2f}\".format(float(test_y[step][-1,-1].data)))\n",
    "\n",
    "# actual_line = test_y[step][-1].cpu().detach().flatten().numpy()        # Only plot the last sequence of test batch\n",
    "# pred_line   = pred[-1].cpu().detach().flatten().numpy()                # Only plot the last sequence of test batch\n",
    "plt.plot(actual_line, 'r--')\n",
    "plt.plot(pred_line, 'b-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b83f3b-d110-4968-a685-13beebc4dec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
