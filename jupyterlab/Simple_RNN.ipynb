{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07092afd-7a4b-4e71-aab7-e805f5267f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是用RNN模型将sin的输入转换为cos的输出；\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764d4b5-314a-4299-9a41-6f2ab10658b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP = 9                               # rnn 定义的时间步数\n",
    "INPUT_SIZE = 1                              # 因为输入的是1个数字，输入也就是1个feature\n",
    "LR = 0.01                                   # learning rate\n",
    "HIDDEN_SIZE = 64                            # 隐藏层的特征数量\n",
    "torch.manual_seed(1)                        # 随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c220e-8951-4438-b7e3-ecf54e1852fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):                                   # hidden = np.tanh(np.dot(self.W_hh, hidden) + np.dot(self.W_xh, x))\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(                              # 自己定义一个RNN神经网络\n",
    "            input_size = INPUT_SIZE,                    # 输入 x 的feature 维度\n",
    "            hidden_size = HIDDEN_SIZE,                  # 隐状态 hidden_state 中的feature维度\n",
    "            num_layers = 5,                             # RNN 的层数\n",
    "            nonlinearity='relu',                        # 指定激活函数 [‘tanh’ | ’relu’]. 默认: ‘tanh’\n",
    "            bias = True,                                # 如果是 False , 那么 RNN 层就不会使用偏置权重 b_ih 和 b_hh, 默认: True\n",
    "            batch_first = True,                         # 如果 True, 输入Tensor的shape应该是(batch, seq, features),并且输出也是一样.\n",
    "            dropout = 0,                                # 如果值非零, 那么除了最后一层外, 其它层的输出都会套上一个 dropout 层\n",
    "            bidirectional = False                       # 如果 True , 将会变成一个双向 RNN, 默认为 False\n",
    "        )\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, 1)            # 定义一个输出层，这是RNN的最后输出，只用输出output_vector\n",
    "\n",
    "    def forward(self, x, h_state):                      # 这就是RNN每次输入的参数x和h。RNN输入必需是3维tensor\n",
    "        # x (batch, time_step, input_size)              # 这是RNN的ｘ的维度            (批量, 序列长度, 输入的特征维度）\n",
    "        # h_state (n_layers, batch, hidden_size)        # 这是hidden_state的维度       (层数×方向, 批量, 输出的特征维度）/*方向：单向是１；双向是２*/\n",
    "        # r_out (batch, time_step, hidden_size)         # 这是网络实际输出的r_out的维度 (批量，序列长度，输出的特征维度X方向）\n",
    "        r_out, h_state = self.rnn(x, h_state)           # RNN每次输入x, hidden_state; 输出r_out, hidden_state; 给定一个序列x,每个x.size=[BATCH_SIZE, feature].\n",
    "        outs = []\n",
    "        for time_step in range(r_out.size(1)):\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        return torch.stack(outs, dim=1), h_state        # RNN的forward输出了output_vector, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3023ab-f77e-4f61-8e89-afe6b0d73c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR, weight_decay=1e-6)    # 定义了优化函数ADAM对RNN的参数反向优化，定义了 learning rate 和 权重衰减率\n",
    "loss_func = nn.MSELoss()                                                    # 定义了模型的损失函数用 mean-square-error\n",
    "\n",
    "\n",
    "h_state = None                                                          # 起始时输入给RNN的hidden_state就是None\n",
    "\n",
    "for step in range(200):                                                # 计算２000次。相当于２000个顺序的时间片数据丢进去计算\n",
    "    start, end = step*np.pi, (step+1)*np.pi                             # 每次计算的区间就是一个PI里的数据\n",
    "    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32)        # 在[star，star+PI]之间均匀地取TIME_STEP个点\n",
    "    x_np = np.sin(steps)                                                # 这就是用来输入的数据。就是每个点的sin值\n",
    "    y_np = np.cos(steps+np.pi/2)                                          # 这就是需要被预测出的数据。就是每个点的cos值\n",
    "    \n",
    "    x = Variable(torch.from_numpy(x_np[np.newaxis, :, np.newaxis]))     # 将x_np从1维(n)转化成3维(1,n,1);forward()里定义了x格式(batch, seq_len, input_size)\n",
    "    y = Variable(torch.from_numpy(y_np[np.newaxis, :, np.newaxis]))     # 将y_np从1维(n)转化成3维(1,n,1);forward()里定义了out格式(batch, seq_len, input_size)\n",
    "    # print(x.size())\n",
    "    # print(y.size())\n",
    "    \n",
    "    prediction, h_state = rnn(x, h_state)                               # 这就是一次RNN模型forward()得出来的结果\n",
    "    # print(h_state.size())                                             # h_state是tensor(data, grad_fn=xxxx)它的data格式为tensor(n_layers, batch, hidden_size)\n",
    "    h_state = Variable(h_state.data)                                    # 把 h_state(tensor)中的数据取出来,因为h_state包含.data和.grad 舍弃了梯度\n",
    "\n",
    "    loss = loss_func(prediction, y)                                     # 用损失函数计算误差\n",
    "    optimizer.zero_grad()                                               # 清空优化器梯度\n",
    "    loss.backward()                                                     # 反向传播求解梯度\n",
    "    optimizer.step()                                                    # 更新权重的参数。优化网络参数具体应指 W_xh, W_hh, b_h 以及 W_hq, b_q\n",
    "    \n",
    "    if (step+1)%20==0:\n",
    "        print(\"step:{}  loss:{:.9f}\".format(step, loss.data.float()))\n",
    "\n",
    "        plt.ion()                                                           # 把实际图和预测图动态打印出来\n",
    "        plt.plot(steps, y_np, color='b')\n",
    "        plt.plot(steps, np.squeeze(prediction.data.numpy()), color='r')\n",
    "        plt.show()\n",
    "        plt.pause(0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a65d6-139e-41a6-9101-a5b76d64f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下为新的 RNN 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c351fb3-10cb-4534-99ea-45dc3e8ef7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 试一下 RNN 的变长seq的处理情况\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5742016-4d2e-455d-a88a-dcdf2553c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 3 句话\n",
    "sa = [[1],[2],[3]]\n",
    "sb = [[4], [5]]\n",
    "sc = [[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fc33c-862c-4a04-ac47-291742704f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再来定义一个RNN\n",
    "rnn = nn.RNN(1, 3, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e8a4a-1fda-4e86-ae51-0fd4ddf0ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用rnn处理一下第一句话\n",
    "input = torch.Tensor([sa])\n",
    "rnn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4646ee-c3ea-4b25-9ccf-c32b0fe361a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 sa,sb,sc 做成一个batch来处理\n",
    "# 下面这样处理是错误的\n",
    "# input = torch.Tensor([sa, sb, sc])\n",
    "# 因为sa的dim是3，sb的dim是2，sc的dim是1，这不匹配\n",
    "# [\n",
    "#  [[1],[2],[3]],\n",
    "#  [[4],[5]],\n",
    "#  [[6]]\n",
    "# ]\n",
    "# 要把矩阵调整到一样维度的话，做补齐\n",
    "# [\n",
    "#  [[1],[2],[3]]\n",
    "#  [[4],[5],[0]]\n",
    "#  [[6],[0],[0]]\n",
    "# ]\n",
    "\n",
    "# 下面用 0 来补齐，并调整成 batch_first\n",
    "padded_seq = pad_sequence([torch.Tensor(sa), torch.Tensor(sb), torch.Tensor(sc)], batch_first=True)\n",
    "print(padded_seq)\n",
    "\n",
    "# 如果不是 batch_first 的模式\n",
    "# padded_seq = pad_sequence([torch.Tensor(sa), torch.Tensor(sb), torch.Tensor(sc)], batch_first=False)\n",
    "# print(padded_seq)\n",
    "\n",
    "rnn(padded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d006ee-8039-49e5-bdbc-b441eb69a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 或者不再补 0， 直接每次抽取一个元素后生成seq 后再发给RNN\n",
    "packed_seq = pack_sequence([torch.Tensor(i) for i in [sa, sb, sc]])\n",
    "print(packed_seq)\n",
    "# 但是packed_seq默认是seq_first的，所以前面的rnn要重新来\n",
    "rnn = nn.RNN(1, 3)\n",
    "rnn(packed_seq)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b23fed-a5fa-4b5a-9e9d-8055109464d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# 下面试试股票数据处理 #################\n",
    "################ CPU CPU CPU CPU CPU #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0995d4-1d17-4117-b2d1-b28c0a4abfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4daaba-e6cc-4e47-9d55-d7b114ebaf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "dataset = pd.read_csv(\"000600.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date','prediction'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset['updown'] = dataset['updown']\n",
    "print(dataset.shape)\n",
    "# print(dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9d701-848a-44d2-bc7d-69932fb134a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法1，直接将数据等分成batch\n",
    "# 把数据分成训练数据和测试数据\n",
    "# print(dataset)\n",
    "# n_train = 120\n",
    "# train = dataset.iloc[:n_train, :].reset_index(drop=True)\n",
    "# test  = dataset.iloc[n_train:, :].reset_index(drop=True)\n",
    "\n",
    "# train_x, train_y = train.iloc[:, 1:], train.iloc[:, 0]\n",
    "# test_x, test_y = test.iloc[:, 1:], test.iloc[:, 0]\n",
    "\n",
    "# train_y = np.array(train_y).reshape(-1, 1)\n",
    "# test_y = np.array(test_y).reshape(-1, 1)\n",
    "\n",
    "# print(train_x.shape)\n",
    "# print(train_y.shape)\n",
    "# print(test_x.shape)\n",
    "# print(test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d6c5b-6c01-4d1b-a0f5-a267791eb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据转换成 tesnsor\n",
    "# train_x = torch.tensor(np.array(train_x))\n",
    "# train_y = torch.tensor(np.array(train_y))\n",
    "# test_x = torch.tensor(np.array(test_x))\n",
    "# test_y = torch.tensor(np.array(test_y))\n",
    "# print(train_x.shape)\n",
    "# print(train_y.shape)\n",
    "# print(test_x.shape)\n",
    "# print(test_y.shape)\n",
    "\n",
    "# # 数据转成 batch 的格式\n",
    "# batch_count = 8               # 把训练数据切割成 batch_count 个batch\n",
    "# BATCH_SIZE = 15               # 每个batch里面有 BATCH_SIZE 个数据\n",
    "\n",
    "# # 把训练数据分成 8 个batch，每个 15 个数据\n",
    "# train_x = train_x.reshape(batch_count, BATCH_SIZE, 117)\n",
    "# train_y = train_y.reshape(batch_count, BATCH_SIZE, 1)\n",
    "# # 把测试数据分成 2 个batch，每个 batch 有 15 个数据\n",
    "# test_x = test_x.reshape(2, 15, 117)\n",
    "# test_y = test_y.reshape(2, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6c124-6a1f-4590-a251-d991d14e4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法2，将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "seq_len = 15\n",
    "BATCH_SIZE = 2                                                    # 注意：BATCH_SIZE是要能够整除seq_count的\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(seq_len):\n",
    "    if i.shape[0] == seq_len:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, seq_len, 118)                 # 数据一共是 seq_count x seq_len x in_dim\n",
    "\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                 # 所以一共有 seq_count 列数据，每一行的数据是118维 （包括y）\n",
    "print(\"seq length: {}\".format(seq_len))\n",
    "\n",
    "\n",
    "total_batch_count = int(rolling_data.shape[0]/BATCH_SIZE)                          # 把数据规划成 batch_count 个 batch\n",
    "\n",
    "print(\"total batch count: {}\".format(total_batch_count))\n",
    "print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "rolling_data = rolling_data.reshape(total_batch_count, BATCH_SIZE, seq_len, 118)  # 把数据转成 total_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "rolling_data = torch.tensor(rolling_data)\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "\n",
    "\n",
    "train_batch_count = total_batch_count - 1\n",
    "test_batch_count = total_batch_count - train_batch_count\n",
    "\n",
    "train = rolling_data[:train_batch_count, :, :, :]\n",
    "test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "\n",
    "# train = rolling_data.iloc[:train_batch_count*BATCH_SIZE, :].reset_index(drop=True)\n",
    "# test  = rolling_data.iloc[train_batch_count*BATCH_SIZE:, :].reset_index(drop=True)\n",
    "\n",
    "# train_x, train_y = train.iloc[:, 1:], train.iloc[:, 0]\n",
    "# test_x, test_y = test.iloc[:, 1:], test.iloc[:, 0]\n",
    "\n",
    "train_x, train_y = train[:,:,:,1:], train[:,:,:,0:1]\n",
    "test_x,  test_y  = test[:,:,:, 1:],  test[:,:,:,0:1]\n",
    "\n",
    "# train_y = torch.tensor(np.array(train_y).reshape(-1, 1))\n",
    "# test_y = torch.tensor(np.array(test_y).reshape(-1, 1))\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train_batch_count))\n",
    "print(\"test_batch_count:  {}\".format(test_batch_count))\n",
    "\n",
    "# # 数据转换成 tesnsor\n",
    "# train_x = torch.tensor(np.array(train_x))\n",
    "# train_y = torch.tensor(np.array(train_y))\n",
    "# test_x = torch.tensor(np.array(test_x))\n",
    "# test_y = torch.tensor(np.array(test_y))\n",
    "# print(train_x.shape)\n",
    "# print(train_y.shape)\n",
    "# print(test_x.shape)\n",
    "# print(test_y.shape)\n",
    "\n",
    "# # batch first\n",
    "# # 把训练数据分成 ... 个batch，每 batch 有 15 个sequence\n",
    "# train_x = train_x.reshape(train_batch_count, BATCH_SIZE, seq_len, 117)\n",
    "# train_y = train_y.reshape(train_batch_count, BATCH_SIZE, seq_len, 1)\n",
    "# # 把测试数据分成 1 个batch，每个 batch 有 15 个sequence\n",
    "# test_x = test_x.reshape(test_batch_count, BATCH_SIZE, seq_len, 117)\n",
    "# test_y = test_y.reshape(test_batch_count, BATCH_SIZE, seq_len, 1)\n",
    "\n",
    "# seq first\n",
    "# # 把训练数据分成 ... 个batch，每 batch 有 15 个sequence\n",
    "# train_x = train_x.reshape(BATCH_SIZE, train_batch_count, 117)\n",
    "# train_y = train_y.reshape(BATCH_SIZE, train_batch_count, 1)\n",
    "# # 把测试数据分成 1 个batch，每个 batch 有 15 个sequence\n",
    "# test_x = test_x.reshape(BATCH_SIZE, test_batch_count, 117)\n",
    "# test_y = test_y.reshape(BATCH_SIZE, test_batch_count, 1)\n",
    "# print(\"train_x: {}\".format(train_x.size))\n",
    "# print(\"train_y: {}\".format(train_y.size))\n",
    "# print(\"test_x:  {}\".format(test_x.size))\n",
    "# print(\"test_y:  {}\".format(test_y.size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4204c-702a-40e0-b1e3-2753059ed306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始定义 RNN 模型\n",
    "TIME_STEP = seq_len                                        # 一般这个单独设定，这里为了简单，还是直接就等于seq_len的方便。其实也就是等于最长的那个sequence length\n",
    "INPUT_SIZE = 117\n",
    "HIDDEN_SIZE = 768\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "                input_size = INPUT_SIZE,                    # 输入 x 的feature 维度\n",
    "                hidden_size = HIDDEN_SIZE,                  # 隐状态 hidden_state 中的feature维度\n",
    "                num_layers = 5,                             # RNN 的层数\n",
    "                nonlinearity='relu',                        # 指定激活函数 [‘tanh’ | ’relu’]. 默认: ‘tanh’\n",
    "                bias = True,                                # 如果是 False , 那么 RNN 层就不会使用偏置权重 b_ih 和 b_hh, 默认: True\n",
    "                batch_first = True,                         # 如果 True, 输入Tensor的shape应该是(BATCH_SIZE, seq_len, features),并且输出也是一样.\n",
    "                dropout = 0.1,                                # 如果值非零, 那么除了最后一层外, 其它层的输出都会套上一个 dropout 层\n",
    "                bidirectional = False                       # 如果 True , 将会变成一个双向 RNN, 默认为 False\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        \n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        outs = []\n",
    "        for time_step in range(TIME_STEP):\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        return torch.stack(outs, dim=1), h_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7890d19-d716-415d-b7b4-e0ecded256ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始化 RNN 模型\n",
    "rnn = RNN()\n",
    "LR = 5e-5\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR, weight_decay=1e-8)\n",
    "\n",
    "i = 0\n",
    "h_state = None\n",
    "# 训练 RNN 模型; 计划训练 20 轮\n",
    "for step in range(train_batch_count*20):\n",
    "    rnn = rnn.double()\n",
    "    pred, h_state = rnn(train_x[i], h_state)\n",
    "    h_state = h_state.data\n",
    "    loss = loss_func(pred, train_y[i])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i+1)%train_batch_count == 0:\n",
    "        print(loss)\n",
    "        i=0\n",
    "    else:\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a81ba-7c89-40cf-961d-cb0e0615e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用模型来做测试数据的误差\n",
    "h_state = None\n",
    "pred, h_state = rnn(test_x[0], h_state)\n",
    "loss = loss_func(pred, test_y[0])\n",
    "print(loss)\n",
    "\n",
    "actual_line = np.array(test_y[0]).flatten().tolist()\n",
    "pred_line = pred.data.numpy().flatten().tolist()\n",
    "\n",
    "print(len(actual_line) == len(pred_line))\n",
    "plt.plot(actual_line, 'r--')\n",
    "plt.plot(pred_line, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a103a-c9ee-45ed-aeb5-fe95e16d473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ GPU GPU GPU GPU GPU #################\n",
    "################ GPU GPU GPU GPU GPU #################\n",
    "################ GPU GPU GPU GPU GPU #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36d945-304a-4b17-be13-b5ef1020c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU -- 准备来做一个GPU版本的RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date','prediction'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset['future'] = dataset['future']\n",
    "print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "# 方法2，将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "seq_len = 45\n",
    "BATCH_SIZE = 1                                                    # 注意：BATCH_SIZE是要能够整除seq_count的\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(seq_len):\n",
    "    if i.shape[0] == seq_len:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, seq_len, dataset.shape[1])                 # 数据一共是 seq_count x seq_len x (in_dim+1)\n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(seq_len))\n",
    "\n",
    "\n",
    "total_batch_count = int(rolling_data.shape[0]/BATCH_SIZE)                                   # 把数据规划成 batch_count 个 batch\n",
    "\n",
    "print(\"total batch count: {}\".format(total_batch_count))\n",
    "print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "rolling_data = rolling_data.reshape(total_batch_count, BATCH_SIZE, seq_len, dataset.shape[1])  # 把数据转成 total_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "rolling_data = torch.tensor(rolling_data)\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "\n",
    "\n",
    "train_batch_count = total_batch_count - 1\n",
    "test_batch_count = total_batch_count - train_batch_count\n",
    "\n",
    "train = rolling_data[:train_batch_count, :, :, :]\n",
    "test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,1:], train[:,:,:,0:1]\n",
    "test_x,  test_y  = test[:,:,:, 1:],  test[:,:,:,0:1]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train_batch_count))\n",
    "print(\"test_batch_count:  {}\".format(test_batch_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148659b-0dcb-441c-802e-727c30bc0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU -- 开始定义 RNN 模型\n",
    "TIME_STEP = seq_len                                        # 一般这个单独设定，这里为了简单，还是直接就等于seq_len的方便。其实也就是等于最长的那个sequence length\n",
    "INPUT_SIZE = 122\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 8\n",
    "\n",
    "np.random.seed(3)\n",
    "torch.manual_seed(3)\n",
    "torch.cuda.manual_seed(3)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "                input_size = INPUT_SIZE,                    # 输入 x 的feature 维度\n",
    "                hidden_size = HIDDEN_SIZE,                  # 隐状态 hidden_state 中的feature维度\n",
    "                num_layers = NUM_LAYERS,                    # RNN 的层数\n",
    "                nonlinearity='relu',                        # 指定激活函数 [‘tanh’ | ’relu’]. 默认: ‘tanh’\n",
    "                bias = True,                                # 如果是 False , 那么 RNN 层就不会使用偏置权重 b_ih 和 b_hh, 默认: True\n",
    "                batch_first = True,                         # 如果 True, 输入Tensor的shape应该是(BATCH_SIZE, seq_len, features),并且输出也是一样.\n",
    "                dropout = 0.5,                              # 如果值非零, 那么除了最后一层外, 其它层的输出都会套上一个 dropout 层\n",
    "                bidirectional = False                       # 如果 True , 将会变成一个双向 RNN, 默认为 False\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        \n",
    "        # weight的参数用kaiming初始化；bias用zeros初始化；\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if name.startswith(\"weight\"):\n",
    "                nn.init.kaiming_normal_(param, nonlinearity='relu')\n",
    "            else:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "        # weight的参数用xavier初始化；bias用zeros初始化；\n",
    "        # for name, param in self.rnn.named_parameters():\n",
    "        #     if name.startswith(\"weight\"):\n",
    "        #         nn.init.xavier_normal_(param)\n",
    "        #     else:\n",
    "        #         nn.init.zeros_(param)\n",
    "                \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        h_state = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "        r_out, h_state = self.rnn(x, h_state)                         # compute from [1, seq_len, input_dim] to [1, seq_len, hidden_dim] \n",
    "\n",
    "        outs = []\n",
    "        \n",
    "        for time_step in range(TIME_STEP):\n",
    "            out = self.fc_out(r_out[:, time_step, :])                 # compute from [1, 1, hidden_dim] to [output_dim]\n",
    "            outs.append(out)                                          # Add all seq_len data to list like : [[pred1, pred2,...,predn]]\n",
    "        \n",
    "        return torch.stack(outs, dim=1)                               # conver from [[pred1,...,predn]] to [[[pred1], ... ,[prdn]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f164227-54ee-4273-80f2-fa79cbac8cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# GPU -- 初始化 RNN 模型\n",
    "model = RNN()\n",
    "model.double().to(device)\n",
    "LR = 1e-5\n",
    "epoches = 500\n",
    "\n",
    "epoch_loss_list = []\n",
    "epoch_loss = 0\n",
    "lr_list = []\n",
    "\n",
    "\n",
    "\n",
    "# loss_func = nn.L1Loss()\n",
    "loss_func = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epoches, eta_min=0, last_epoch=-1)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995, last_epoch=-1)\n",
    "\n",
    "\n",
    "# 训练 RNN 模型; 计划训练 100 轮\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        pred = model(train_x[step])\n",
    "        pred.to(device)\n",
    "        # h_state = h_state.data.to(device)\n",
    "\n",
    "        loss = loss_func(pred, train_y[step])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data\n",
    "    \n",
    "\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(\"{} of {} epoches get epoch_loss: {:.6f}\".format(epoch, epoches, epoch_loss))\n",
    "        lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        epoch_loss_list.append(epoch_loss.cpu())\n",
    "        # print(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        # for p in optimizer.param_groups:\n",
    "        #     p['lr'] *= 0.9\n",
    "        print(scheduler.get_last_lr())\n",
    "    \n",
    "    scheduler.step()    \n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "plt.plot(lr_list)\n",
    "plt.show()\n",
    "                   \n",
    "plt.plot(epoch_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890be23a-cdef-4a83-ab25-04162eb1093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU -- 用模型来做测试数据的误差\n",
    "\n",
    "for step in range(test_batch_count):\n",
    "    pred = model(test_x[step])\n",
    "\n",
    "    loss = loss_func(pred, test_y[step].to(device))\n",
    "\n",
    "    print(\"Prediction: {:.2f}\".format(float(pred[-1][-1].data)))\n",
    "    \n",
    "    print(\"loss : {:.6f}\".format(float(loss.data)))\n",
    "\n",
    "    # print(test_y[step][-1].flatten().shape)\n",
    "    # print(pred[-1].flatten().shape)\n",
    "    \n",
    "    # actual_line = test_y[step][-1].cpu().detach().flatten().numpy()\n",
    "    # pred_line   = pred[-1].cpu().detach().flatten().numpy()\n",
    "    \n",
    "    actual_line = test_y[step].cpu().detach().flatten().numpy()\n",
    "    pred_line   = pred.cpu().detach().flatten().numpy()\n",
    "    \n",
    "    plt.plot(actual_line, 'r--')\n",
    "    plt.plot(pred_line, 'b-')\n",
    "    plt.show()\n",
    "    \n",
    "# actual_line = np.array(test_y[-1]).flatten().tolist()\n",
    "# pred_line = pred.data.numpy().flatten().tolist()\n",
    "\n",
    "# actual_line = np.array(test_y[0][-1]).flatten().tolist()\n",
    "# pred_line = pred[-1].data.numpy().flatten().tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2afa5-2a6e-4a83-b5dc-3b687dc76a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53306a3d-2599-4718-8327-c3bbde4831cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
