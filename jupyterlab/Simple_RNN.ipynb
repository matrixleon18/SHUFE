{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d9768-347c-4f76-b310-7ccf808a033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是用RNN模型将sin的输入转换为cos的输出\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb4441-d214-4e9b-b69b-4282392f0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TIME_STEP = 5                               # rnn 定义的时间步数\n",
    "INPUT_SIZE = 1                              # 因为输入的是1个数字，也就是1个feature\n",
    "LR = 0.02                                   # learning rate\n",
    "HIDDEN_SIZE = 64                            # 隐藏层的特征数量\n",
    "torch.manual_seed(1)                        # 随机数种子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26e8c479-86a0-4883-828f-a5e494aace11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):                                   # hidden = np.tanh(np.dot(self.W_hh, hidden) + np.dot(self.W_xh, x))\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(                              # 自己定义一个RNN神经网络\n",
    "            input_size = INPUT_SIZE,                    # 输入 x 的feature 维度\n",
    "            hidden_size = HIDDEN_SIZE,                  # 隐状态 hidden_state 中的feature维度\n",
    "            num_layers = 3,                             # RNN 的层数\n",
    "            nonlinearity='relu',                        # 指定激活函数 [‘tanh’ | ’relu’]. 默认: ‘tanh’\n",
    "            bias = True,                                # 如果是 False , 那么 RNN 层就不会使用偏置权重 b_ih 和 b_hh, 默认: True\n",
    "            batch_first = True,                         # 如果 True, 输入Tensor的shape应该是(batch, seq, features),并且输出也是一样.\n",
    "            dropout = 0,                                # 如果值非零, 那么除了最后一层外, 其它层的输出都会套上一个 dropout 层\n",
    "            bidirectional = False                       # 如果 True , 将会变成一个双向 RNN, 默认为 False\n",
    "        )\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, 1)            # 定义一个输出层，这是RNN的最后输出，只用输出output_vector\n",
    "\n",
    "    def forward(self, x, h_state):                      # 这就是RNN每次输入的参数x和h\n",
    "        # x (batch, time_step, input_size)              # 这是RNN的ｘ的维度            (批量, 序列长度, 输入的特征维度）\n",
    "        # h_state (n_layers, batch, hidden_size)        # 这是hidden_state的维度       (层数×方向, 批量, 输出的特征维度）/*方向：单向是１；双向是２*/\n",
    "        # r_out (batch, time_step, hidden_size)         # 这是网络实际输出的r_out的维度 (批量，序列长度，输出的特征维度X方向）\n",
    "        r_out, h_state = self.rnn(x, h_state)           # RNN每次输入x, hidden_state; 输出r_out, hidden_state;\n",
    "        outs = []\n",
    "        for time_step in range(r_out.size(1)):\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        return torch.stack(outs, dim=1), h_state        # RNN的forward输出了output_vector, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d4d19f0-29ba-44b5-bbba-7a7453844c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn = RNN()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR, weight_decay=1e-6)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f6dcca70-71cf-4d7f-95ea-bbc063970f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0  loss:0.769753754\n",
      "step:1  loss:0.018608779\n",
      "step:2  loss:0.196479589\n",
      "step:3  loss:0.071586773\n",
      "step:4  loss:0.012409372\n",
      "step:5  loss:0.044172186\n",
      "step:6  loss:0.158242732\n",
      "step:7  loss:0.023251805\n",
      "step:8  loss:0.023255471\n",
      "step:9  loss:0.033692829\n",
      "step:10  loss:0.047859583\n",
      "step:11  loss:0.024848435\n",
      "step:12  loss:0.027895218\n",
      "step:13  loss:0.020639541\n",
      "step:14  loss:0.021983327\n",
      "step:15  loss:0.019498849\n",
      "step:16  loss:0.027365714\n",
      "step:17  loss:0.032652780\n",
      "step:18  loss:0.015358785\n",
      "step:19  loss:0.008105570\n",
      "step:20  loss:0.013356498\n",
      "step:21  loss:0.013916977\n",
      "step:22  loss:0.029352468\n",
      "step:23  loss:0.014032751\n",
      "step:24  loss:0.003490339\n",
      "step:25  loss:0.001934425\n",
      "step:26  loss:0.025918942\n",
      "step:27  loss:0.002195222\n",
      "step:28  loss:0.008604687\n",
      "step:29  loss:0.001199994\n",
      "step:30  loss:0.008275824\n",
      "step:31  loss:0.004451863\n",
      "step:32  loss:0.015970895\n",
      "step:33  loss:0.005393638\n",
      "step:34  loss:0.005837458\n",
      "step:35  loss:0.002005154\n",
      "step:36  loss:0.002670521\n",
      "step:37  loss:0.004802625\n",
      "step:38  loss:0.010009422\n",
      "step:39  loss:0.003454528\n",
      "step:40  loss:0.001179569\n",
      "step:41  loss:0.001060208\n",
      "step:42  loss:0.005945745\n",
      "step:43  loss:0.003880204\n",
      "step:44  loss:0.006506270\n",
      "step:45  loss:0.001725445\n",
      "step:46  loss:0.000770308\n",
      "step:47  loss:0.000272428\n",
      "step:48  loss:0.004655907\n",
      "step:49  loss:0.001551767\n",
      "step:50  loss:0.003057210\n",
      "step:51  loss:0.000475838\n",
      "step:52  loss:0.000996751\n",
      "step:53  loss:0.000876740\n",
      "step:54  loss:0.003314790\n",
      "step:55  loss:0.001240851\n",
      "step:56  loss:0.001575481\n",
      "step:57  loss:0.000283298\n",
      "step:58  loss:0.000657732\n",
      "step:59  loss:0.000299390\n",
      "step:60  loss:0.001830570\n",
      "step:61  loss:0.000378167\n",
      "step:62  loss:0.000513718\n",
      "step:63  loss:0.000064605\n",
      "step:64  loss:0.000443299\n",
      "step:65  loss:0.000475278\n",
      "step:66  loss:0.001039790\n",
      "step:67  loss:0.000344274\n",
      "step:68  loss:0.000215876\n",
      "step:69  loss:0.000037173\n",
      "step:70  loss:0.000343775\n",
      "step:71  loss:0.000310807\n",
      "step:72  loss:0.000746909\n",
      "step:73  loss:0.000154534\n",
      "step:74  loss:0.000139684\n",
      "step:75  loss:0.000035487\n",
      "step:76  loss:0.000516682\n",
      "step:77  loss:0.000244754\n",
      "step:78  loss:0.000462725\n",
      "step:79  loss:0.000097666\n",
      "step:80  loss:0.000028702\n",
      "step:81  loss:0.000026139\n",
      "step:82  loss:0.000299570\n",
      "step:83  loss:0.000147646\n",
      "step:84  loss:0.000196761\n",
      "step:85  loss:0.000034615\n",
      "step:86  loss:0.000025640\n",
      "step:87  loss:0.000037565\n",
      "step:88  loss:0.000199893\n",
      "step:89  loss:0.000080964\n",
      "step:90  loss:0.000133258\n",
      "step:91  loss:0.000009783\n",
      "step:92  loss:0.000027299\n",
      "step:93  loss:0.000030663\n",
      "step:94  loss:0.000148521\n",
      "step:95  loss:0.000045830\n",
      "step:96  loss:0.000050497\n",
      "step:97  loss:0.000002122\n",
      "step:98  loss:0.000033787\n",
      "step:99  loss:0.000035197\n",
      "step:100  loss:0.000095816\n",
      "step:101  loss:0.000030531\n",
      "step:102  loss:0.000012179\n",
      "step:103  loss:0.000001218\n",
      "step:104  loss:0.000036318\n",
      "step:105  loss:0.000021836\n",
      "step:106  loss:0.000044556\n",
      "step:107  loss:0.000010453\n",
      "step:108  loss:0.000000902\n",
      "step:109  loss:0.000003219\n",
      "step:110  loss:0.000028078\n",
      "step:111  loss:0.000014330\n",
      "step:112  loss:0.000025386\n",
      "step:113  loss:0.000003003\n",
      "step:114  loss:0.000000818\n",
      "step:115  loss:0.000005454\n",
      "step:116  loss:0.000027964\n",
      "step:117  loss:0.000011146\n",
      "step:118  loss:0.000011652\n",
      "step:119  loss:0.000000675\n",
      "step:120  loss:0.000004959\n",
      "step:121  loss:0.000005798\n",
      "step:122  loss:0.000019967\n",
      "step:123  loss:0.000005837\n",
      "step:124  loss:0.000002366\n",
      "step:125  loss:0.000000299\n",
      "step:126  loss:0.000006378\n",
      "step:127  loss:0.000005211\n",
      "step:128  loss:0.000009009\n",
      "step:129  loss:0.000002221\n",
      "step:130  loss:0.000000222\n",
      "step:131  loss:0.000000741\n",
      "step:132  loss:0.000006388\n",
      "step:133  loss:0.000003416\n",
      "step:134  loss:0.000005509\n",
      "step:135  loss:0.000000560\n",
      "step:136  loss:0.000000602\n",
      "step:137  loss:0.000001183\n",
      "step:138  loss:0.000006407\n",
      "step:139  loss:0.000002092\n",
      "step:140  loss:0.000001895\n",
      "step:141  loss:0.000000055\n",
      "step:142  loss:0.000001696\n",
      "step:143  loss:0.000001367\n",
      "step:144  loss:0.000003746\n",
      "step:145  loss:0.000000938\n",
      "step:146  loss:0.000000182\n",
      "step:147  loss:0.000000135\n",
      "step:148  loss:0.000001951\n",
      "step:149  loss:0.000001005\n",
      "step:150  loss:0.000001580\n",
      "step:151  loss:0.000000189\n",
      "step:152  loss:0.000000089\n",
      "step:153  loss:0.000000348\n",
      "step:154  loss:0.000001712\n",
      "step:155  loss:0.000000651\n",
      "step:156  loss:0.000000620\n",
      "step:157  loss:0.000000007\n",
      "step:158  loss:0.000000477\n",
      "step:159  loss:0.000000458\n",
      "step:160  loss:0.000001329\n",
      "step:161  loss:0.000000294\n",
      "step:162  loss:0.000000033\n",
      "step:163  loss:0.000000053\n",
      "step:164  loss:0.000000708\n",
      "step:165  loss:0.000000393\n",
      "step:166  loss:0.000000486\n",
      "step:167  loss:0.000000068\n",
      "step:168  loss:0.000000054\n",
      "step:169  loss:0.000000142\n",
      "step:170  loss:0.000000606\n",
      "step:171  loss:0.000000202\n",
      "step:172  loss:0.000000130\n",
      "step:173  loss:0.000000003\n",
      "step:174  loss:0.000000206\n",
      "step:175  loss:0.000000165\n",
      "step:176  loss:0.000000394\n",
      "step:177  loss:0.000000070\n",
      "step:178  loss:0.000000008\n",
      "step:179  loss:0.000000032\n",
      "step:180  loss:0.000000290\n",
      "step:181  loss:0.000000121\n",
      "step:182  loss:0.000000142\n",
      "step:183  loss:0.000000010\n",
      "step:184  loss:0.000000054\n",
      "step:185  loss:0.000000065\n",
      "step:186  loss:0.000000196\n",
      "step:187  loss:0.000000060\n",
      "step:188  loss:0.000000012\n",
      "step:189  loss:0.000000007\n",
      "step:190  loss:0.000000104\n",
      "step:191  loss:0.000000057\n",
      "step:192  loss:0.000000083\n",
      "step:193  loss:0.000000008\n",
      "step:194  loss:0.000000009\n",
      "step:195  loss:0.000000029\n",
      "step:196  loss:0.000000106\n",
      "step:197  loss:0.000000032\n",
      "step:198  loss:0.000000019\n",
      "step:199  loss:0.000000003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "h_state = None                                                          # 起始时输入给RNN的hidden_state就是None\n",
    "\n",
    "for step in range(200):                                                 # 计算２００次。相当于２００个顺序的时间片数据丢进去计算\n",
    "    start, end = step*np.pi, (step+1)*np.pi                             # 设计一小段数据起始点\n",
    "    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32)        # 生成一小段数据\n",
    "    x_np = np.sin(steps)                                                # 这就是用来输入的数据\n",
    "    y_np = np.cos(steps)                                                # 这就是需要被预测的数据\n",
    "    x = Variable(torch.from_numpy(x_np[np.newaxis, :, np.newaxis]))     # shape 1D -> 3D\n",
    "    y = Variable(torch.from_numpy(y_np[np.newaxis, :, np.newaxis]))\n",
    "    # print(x.size())\n",
    "    # print(y.size())\n",
    "    prediction, h_state = rnn(x, h_state)                               # 这就是一次RNN训练出来的结果\n",
    "    h_state = Variable(h_state.data)                                    # 把tensor中的数据取出来\n",
    "\n",
    "    loss = loss_func(prediction, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"step:{}  loss:{:.9f}\".format(step, loss.data.float()))\n",
    "\n",
    "#     plt.ion()                                                           # 把实际图和预测图动态打印出来\n",
    "#     plt.plot(steps, y_np, color='b')\n",
    "#     plt.plot(steps, np.squeeze(prediction.data.numpy()), color='r')\n",
    "#     plt.show()\n",
    "#     plt.pause(0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15a06a-9198-4253-9099-ff1bb02db5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ad0bd-8093-4a2b-8b69-92fe3894ba7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
