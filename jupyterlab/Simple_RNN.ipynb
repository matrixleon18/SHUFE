{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d9768-347c-4f76-b310-7ccf808a033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是用RNN模型将sin的输入转换为cos的输出；\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb4441-d214-4e9b-b69b-4282392f0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP = 9                               # rnn 定义的时间步数\n",
    "INPUT_SIZE = 1                              # 因为输入的是1个数字，输入也就是1个feature\n",
    "LR = 0.01                                   # learning rate\n",
    "HIDDEN_SIZE = 64                            # 隐藏层的特征数量\n",
    "torch.manual_seed(1)                        # 随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8c479-86a0-4883-828f-a5e494aace11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):                                   # hidden = np.tanh(np.dot(self.W_hh, hidden) + np.dot(self.W_xh, x))\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(                              # 自己定义一个RNN神经网络\n",
    "            input_size = INPUT_SIZE,                    # 输入 x 的feature 维度\n",
    "            hidden_size = HIDDEN_SIZE,                  # 隐状态 hidden_state 中的feature维度\n",
    "            num_layers = 5,                             # RNN 的层数\n",
    "            nonlinearity='relu',                        # 指定激活函数 [‘tanh’ | ’relu’]. 默认: ‘tanh’\n",
    "            bias = True,                                # 如果是 False , 那么 RNN 层就不会使用偏置权重 b_ih 和 b_hh, 默认: True\n",
    "            batch_first = True,                         # 如果 True, 输入Tensor的shape应该是(batch, seq, features),并且输出也是一样.\n",
    "            dropout = 0,                                # 如果值非零, 那么除了最后一层外, 其它层的输出都会套上一个 dropout 层\n",
    "            bidirectional = False                       # 如果 True , 将会变成一个双向 RNN, 默认为 False\n",
    "        )\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, 1)            # 定义一个输出层，这是RNN的最后输出，只用输出output_vector\n",
    "\n",
    "    def forward(self, x, h_state):                      # 这就是RNN每次输入的参数x和h。RNN输入必需是3维tensor\n",
    "        # x (batch, time_step, input_size)              # 这是RNN的ｘ的维度            (批量, 序列长度, 输入的特征维度）\n",
    "        # h_state (n_layers, batch, hidden_size)        # 这是hidden_state的维度       (层数×方向, 批量, 输出的特征维度）/*方向：单向是１；双向是２*/\n",
    "        # r_out (batch, time_step, hidden_size)         # 这是网络实际输出的r_out的维度 (批量，序列长度，输出的特征维度X方向）\n",
    "        r_out, h_state = self.rnn(x, h_state)           # RNN每次输入x, hidden_state; 输出r_out, hidden_state; 给定一个序列x,每个x.size=[batch_size, feature].\n",
    "        outs = []\n",
    "        for time_step in range(r_out.size(1)):\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        return torch.stack(outs, dim=1), h_state        # RNN的forward输出了output_vector, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52c858-7c44-4db1-ae6b-c384c93ad48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR, weight_decay=1e-6)    # 定义了优化函数ADAM对RNN的参数反向优化，定义了 learning rate 和 权重衰减率\n",
    "loss_func = nn.MSELoss()                                                    # 定义了模型的损失函数用 mean-square-error\n",
    "\n",
    "\n",
    "h_state = None                                                          # 起始时输入给RNN的hidden_state就是None\n",
    "\n",
    "for step in range(200):                                                # 计算２000次。相当于２000个顺序的时间片数据丢进去计算\n",
    "    start, end = step*np.pi, (step+1)*np.pi                             # 每次计算的区间就是一个PI里的数据\n",
    "    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32)        # 在[star，star+PI]之间均匀地取TIME_STEP个点\n",
    "    x_np = np.sin(steps)                                                # 这就是用来输入的数据。就是每个点的sin值\n",
    "    y_np = np.cos(steps+np.pi/2)                                          # 这就是需要被预测出的数据。就是每个点的cos值\n",
    "    \n",
    "    x = Variable(torch.from_numpy(x_np[np.newaxis, :, np.newaxis]))     # 将x_np从1维(n)转化成3维(1,n,1);forward()里定义了x格式(batch, seq_len, input_size)\n",
    "    y = Variable(torch.from_numpy(y_np[np.newaxis, :, np.newaxis]))     # 将y_np从1维(n)转化成3维(1,n,1);forward()里定义了out格式(batch, seq_len, input_size)\n",
    "    # print(x.size())\n",
    "    # print(y.size())\n",
    "    \n",
    "    prediction, h_state = rnn(x, h_state)                               # 这就是一次RNN模型forward()得出来的结果\n",
    "    # print(h_state.size())                                             # h_state是tensor(data, grad_fn=xxxx)它的data格式为tensor(n_layers, batch, hidden_size)\n",
    "    h_state = Variable(h_state.data)                                    # 把 h_state(tensor)中的数据取出来,因为h_state包含.data和.grad 舍弃了梯度\n",
    "\n",
    "    loss = loss_func(prediction, y)                                     # 用损失函数计算误差\n",
    "    optimizer.zero_grad()                                               # 清空优化器梯度\n",
    "    loss.backward()                                                     # 反向传播求解梯度\n",
    "    optimizer.step()                                                    # 更新权重的参数。优化网络参数具体应指 W_xh, W_hh, b_h 以及 W_hq, b_q\n",
    "    \n",
    "    if (step+1)%20==0:\n",
    "        print(\"step:{}  loss:{:.9f}\".format(step, loss.data.float()))\n",
    "\n",
    "        plt.ion()                                                           # 把实际图和预测图动态打印出来\n",
    "        plt.plot(steps, y_np, color='b')\n",
    "        plt.plot(steps, np.squeeze(prediction.data.numpy()), color='r')\n",
    "        plt.show()\n",
    "        plt.pause(0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ad0bd-8093-4a2b-8b69-92fe3894ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下为新的 RNN 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689f3a6-7a7a-4a28-a7cb-41e0d34fd5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 试一下 RNN 的变长seq的处理情况\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd0745-0b9b-4ca1-aaea-16fb40f50d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 3 句话\n",
    "sa = [[1],[2],[3]]\n",
    "sb = [[4], [5]]\n",
    "sc = [[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf40df3-5d56-44d8-a74b-f3deaeed7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再来定义一个RNN\n",
    "rnn = nn.RNN(1, 3, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ccc11b-2f09-450e-add6-c9370ca54d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用rnn处理一下第一句话\n",
    "input = torch.Tensor([sa])\n",
    "rnn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f0fbd-c095-4660-a42e-f81d9a1f8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 sa,sb,sc 做成一个batch来处理\n",
    "# 下面这样处理是错误的\n",
    "# input = torch.Tensor([sa, sb, sc])\n",
    "# 因为sa的dim是3，sb的dim是2，sc的dim是1，这不匹配\n",
    "# [\n",
    "#  [[1],[2],[3]],\n",
    "#  [[4],[5]],\n",
    "#  [[6]]\n",
    "# ]\n",
    "# 要把矩阵调整到一样维度的话，做补齐\n",
    "# [\n",
    "#  [[1],[2],[3]]\n",
    "#  [[4],[5],[0]]\n",
    "#  [[6],[0],[0]]\n",
    "# ]\n",
    "\n",
    "# 下面用 0 来补齐，并调整成 batch_first\n",
    "padded_seq = pad_sequence([torch.Tensor(sa), torch.Tensor(sb), torch.Tensor(sc)], batch_first=True)\n",
    "print(padded_seq)\n",
    "\n",
    "# 如果不是 batch_first 的模式\n",
    "# padded_seq = pad_sequence([torch.Tensor(sa), torch.Tensor(sb), torch.Tensor(sc)], batch_first=False)\n",
    "# print(padded_seq)\n",
    "\n",
    "rnn(padded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b11ba-e275-4a61-b787-6ff63644341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 或者不再补 0， 直接每次抽取一个元素后生成seq 后再发给RNN\n",
    "packed_seq = pack_sequence([torch.Tensor(i) for i in [sa, sb, sc]])\n",
    "print(packed_seq)\n",
    "# 但是packed_seq默认是seq_first的，所以前面的rnn要重新来\n",
    "rnn = nn.RNN(1, 3)\n",
    "rnn(packed_seq)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492cfb3-717b-455e-a0c2-ea99ea3fa15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# 下面试试股票数据处理 #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5bc6cd-84a8-4042-90de-0ddad98d3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d3500d-7c41-429b-907e-f82cf2430061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "dataset = pd.read_csv(\"000600.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date','prediction'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset['updown'] = dataset['updown']\n",
    "print(dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80098777-883e-4103-ba71-26a85e060e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据分成训练数据和测试数据\n",
    "# print(dataset)\n",
    "n_train = 120\n",
    "train = dataset.iloc[:n_train, :].reset_index(drop=True)\n",
    "test  = dataset.iloc[n_train:, :].reset_index(drop=True)\n",
    "\n",
    "train_x, train_y = train.iloc[:, 1:], train.iloc[:, 0]\n",
    "test_x, test_y = test.iloc[:, 1:], test.iloc[:, 0]\n",
    "\n",
    "train_y = np.array(train_y).reshape(-1, 1)\n",
    "test_y = np.array(test_y).reshape(-1, 1)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "# print(train_x.head())\n",
    "# print(test_y)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb922a-6a93-4c67-b51c-622dee71748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据转换成 tesnsor\n",
    "train_x = torch.tensor(np.array(train_x))\n",
    "train_y = torch.tensor(np.array(train_y))\n",
    "test_x = torch.tensor(np.array(test_x))\n",
    "test_y = torch.tensor(np.array(test_y))\n",
    "# print(train_x.shape)\n",
    "# print(train_y.shape)\n",
    "# print(test_x.shape)\n",
    "# print(test_y.shape)\n",
    "\n",
    "# 数据转成 batch 的格式\n",
    "batch_count = 8               # 把训练数据切割成 batch_count 个batch\n",
    "batch_size = 15               # 每个batch里面有 batch_size 个数据\n",
    "\n",
    "# 把训练数据分成 8 个batch，每个 15 个数据\n",
    "train_x = train_x.reshape(batch_count, batch_size, 117)\n",
    "train_y = train_y.reshape(batch_count, batch_size, 1)\n",
    "# 把测试数据分成 2 个batch，每个 batch 有 15 个数据\n",
    "test_x = test_x.reshape(2, 15, 117)\n",
    "test_y = test_y.reshape(2, 15, 1)\n",
    "\n",
    "print(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b1054-e3bc-45ad-bba6-96f98d3e0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始定义 RNN 模型\n",
    "TIME_STEP = 15\n",
    "INPUT_SIZE = 117\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "                input_size = INPUT_SIZE,                    # 输入 x 的feature 维度\n",
    "                hidden_size = HIDDEN_SIZE,                  # 隐状态 hidden_state 中的feature维度\n",
    "                num_layers = 5,                             # RNN 的层数\n",
    "                nonlinearity='relu',                        # 指定激活函数 [‘tanh’ | ’relu’]. 默认: ‘tanh’\n",
    "                bias = True,                                # 如果是 False , 那么 RNN 层就不会使用偏置权重 b_ih 和 b_hh, 默认: True\n",
    "                batch_first = True,                         # 如果 True, 输入Tensor的shape应该是(batch, seq, features),并且输出也是一样.\n",
    "                dropout = 0,                                # 如果值非零, 那么除了最后一层外, 其它层的输出都会套上一个 dropout 层\n",
    "                bidirectional = False                       # 如果 True , 将会变成一个双向 RNN, 默认为 False\n",
    "        )\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        \n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        outs = []\n",
    "        for time_step in range(TIME_STEP):\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        return torch.stack(outs, dim=1), h_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2877e5f-6cb4-4851-a07b-0a1c121e9641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始化 RNN 模型\n",
    "rnn = RNN()\n",
    "LR = 0.01\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR, weight_decay=1e-7)\n",
    "\n",
    "i = 0\n",
    "h_state = None\n",
    "\n",
    "# 训练 RNN 模型\n",
    "for step in range(800):\n",
    "    i = i+1\n",
    "    rnn = rnn.double()\n",
    "    pred, h_state = rnn(train_x, h_state)\n",
    "    h_state = h_state.data\n",
    "    loss = loss_func(pred, train_y)\n",
    "    if (i+1)%50 == 0:\n",
    "        print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de7088-2b08-4373-a294-6ddaa5a43455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用模型来做测试数据的误差\n",
    "h_state = None\n",
    "pred, h_state = rnn(test_x, h_state)\n",
    "loss = loss_func(pred, test_y)\n",
    "print(loss)\n",
    "print(pred)\n",
    "actual_line = np.array(test_y).flatten().tolist()\n",
    "pred_line = pred.data.numpy().flatten()\n",
    "num = list(range(30))\n",
    "plt.plot(num, actual_line, 'r-')\n",
    "plt.plot(num, pred_line, 'b-')\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125af7c-b15b-4ebb-ae0d-bf496563f44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
