{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c79d9768-347c-4f76-b310-7ccf808a033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是用RNN模型将sin的输入转换为cos的输出\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f2eb4441-d214-4e9b-b69b-4282392f0355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1124a2e28>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_STEP = 10                               # rnn 定义的时间步数\n",
    "INPUT_SIZE = 1                              # 因为输入的是1个数字，也就是1个feature\n",
    "LR = 0.01                                   # learning rate\n",
    "HIDDEN_SIZE = 64                            # 隐藏层的特征数量\n",
    "torch.manual_seed(1)                        # 随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "26e8c479-86a0-4883-828f-a5e494aace11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):                                   # hidden = np.tanh(np.dot(self.W_hh, hidden) + np.dot(self.W_xh, x))\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(                              # 自己定义一个RNN神经网络\n",
    "            input_size = INPUT_SIZE,                    # 输入 x 的feature 维度\n",
    "            hidden_size = HIDDEN_SIZE,                  # 隐状态 hidden_state 中的feature维度\n",
    "            num_layers = 5,                             # RNN 的层数\n",
    "            nonlinearity='relu',                        # 指定激活函数 [‘tanh’ | ’relu’]. 默认: ‘tanh’\n",
    "            bias = True,                                # 如果是 False , 那么 RNN 层就不会使用偏置权重 b_ih 和 b_hh, 默认: True\n",
    "            batch_first = True,                         # 如果 True, 输入Tensor的shape应该是(batch, seq, features),并且输出也是一样.\n",
    "            dropout = 0,                                # 如果值非零, 那么除了最后一层外, 其它层的输出都会套上一个 dropout 层\n",
    "            bidirectional = False                       # 如果 True , 将会变成一个双向 RNN, 默认为 False\n",
    "        )\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, 1)            # 定义一个输出层，这是RNN的最后输出，只用输出output_vector\n",
    "\n",
    "    def forward(self, x, h_state):                      # 这就是RNN每次输入的参数x和h\n",
    "        # x (batch, time_step, input_size)              # 这是RNN的ｘ的维度            (批量, 序列长度, 输入的特征维度）\n",
    "        # h_state (n_layers, batch, hidden_size)        # 这是hidden_state的维度       (层数×方向, 批量, 输出的特征维度）/*方向：单向是１；双向是２*/\n",
    "        # r_out (batch, time_step, hidden_size)         # 这是网络实际输出的r_out的维度 (批量，序列长度，输出的特征维度X方向）\n",
    "        r_out, h_state = self.rnn(x, h_state)           # RNN每次输入x, hidden_state; 输出r_out, hidden_state;\n",
    "        outs = []\n",
    "        for time_step in range(r_out.size(1)):\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        return torch.stack(outs, dim=1), h_state        # RNN的forward输出了output_vector, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6d4d19f0-29ba-44b5-bbba-7a7453844c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR, weight_decay=1e-6)    # 定义了优化函数ADAM对RNN的参数反向优化，定义了 learning rate 和 权重衰减率\n",
    "loss_func = nn.MSELoss()                                                    # 定义了模型的损失函数用 mean-square-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f6dcca70-71cf-4d7f-95ea-bbc063970f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0  loss:0.616753757\n",
      "step:20  loss:0.599726737\n",
      "step:40  loss:0.598618388\n",
      "step:60  loss:0.614574015\n",
      "step:80  loss:0.597458482\n",
      "step:100  loss:0.600382805\n",
      "step:120  loss:0.597702026\n",
      "step:140  loss:0.937314034\n",
      "step:160  loss:0.600224555\n",
      "step:180  loss:0.599694431\n",
      "step:200  loss:0.590930045\n",
      "step:220  loss:0.094163097\n",
      "step:240  loss:0.582085848\n",
      "step:260  loss:0.578835607\n",
      "step:280  loss:0.589287341\n",
      "step:300  loss:0.729984403\n",
      "step:320  loss:0.597867250\n",
      "step:340  loss:0.536698222\n",
      "step:360  loss:0.505166292\n",
      "step:380  loss:0.534887910\n",
      "step:400  loss:35.706615448\n",
      "step:420  loss:0.609800935\n",
      "step:440  loss:0.600955725\n",
      "step:460  loss:0.599595129\n",
      "step:480  loss:0.600089133\n",
      "step:500  loss:0.600345314\n",
      "step:520  loss:0.602997720\n",
      "step:540  loss:0.602400184\n",
      "step:560  loss:0.601933479\n",
      "step:580  loss:0.601537406\n",
      "step:600  loss:0.601230443\n",
      "step:620  loss:0.600997031\n",
      "step:640  loss:0.600822091\n",
      "step:660  loss:0.600693882\n",
      "step:680  loss:0.600604296\n",
      "step:700  loss:0.600492239\n",
      "step:720  loss:2.285447359\n",
      "step:740  loss:0.721879363\n",
      "step:760  loss:0.610277176\n",
      "step:780  loss:0.629911065\n",
      "step:800  loss:0.604597270\n",
      "step:820  loss:0.604400277\n",
      "step:840  loss:0.447923094\n",
      "step:860  loss:0.223692492\n",
      "step:880  loss:0.211035416\n",
      "step:900  loss:6.988791466\n",
      "step:920  loss:0.164445505\n",
      "step:940  loss:0.765079618\n",
      "step:960  loss:0.240110829\n",
      "step:980  loss:0.094379984\n",
      "step:1000  loss:0.074454978\n",
      "step:1020  loss:0.089155726\n",
      "step:1040  loss:0.054146111\n",
      "step:1060  loss:0.054339647\n",
      "step:1080  loss:0.049799632\n",
      "step:1100  loss:0.045222241\n",
      "step:1120  loss:0.040572874\n",
      "step:1140  loss:0.036645211\n",
      "step:1160  loss:0.032169033\n",
      "step:1180  loss:0.027553458\n",
      "step:1200  loss:0.018533474\n",
      "step:1220  loss:0.013316480\n",
      "step:1240  loss:0.005987305\n",
      "step:1260  loss:0.003754137\n",
      "step:1280  loss:0.003038892\n",
      "step:1300  loss:0.001233196\n",
      "step:1320  loss:0.007575143\n",
      "step:1340  loss:0.007729287\n",
      "step:1360  loss:0.004195057\n",
      "step:1380  loss:0.852181315\n",
      "step:1400  loss:0.033068143\n",
      "step:1420  loss:0.001011028\n",
      "step:1440  loss:0.000368611\n",
      "step:1460  loss:0.000041177\n",
      "step:1480  loss:0.000382410\n",
      "step:1500  loss:0.000497294\n",
      "step:1520  loss:0.000196195\n",
      "step:1540  loss:0.001530558\n",
      "step:1560  loss:0.000007572\n",
      "step:1580  loss:0.002693258\n",
      "step:1600  loss:0.000526988\n",
      "step:1620  loss:0.002756933\n",
      "step:1640  loss:0.001788675\n",
      "step:1660  loss:0.001208461\n",
      "step:1680  loss:0.003041540\n",
      "step:1700  loss:0.000213963\n",
      "step:1720  loss:0.000709375\n",
      "step:1740  loss:0.000509458\n",
      "step:1760  loss:0.000014579\n",
      "step:1780  loss:0.000001319\n",
      "step:1800  loss:0.000001049\n",
      "step:1820  loss:0.000000150\n",
      "step:1840  loss:0.000000004\n",
      "step:1860  loss:0.000000001\n",
      "step:1880  loss:0.000000000\n",
      "step:1900  loss:0.000000000\n",
      "step:1920  loss:0.000000000\n",
      "step:1940  loss:0.000000000\n",
      "step:1960  loss:0.000000000\n",
      "step:1980  loss:0.000000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "h_state = None                                                          # 起始时输入给RNN的hidden_state就是None\n",
    "\n",
    "for step in range(2000):                                                 # 计算２００次。相当于２００个顺序的时间片数据丢进去计算\n",
    "    start, end = step*np.pi, (step+1)*np.pi                             # 设计一小段数据起始点\n",
    "    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32)        # 生成一小段数据\n",
    "    x_np = np.sin(steps)                                                # 这就是用来输入的数据\n",
    "    y_np = np.cos(steps)                                                # 这就是需要被预测的数据\n",
    "    \n",
    "    # x = Variable(torch.from_numpy(x_np[np.newaxis, :, np.newaxis]))     # x_np[np.newaxis, :, np.nexaxis]这句将x_np从1维(n)转化成3维(1,n,1);然后成为输入变量x\n",
    "    # y = Variable(torch.from_numpy(y_np[np.newaxis, :, np.newaxis]))     # y_np[np.newaxis, :, np.nexaxis]这句将y_np从1维(n)转化成3维(1,n,1);然后成为真实值 y\n",
    "    # print(x.size())\n",
    "    # print(y.size())\n",
    "    \n",
    "    prediction, h_state = rnn(x, h_state)                               # 这就是一次RNN模型forward()得出来的结果\n",
    "    h_state = Variable(h_state.data)                                    # 把h_state(tensor)中的数据取出来,给下一轮的rnn作为数据输入\n",
    "\n",
    "    loss = loss_func(prediction, y)                                     # 用损失函数计算误差\n",
    "    optimizer.zero_grad()                                               # 清空优化器梯度\n",
    "    loss.backward()                                                     # 反向传播求解梯度\n",
    "    optimizer.step()                                                    # 更新权重的参数\n",
    "    \n",
    "    if step%20==0:\n",
    "        print(\"step:{}  loss:{:.9f}\".format(step, loss.data.float()))\n",
    "\n",
    "#     plt.ion()                                                           # 把实际图和预测图动态打印出来\n",
    "#     plt.plot(steps, y_np, color='b')\n",
    "#     plt.plot(steps, np.squeeze(prediction.data.numpy()), color='r')\n",
    "#     plt.show()\n",
    "#     plt.pause(0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15a06a-9198-4253-9099-ff1bb02db5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ad0bd-8093-4a2b-8b69-92fe3894ba7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
