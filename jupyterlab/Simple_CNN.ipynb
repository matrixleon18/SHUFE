{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839a013-5d7d-4340-a0ff-d171c069de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fa601-18e7-409d-a227-09ef9bd9e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是最简单的CNN模型，有4做卷积的层和1个做全连接的层。\n",
    "# 做卷积的层都是：卷积层 + 激活层 + 池化层\n",
    "# 做全连接的层用了3个全连接，把维度从 256-> 512 -> 64 -> 10\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential()                                         # size = 3 * 32 * 32\n",
    "        self.layer1.add_module('conv1', nn.Conv2d(3, 32, 3, 1, padding=1))    # in_channel, out_channel, kernel_size, stride, padding\n",
    "        self.layer1.add_module('relu1', nn.ReLU(True))\n",
    "        self.layer1.add_module('pool1', nn.MaxPool2d(2,2))                    # size = 32 * 16 * 16\n",
    "        \n",
    "        self.layer2 = nn.Sequential()\n",
    "        self.layer2.add_module('conv2', nn.Conv2d(32, 64, 3, 1, padding=1))\n",
    "        self.layer2.add_module('relu2', nn.ReLU(True))\n",
    "        self.layer2.add_module('pool2', nn.MaxPool2d(2,2))                    # size = 64 * 8 * 8\n",
    "        \n",
    "        self.layer3 = nn.Sequential()\n",
    "        self.layer3.add_module('conv3', nn.Conv2d(64, 128, 3, 1, padding=1))\n",
    "        self.layer3.add_module('pool3', nn.MaxPool2d(True))\n",
    "        self.layer3.add_module('relu3', nn.ReLU(True))                        # size = 128 * 4 * 4\n",
    "        \n",
    "        self.layer4 = nn.Sequential()\n",
    "        self.layer4.add_module('conv4', nn.Conv2d(128, 256, 3, 1, padding=1))\n",
    "        self.layer4.add_module('pool4', nn.MaxPool2d(True))\n",
    "        self.layer4.add_module('relu4', nn.ReLU(True))                        # size = 256 * 2 * 2\n",
    "        \n",
    "        self.layer5 = nn.Sequential()\n",
    "        self.layer5.add_module('fc1', nn.Linear(256*2*2, 512))\n",
    "        self.layer5.add_module('fc_relu1', nn.ReLU(True))\n",
    "        self.layer5.add_module('fc2', nn.Linear(512, 64))\n",
    "        self.layer5.add_module('fc_relu2', nn.ReLU(True))\n",
    "        self.layer5.add_module('fc3', nn.Linear(64, 10))\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data)\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.fill_(0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv1 = self.layer1(x)\n",
    "        conv2 = self.layer2(conv1)\n",
    "        conv3 = self.layer3(conv2)\n",
    "        conv4 = self.layer4(conv3)\n",
    "        fc_input = conv4.view(conv4.size(0), -1)                            # 把数据展开成一维的\n",
    "        fc_out = self.layer5(fc_input)\n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9bbd4-d04e-4dee-a921-54a8f6087943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化一个模型\n",
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f2d3e5-2748-414a-bd13-b2118f125862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下可以列出模型的具体组成\n",
    "print(model)\n",
    "print(model.modules)\n",
    "print(model.children)\n",
    "print(model.named_children)\n",
    "print(model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b622a-9955-4223-b7a0-360cd331f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下可以列出模型的前两层\n",
    "new_model = nn.Sequential(*list(model.children())[:2])\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1986e-c954-4f08-842e-a300749430fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把模型中属于Conv2d的层列出来。不过新版本的层名不支持. 所以要换一下\n",
    "conv_model = nn.Sequential()\n",
    "for layer in model.named_modules():\n",
    "    if isinstance(layer[1], nn.Conv2d):\n",
    "        conv_model.add_module(layer[0].replace('.','-'), layer[1])\n",
    "print(conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871aed5-f1ef-47aa-a37c-b471f63e580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把模型中所有参数名打印出来\n",
    "for param in model.named_parameters():\n",
    "    print(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6861fe-224f-4489-8d3b-8675c0ffdc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对权重做初始化\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99805b12-83a3-45de-ae19-1664b515df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(m)\n",
    "        print(m.weight.shape)\n",
    "        print(m.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d54f2877-cf8d-42f8-a65d-674be3342fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是最简单的CNN模型，有4做卷积的层和1个做全连接的层。\n",
    "# 做卷积的层都是：卷积层 + 激活层 + 池化层\n",
    "# 做全连接的层用了3个全连接，把维度从 256-> 512 -> 64 -> 10\n",
    "class MNISTCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )   \n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128*7*7, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data)\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.fill_(0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)                                  # 16 * 28 * 28\n",
    "        x = self.layer2(x)                                  # 32 * 14 * 14\n",
    "        x = self.layer3(x)                                  # 64 * 14 * 14\n",
    "        x = self.layer4(x)                                  # 128 * 7 * 7\n",
    "        x = x.view(x.size(0), -1)                            # 把数据展开成一维的\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "216c9874-377c-4c4a-a73a-83cecfe2c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始准备数据\n",
    "batch_size = 64\n",
    "lr = 1e-2\n",
    "num_epoches = 10\n",
    "\n",
    "data_tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5],[0.5])])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=data_tf, download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=data_tf)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = MNISTCNN()\n",
    "# model = Batch_Net(28*28, 300, 100, 10)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "opti_func = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3afabc46-98b4-428a-87ba-c524c95db1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.05323347449302673\n",
      "train loss: 0.024843575432896614\n",
      "train loss: 0.10679031908512115\n",
      "train loss: 0.004946841858327389\n",
      "train loss: 7.729105709586293e-05\n",
      "train loss: 0.019284864887595177\n",
      "train loss: 0.059963155537843704\n",
      "train loss: 0.00047266847104765475\n",
      "train loss: 0.05882607400417328\n",
      "train loss: 0.0003062101313844323\n",
      "[tensor(0.0532), tensor(0.0248), tensor(0.1068), tensor(0.0049), tensor(7.7291e-05), tensor(0.0193), tensor(0.0600), tensor(0.0005), tensor(0.0588), tensor(0.0003)]\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "train_loss = list(range(num_epoches))\n",
    "for epoch in range(num_epoches):\n",
    "    train_loss[epoch] = 0\n",
    "    for train_data in train_loader:\n",
    "        img, label = train_data\n",
    "        # img = img.view(img.size(0), -1)\n",
    "        img = Variable(img)\n",
    "        label = Variable(label)\n",
    "        ##### forward #####\n",
    "        out = model(img)\n",
    "        loss = loss_func(out, label)\n",
    "        ##### backward ###\n",
    "        opti_func.zero_grad()\n",
    "        loss.backward()\n",
    "        opti_func.step()\n",
    "    print(\"train loss: {}\".format(loss))\n",
    "    train_loss[epoch] = loss.data\n",
    "print(train_loss)\n",
    "    \n",
    "# Simple_Net      : train loss: 0.49769243597984314\n",
    "# Activation_Net  : train loss: 0.025277957320213318\n",
    "# Batch_Net.      : train loss: 0.05666544288396835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "868b7f93-4cf5-4025-8dd1-fa85339392c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.060916, Acc: 0.988600\n"
     ]
    }
   ],
   "source": [
    "# 开始测试\n",
    "model.eval()\n",
    "eval_loss = 0\n",
    "eval_acc = 0\n",
    "\n",
    "for data in test_loader:\n",
    "    img, label = data\n",
    "    # img = img.view(img.size(0), -1)\n",
    "    img = Variable(img)\n",
    "    label = Variable(label)\n",
    "    \n",
    "    out = model(img)\n",
    "    loss = loss_func(out, label)\n",
    "    eval_loss += loss.data * label.size(0)\n",
    "    _, pred = torch.max(out, 1)\n",
    "    num_correct = (pred==label).sum()\n",
    "    eval_acc += num_correct.data\n",
    "\n",
    "print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss/(len(test_dataset)), eval_acc/(len(test_dataset))))\n",
    "\n",
    "# Simple_Net      : Test Loss: 0.505879, Acc: 0.849400\n",
    "# Activation_Net  : Test Loss: 0.207220, Acc: 0.950500\n",
    "# Batch_Net       : Test Loss: 0.087046, Acc: 0.977900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632221a-5311-4706-a026-36adde0c47b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
