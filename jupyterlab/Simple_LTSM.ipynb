{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84918cd0-ce3b-4221-a645-c15527573f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n",
      "torch.Size([2, 3, 20])\n",
      "torch.Size([2, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 这是最简单的 LTSM 模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 初始化的 lstm 模型\n",
    "lstm_model = nn.LSTM(10, 20, 2)                    # 参数为(input_size, hidden_size, num_layers)\n",
    "\n",
    "# 模型的cell接受的输入参数\n",
    "x = torch.randn(5, 3, 10)                         # 参数为(seq_len, batch, input_size)\n",
    "# 模型的cell接受的输入hidden_state；也是前一个cell的输出hidden_state\n",
    "h0 = torch.randn(2, 3, 20)                         # 参数为(num_layers * num_directions, batch, hidden_size)\n",
    "# 模型的cell初始值\n",
    "c0 = torch.randn(2, 3, 20)                         # 参数为(num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "y, (h1, c1) = lstm_model(x, (h0, c0))\n",
    "\n",
    "print(y.size())\n",
    "print(h1.size())\n",
    "print(c1.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ef4ef8-5b74-4fbf-bc94-13ab3833fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 做股票预测\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93673132-3006-4021-a0cd-6ca155ab1a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 118)\n",
      "rolling_data: (130, 21, 118)\n",
      "seq count: 130\n",
      "seq length: 21\n",
      "total batch count: 65\n",
      "batch size: 2\n",
      "rolling_data: torch.Size([65, 2, 21, 118])\n",
      "train_x: torch.Size([64, 2, 21, 117])\n",
      "train_y: torch.Size([64, 2, 21, 1])\n",
      "test_x:  torch.Size([1, 2, 21, 117])\n",
      "test_y:  torch.Size([1, 2, 21, 1])\n",
      "train_batch_count: 64\n",
      "test_batch_count:  1\n"
     ]
    }
   ],
   "source": [
    "# GPU -- 准备来做一个GPU版本的RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"000600.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date','prediction'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset['updown'] = dataset['updown']\n",
    "print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "# 方法2，将数据按照batch_size的窗口进行滑动，每个窗口数据做一组\n",
    "\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "seq_len = 21\n",
    "batch_size = 2                                                    # 注意：batch_size是要能够整除seq_count的\n",
    "\n",
    "# 把数据切换成 batch_size 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(seq_len):\n",
    "    if i.shape[0] == seq_len:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, seq_len, 118)                 # 数据一共是 seq_count x seq_len x in_dim\n",
    "\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                 # 所以一共有 seq_count 列数据，每一行的数据是118维 （包括y）\n",
    "print(\"seq length: {}\".format(seq_len))\n",
    "\n",
    "\n",
    "total_batch_count = int(rolling_data.shape[0]/batch_size)                          # 把数据规划成 batch_count 个 batch\n",
    "\n",
    "print(\"total batch count: {}\".format(total_batch_count))\n",
    "print(\"batch size: {}\".format(batch_size))\n",
    "\n",
    "rolling_data = rolling_data.reshape(total_batch_count, batch_size, seq_len, 118)  # 把数据转成 total_batch_count x batch_size x seq_len x in_dim 格式\n",
    "rolling_data = torch.tensor(rolling_data)\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "\n",
    "\n",
    "train_batch_count = total_batch_count - 1\n",
    "test_batch_count = total_batch_count - train_batch_count\n",
    "\n",
    "train = rolling_data[:train_batch_count, :, :, :]\n",
    "test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,1:], train[:,:,:,0:1]\n",
    "test_x,  test_y  = test[:,:,:, 1:],  test[:,:,:,0:1]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train_batch_count))\n",
    "print(\"test_batch_count:  {}\".format(test_batch_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89afd6a6-b863-43bc-8884-34c3b669701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始定义 LSTM 模型\n",
    "TIME_STEP = seq_len                                        # 一般这个单独设定，这里为了简单，还是直接就等于seq_len的方便。其实也就是等于最长的那个sequence length\n",
    "INPUT_SIZE = 117\n",
    "HIDDEN_SIZE = 768\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "                input_size = INPUT_SIZE,                    # 输入 x 的feature 维度\n",
    "                hidden_size = HIDDEN_SIZE,                  # 隐状态 hidden_state 中的feature维度\n",
    "                num_layers = 5,                             # RNN 的层数\n",
    "                nonlinearity='relu',                        # 指定激活函数 [‘tanh’ | ’relu’]. 默认: ‘tanh’\n",
    "                bias = True,                                # 如果是 False , 那么 RNN 层就不会使用偏置权重 b_ih 和 b_hh, 默认: True\n",
    "                batch_first = True,                         # 如果 True, 输入Tensor的shape应该是(batch_size, seq_len, features),并且输出也是一样.\n",
    "                dropout = 0.1,                                # 如果值非零, 那么除了最后一层外, 其它层的输出都会套上一个 dropout 层\n",
    "                bidirectional = False                       # 如果 True , 将会变成一个双向 RNN, 默认为 False\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        \n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        outs = []\n",
    "        for time_step in range(TIME_STEP):\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        return torch.stack(outs, dim=1), h_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
