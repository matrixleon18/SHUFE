{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7635ca9d-4939-4a35-a263-2feced7b671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 LSTM 做一个 Seq2Seq 的预测，不考虑准确性\n",
    "## Architecture \n",
    "## General:        [x1, x2, ... , xn]   ==> [Seq2Seq Model] ==> [Prediction1, Prediction2] \n",
    "## Genral Data:    [122-D, ... , 122-D] ==> [Seq2Seq Model] ==> [1-D, 1-D] \n",
    "## Detail :        [x1, x2, ... , xn]   ==> [Encoder] ==> [h_n, c_n] + [xn]    ==> [Decoder] ==> [Prediont1, Prediction2]\n",
    "## Detail Data:    [122-D, ...., 122-D] ==> [Encoder] ==> [h_n, c_n] + [122-D] ==> [Decoder] ==> [1-D, 1-D]\n",
    "## Decoder :       xn, [h_n, c_n] ==> FC(xn), [h_n, c_n] ==> [1-D, h_n, c_n] ==> LSTM(1-D, h_n, c_n) ==> pred1, h1, c1 ==> LSTM(pred1, h, c) ==> pred2, h2, c2 ==> LSTM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13727f9-0a47-458a-ade9-686a82621b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a209f559-0c6c-410d-8fe3-d8174aabdfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1027)\n",
    "torch.manual_seed(1027)\n",
    "torch.cuda.manual_seed(1027)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374dd4d1-e07f-4caa-b61b-70f32e1ba608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "dataset = dataset.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17c9777-7fb2-461b-8ecc-b33f83fc0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照 Seq2Seq 方法生成 y 所需要的第二列数据\n",
    "dataset.insert(1, 'future2',dataset.future)\n",
    "dataset['future2'] = dataset['future'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69455688-94fa-4811-8e98-7851928ebfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling_data shape: (601, 60, 136)\n",
      "seq count: 601\n",
      "seq length: 60\n",
      "train_x: torch.Size([6, 100, 60, 134])\n",
      "train_y: torch.Size([6, 100, 2, 1])\n",
      "test_x:  torch.Size([1, 1, 60, 134])\n",
      "test_y:  torch.Size([1, 1, 2, 1])\n",
      "train_batch_count: 6\n",
      "test_batch_count:  1\n"
     ]
    }
   ],
   "source": [
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 60\n",
    "TRAIN_BATCH_SIZE = 100                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_SIZE = 1                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_COUNT = 1\n",
    "Y_SEQ_LEN = 2                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "Y_DIM = 1\n",
    "X_DIM = dataset.shape[1]-Y_SEQ_LEN                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                   # 数据一共是 seq_count x seq_len x (x_in_dim+Y_SEQ_LEN) \n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(SEQ_LENGTH))\n",
    "# print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "test_seq_count = TEST_BATCH_COUNT * TEST_BATCH_SIZE\n",
    "\n",
    "\n",
    "# train = rolling_data[:-test_seq_count].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "# test  = rolling_data[-test_seq_count:].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "train = rolling_data[:-test_seq_count].reshape(-1, TRAIN_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                    # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "test  = rolling_data[-test_seq_count:].reshape(-1, TEST_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)      # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "TRAIN_BATCH_SIZE = train.shape[1]\n",
    "TRAIN_BATCH_COUNT = train.shape[0]\n",
    "TEST_BATCH_SIZE = test.shape[1]\n",
    "TEST_BATCH_COUNT = test.shape[0]\n",
    "\n",
    "train = torch.tensor(train)\n",
    "test  = torch.tensor(test)\n",
    "\n",
    "# train = rolling_data[:train_batch_count, :, :, :]\n",
    "# test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_SEQ_LEN:], train[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_SEQ_LEN:],  test[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y_seq_len]  to  [test_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train.shape[0]))\n",
    "print(\"test_batch_count:  {}\".format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ac5928d-7984-49aa-b1ae-57e902a703f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder & Decoder class\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(input_size=self.input_dim, hidden_size = self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        # print(\"Encoder self.input_dim  : {}\".format(self.input_dim))\n",
    "        # print(\"Encoder self.hidden_dim  : {}\".format(self.hidden_dim))\n",
    "        self.init_weights2()\n",
    "\n",
    "    def init_weights2(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)    \n",
    "                \n",
    "    def forward(self, x):\n",
    "        # print(\"Encoder forward() x: {}\".format(x.shape))\n",
    "        x = self.dropout(x)\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "        # print(\"Encoder outputs :{}\".format(outputs.shape))\n",
    "        # print(\"Encoder h_n     :{}\".format(h_n.shape))\n",
    "        # print(\"Encoder c_n     :{}\".format(c_n.shape))\n",
    "        return outputs, h_n, c_n\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.fc_in = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.init_weights2()\n",
    "\n",
    "    def init_weights2(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)        \n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input : input batch data, size(input): [batch_size, feature_size]\n",
    "        # notice input only has two dimensions since the input is batchs\n",
    "        # of last coordinate of observed trajectory so the sequence length has been removed.\n",
    "        \n",
    "        # add sequence dimension to input, to allow use of nn.LSTM\n",
    "        # print(\"Decoder forward() input size : {}\".format(input.shape))\n",
    "        # print(\"Decoder forward() hidden size: {}\".format(hidden.shape))\n",
    "        # print(\"Decoder forward() cell size  : {}\".format(cell.shape))\n",
    "        \n",
    "        input = self.fc_in(input)\n",
    "\n",
    "        lstm_output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        \n",
    "        # print(\"Decoder forward() lstm_output: {}\".format(lstm_output.shape))\n",
    "        \n",
    "        prediction = self.fc_out(lstm_output)         # prediction is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80326ab-dd8a-4bdb-96e4-a3480314b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model class\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.decoder_fc_init= nn.Linear(encoder.input_dim, decoder.input_dim)\n",
    "        # self.decoder_fc_input= nn.Linear(decoder.output_dim, decoder.input_dim)\n",
    "        \n",
    "        assert (encoder.hidden_dim == decoder.hidden_dim), \"hidden dimension in encoder and decoder must be equal\"       \n",
    "        assert (encoder.num_layers == decoder.num_layers), \"hidden layer numbers in encoder and decoder must be equal\"\n",
    "        \n",
    "            \n",
    "    def forward(self, x, y, teacher_forcing_ratio = 0.5):\n",
    "        # x is the input to the encoder.\n",
    "        # y is the output from the decoder\n",
    "        # x = [batch size, encoder_in_sequence_len,  encoder_in_dim]               encoder_in_sequence_len=60, encoder_in_dim=122\n",
    "        # y = [batch size, decoder_out_sequence_len, decoder_out_dim]              decoder_out_sequence_len=2, decoder_out_dim=1    \n",
    "        # print(\"Seq2Seq forwar() x shape : {}\".format(x.shape))\n",
    "        # print(\"Seq2Seq forwar() y shape : {}\".format(y.shape))\n",
    "\n",
    "        decoder_out_seq_len = y.shape[1]                                                     # This is most important that define the output length\n",
    "        \n",
    "        # tensor to store decoder outputs of each time step; this outputs will calc loss with y, so its shape is same as y\n",
    "        outputs = torch.zeros(y.shape).to(device)\n",
    "        # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))\n",
    "        \n",
    "        _, hidden, cell = self.encoder(x)\n",
    "        # print(\"encoder hidden shape: {}\".format(hidden.shape))                            # [encoder_hidden_layer_number, batch_size, encoder_hidden_dim]\n",
    "        # print(\"encoder cell shape :  {}\".format(cell.shape))                              # [encoder_hidden_layer_number, batch_size, encoder_hidden_dim]\n",
    "        \n",
    "        # first input to decoder may be last coordinates of x to predict the future: [last_x]+[h_n,c_n] --> [model] --> [future_y]\n",
    "        decoder_input = x[:, -1, :]                                                           # [batch_size, encoder_input_dim] Get last elements of sequences of the batch from input x\n",
    "        decoder_input = decoder_input.unsqueeze(1)                                            # [batch_size, 1, encoder_input_dim] Get last element of sequence of the batch in encoder\n",
    "        # print(\"decoder_input: {}\".format(decoder_input.shape))\n",
    "        decoder_input = self.decoder_fc_init(decoder_input)                                   # [batch_size, 1, decoder_input_dim] Conver to 1st element of sequence of the batch in decoder\n",
    "        # print(\"decoder_input: {}\".format(decoder_input.shape))\n",
    "        \n",
    "        # Becasue the input and target have different sequence length, Get the target prediction one by one [Prev_prediction]+[h_n,c_n] --> [model] --> [Prediction]\n",
    "        for i in range(decoder_out_seq_len):\n",
    "            # run the decoder for one time step\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            # print(\"Seq2Seq forward() output shape: {}\".format(output.shape))\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each time step\n",
    "            outputs[:,i,:] = output[:,0]\n",
    "            # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))            \n",
    "\n",
    "            # assign this prediction as next prediction's input\n",
    "            decoder_input = output\n",
    "            # print(\"Seq2Seq forward() decoder_input shape: {}\".format(output.shape))\n",
    "            \n",
    "            # 或者使用teacher_forcing来优化\n",
    "            teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            decoder_input = y[:, i, :].unsqueeze(1) if teacher_forcing else output\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f9dd074-51e0-475f-a64c-f817959b185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "DEC_INPUT_DIM   = 1\n",
    "HIDDEN_DIM      = 768\n",
    "NUM_LAYERS      = 2\n",
    "ENC_DROPOUT     = 0.2\n",
    "DEC_DROPOUT     = 0.2\n",
    "\n",
    "encoder = Encoder(input_dim=X_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, dropout=ENC_DROPOUT)\n",
    "decoder = Decoder(input_dim=DEC_INPUT_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, output_dim=Y_DIM, dropout=DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).double().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7692e6fe-3005-40a5-8f3e-01cfaeed148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 200 epoch loss:   0.2449   with  lr:   0.0001\n",
      "1 of 200 epoch loss:   0.2166   with  lr:   0.0001\n",
      "2 of 200 epoch loss:   0.1061   with  lr:   0.0001\n",
      "3 of 200 epoch loss:   0.0799   with  lr:   0.0001\n",
      "4 of 200 epoch loss:   0.0707   with  lr:   0.0001\n",
      "5 of 200 epoch loss:   0.0615   with  lr:   0.0001\n",
      "6 of 200 epoch loss:   0.0539   with  lr:   0.0001\n",
      "7 of 200 epoch loss:   0.0525   with  lr:   0.0001\n",
      "8 of 200 epoch loss:   0.0508   with  lr:   0.0001\n",
      "9 of 200 epoch loss:   0.0497   with  lr:   0.0001\n",
      "10 of 200 epoch loss:   0.0476   with  lr:   0.0001\n",
      "11 of 200 epoch loss:   0.0475   with  lr:   0.0001\n",
      "12 of 200 epoch loss:   0.0466   with  lr:   0.0001\n",
      "13 of 200 epoch loss:   0.0468   with  lr:   0.0001\n",
      "14 of 200 epoch loss:   0.0459   with  lr:   0.0001\n",
      "15 of 200 epoch loss:   0.0452   with  lr:   0.0001\n",
      "16 of 200 epoch loss:   0.0455   with  lr:   0.0001\n",
      "17 of 200 epoch loss:   0.0449   with  lr:   0.0001\n",
      "18 of 200 epoch loss:   0.0447   with  lr:   0.0001\n",
      "19 of 200 epoch loss:   0.0450   with  lr:   0.0001\n",
      "20 of 200 epoch loss:   0.0436   with  lr:   0.0001\n",
      "21 of 200 epoch loss:   0.0431   with  lr:   0.0001\n",
      "22 of 200 epoch loss:   0.0434   with  lr:   0.0001\n",
      "23 of 200 epoch loss:   0.0435   with  lr:   0.0001\n",
      "24 of 200 epoch loss:   0.0422   with  lr:   0.0001\n",
      "25 of 200 epoch loss:   0.0416   with  lr:   0.0001\n",
      "26 of 200 epoch loss:   0.0417   with  lr:   0.0001\n",
      "27 of 200 epoch loss:   0.0410   with  lr:   0.0001\n",
      "28 of 200 epoch loss:   0.0422   with  lr:   0.0001\n",
      "29 of 200 epoch loss:   0.0412   with  lr:   0.0001\n",
      "30 of 200 epoch loss:   0.0411   with  lr:   0.0001\n",
      "31 of 200 epoch loss:   0.0398   with  lr:   0.0001\n",
      "32 of 200 epoch loss:   0.0400   with  lr:   0.0001\n",
      "33 of 200 epoch loss:   0.0403   with  lr:   0.0001\n",
      "34 of 200 epoch loss:   0.0397   with  lr:   0.0001\n",
      "35 of 200 epoch loss:   0.0390   with  lr:   0.0001\n",
      "36 of 200 epoch loss:   0.0384   with  lr:   0.0001\n",
      "37 of 200 epoch loss:   0.0397   with  lr:   0.0001\n",
      "38 of 200 epoch loss:   0.0407   with  lr:   0.0001\n",
      "39 of 200 epoch loss:   0.0410   with  lr:   0.0001\n",
      "40 of 200 epoch loss:   0.0448   with  lr:   0.0001\n",
      "41 of 200 epoch loss:   0.0409   with  lr:   0.0001\n",
      "42 of 200 epoch loss:   0.0471   with  lr:   0.0001\n",
      "43 of 200 epoch loss:   0.0445   with  lr:   0.0001\n",
      "44 of 200 epoch loss:   0.0446   with  lr:   0.0001\n",
      "45 of 200 epoch loss:   0.0427   with  lr:   0.0001\n",
      "46 of 200 epoch loss:   0.0401   with  lr:   0.0001\n",
      "47 of 200 epoch loss:   0.0380   with  lr:   0.0001\n",
      "48 of 200 epoch loss:   0.0371   with  lr:   0.0001\n",
      "49 of 200 epoch loss:   0.0379   with  lr:   0.0001\n",
      "50 of 200 epoch loss:   0.0367   with  lr:   0.0001\n",
      "51 of 200 epoch loss:   0.0361   with  lr:   0.0001\n",
      "52 of 200 epoch loss:   0.0363   with  lr:   0.0001\n",
      "53 of 200 epoch loss:   0.0355   with  lr:   0.0001\n",
      "54 of 200 epoch loss:   0.0355   with  lr:   0.0001\n",
      "55 of 200 epoch loss:   0.0349   with  lr:   0.0001\n",
      "56 of 200 epoch loss:   0.0351   with  lr:   0.0001\n",
      "57 of 200 epoch loss:   0.0352   with  lr:   0.0001\n",
      "58 of 200 epoch loss:   0.0348   with  lr:   0.0001\n",
      "59 of 200 epoch loss:   0.0357   with  lr:   0.0001\n",
      "60 of 200 epoch loss:   0.0348   with  lr:   0.0001\n",
      "61 of 200 epoch loss:   0.0370   with  lr:   0.0001\n",
      "62 of 200 epoch loss:   0.0366   with  lr:   0.0001\n",
      "63 of 200 epoch loss:   0.0407   with  lr:   0.0001\n",
      "64 of 200 epoch loss:   0.0377   with  lr:   0.0001\n",
      "65 of 200 epoch loss:   0.0523   with  lr:   0.0001\n",
      "66 of 200 epoch loss:   0.0457   with  lr:   0.0001\n",
      "67 of 200 epoch loss:   0.0502   with  lr:   0.0001\n",
      "68 of 200 epoch loss:   0.0397   with  lr:   0.0001\n",
      "69 of 200 epoch loss:   0.0362   with  lr:   0.0001\n",
      "70 of 200 epoch loss:   0.0350   with  lr:   0.0001\n",
      "71 of 200 epoch loss:   0.0341   with  lr:   0.0001\n",
      "72 of 200 epoch loss:   0.0331   with  lr:   0.0001\n",
      "73 of 200 epoch loss:   0.0330   with  lr:   0.0001\n",
      "74 of 200 epoch loss:   0.0331   with  lr:   0.0001\n",
      "75 of 200 epoch loss:   0.0324   with  lr:   0.0001\n",
      "76 of 200 epoch loss:   0.0324   with  lr:   0.0001\n",
      "77 of 200 epoch loss:   0.0325   with  lr:   0.0001\n",
      "78 of 200 epoch loss:   0.0325   with  lr:   0.0001\n",
      "79 of 200 epoch loss:   0.0326   with  lr:   0.0001\n",
      "80 of 200 epoch loss:   0.0320   with  lr:   0.0001\n",
      "81 of 200 epoch loss:   0.0323   with  lr:   0.0001\n",
      "82 of 200 epoch loss:   0.0326   with  lr:   0.0001\n",
      "83 of 200 epoch loss:   0.0320   with  lr:   0.0001\n",
      "84 of 200 epoch loss:   0.0321   with  lr:   0.0001\n",
      "85 of 200 epoch loss:   0.0318   with  lr:   0.0001\n",
      "86 of 200 epoch loss:   0.0330   with  lr:   0.0001\n",
      "87 of 200 epoch loss:   0.0315   with  lr:   0.0001\n",
      "88 of 200 epoch loss:   0.0323   with  lr:   0.0001\n",
      "89 of 200 epoch loss:   0.0347   with  lr:   0.0001\n",
      "90 of 200 epoch loss:   0.0338   with  lr:   0.0001\n",
      "91 of 200 epoch loss:   0.0319   with  lr:   0.0001\n",
      "92 of 200 epoch loss:   0.0318   with  lr:   0.0001\n",
      "93 of 200 epoch loss:   0.0308   with  lr:   0.0001\n",
      "94 of 200 epoch loss:   0.0304   with  lr:   0.0001\n",
      "95 of 200 epoch loss:   0.0311   with  lr:   0.0001\n",
      "96 of 200 epoch loss:   0.0308   with  lr:   0.0001\n",
      "97 of 200 epoch loss:   0.0306   with  lr:   0.0001\n",
      "98 of 200 epoch loss:   0.0340   with  lr:   0.0001\n",
      "99 of 200 epoch loss:   0.0327   with  lr:   0.0001\n",
      "100 of 200 epoch loss:   0.0325   with  lr:   0.0001\n",
      "101 of 200 epoch loss:   0.0339   with  lr:   0.0001\n",
      "102 of 200 epoch loss:   0.0303   with  lr:   0.0001\n",
      "103 of 200 epoch loss:   0.0306   with  lr:   0.0001\n",
      "104 of 200 epoch loss:   0.0281   with  lr:   0.0001\n",
      "105 of 200 epoch loss:   0.0297   with  lr:   0.0001\n",
      "106 of 200 epoch loss:   0.0278   with  lr:   0.0001\n",
      "107 of 200 epoch loss:   0.0283   with  lr:   0.0001\n",
      "108 of 200 epoch loss:   0.0286   with  lr:   0.0001\n",
      "109 of 200 epoch loss:   0.0290   with  lr:   0.0001\n",
      "110 of 200 epoch loss:   0.0284   with  lr:   0.0001\n",
      "111 of 200 epoch loss:   0.0310   with  lr:   0.0001\n",
      "112 of 200 epoch loss:   0.0313   with  lr:   0.0001\n",
      "113 of 200 epoch loss:   0.0311   with  lr:   0.0001\n",
      "114 of 200 epoch loss:   0.0348   with  lr:   0.0001\n",
      "115 of 200 epoch loss:   0.0361   with  lr:   0.0001\n",
      "116 of 200 epoch loss:   0.0431   with  lr:   0.0001\n",
      "117 of 200 epoch loss:   0.0378   with  lr:   0.0001\n",
      "118 of 200 epoch loss:   0.0339   with  lr:   0.0001\n",
      "119 of 200 epoch loss:   0.0303   with  lr:   0.0001\n",
      "120 of 200 epoch loss:   0.0293   with  lr:   0.0001\n",
      "121 of 200 epoch loss:   0.0284   with  lr:   0.0001\n",
      "122 of 200 epoch loss:   0.0273   with  lr:   0.0001\n",
      "123 of 200 epoch loss:   0.0275   with  lr:   0.0001\n",
      "124 of 200 epoch loss:   0.0268   with  lr:   0.0001\n",
      "125 of 200 epoch loss:   0.0266   with  lr:   0.0001\n",
      "126 of 200 epoch loss:   0.0264   with  lr:   0.0001\n",
      "127 of 200 epoch loss:   0.0258   with  lr:   0.0001\n",
      "128 of 200 epoch loss:   0.0256   with  lr:   0.0001\n",
      "129 of 200 epoch loss:   0.0273   with  lr:   0.0001\n",
      "130 of 200 epoch loss:   0.0254   with  lr:   0.0001\n",
      "131 of 200 epoch loss:   0.0247   with  lr:   0.0001\n",
      "132 of 200 epoch loss:   0.0254   with  lr:   0.0001\n",
      "133 of 200 epoch loss:   0.0237   with  lr:   0.0001\n",
      "134 of 200 epoch loss:   0.0257   with  lr:   0.0001\n",
      "135 of 200 epoch loss:   0.0254   with  lr:   0.0001\n",
      "136 of 200 epoch loss:   0.0243   with  lr:   0.0001\n",
      "137 of 200 epoch loss:   0.0278   with  lr:   0.0001\n",
      "138 of 200 epoch loss:   0.0324   with  lr:   0.0001\n",
      "139 of 200 epoch loss:   0.0400   with  lr:   0.0001\n",
      "140 of 200 epoch loss:   0.0443   with  lr:   0.0001\n",
      "141 of 200 epoch loss:   0.0364   with  lr:   0.0001\n",
      "142 of 200 epoch loss:   0.0312   with  lr:   0.0001\n",
      "143 of 200 epoch loss:   0.0300   with  lr:   0.0001\n",
      "144 of 200 epoch loss:   0.0255   with  lr:   0.0001\n",
      "145 of 200 epoch loss:   0.0242   with  lr:   0.0001\n",
      "146 of 200 epoch loss:   0.0261   with  lr:   0.0001\n",
      "147 of 200 epoch loss:   0.0242   with  lr:   0.0001\n",
      "148 of 200 epoch loss:   0.0244   with  lr:   0.0001\n",
      "149 of 200 epoch loss:   0.0229   with  lr:   0.0001\n",
      "150 of 200 epoch loss:   0.0246   with  lr:   0.0001\n",
      "151 of 200 epoch loss:   0.0221   with  lr:   0.0001\n",
      "152 of 200 epoch loss:   0.0232   with  lr:   0.0001\n",
      "153 of 200 epoch loss:   0.0251   with  lr:   0.0001\n",
      "154 of 200 epoch loss:   0.0234   with  lr:   0.0001\n",
      "155 of 200 epoch loss:   0.0253   with  lr:   0.0001\n",
      "156 of 200 epoch loss:   0.0239   with  lr:   0.0001\n",
      "157 of 200 epoch loss:   0.0232   with  lr:   0.0001\n",
      "158 of 200 epoch loss:   0.0213   with  lr:   0.0001\n",
      "159 of 200 epoch loss:   0.0236   with  lr:   0.0001\n",
      "160 of 200 epoch loss:   0.0234   with  lr:   0.0001\n",
      "161 of 200 epoch loss:   0.0228   with  lr:   0.0001\n",
      "162 of 200 epoch loss:   0.0229   with  lr:   0.0001\n",
      "163 of 200 epoch loss:   0.0233   with  lr:   0.0001\n",
      "164 of 200 epoch loss:   0.0236   with  lr:   0.0001\n",
      "165 of 200 epoch loss:   0.0215   with  lr:   0.0001\n",
      "166 of 200 epoch loss:   0.0217   with  lr:   0.0001\n",
      "167 of 200 epoch loss:   0.0231   with  lr:   0.0001\n",
      "168 of 200 epoch loss:   0.0248   with  lr:   0.0001\n",
      "169 of 200 epoch loss:   0.0242   with  lr:   0.0001\n",
      "170 of 200 epoch loss:   0.0234   with  lr:   0.0001\n",
      "171 of 200 epoch loss:   0.0219   with  lr:   0.0001\n",
      "172 of 200 epoch loss:   0.0218   with  lr:   0.0001\n",
      "173 of 200 epoch loss:   0.0228   with  lr:   0.0001\n",
      "174 of 200 epoch loss:   0.0217   with  lr:   0.0001\n",
      "175 of 200 epoch loss:   0.0218   with  lr:   0.0001\n",
      "176 of 200 epoch loss:   0.0213   with  lr:   0.0001\n",
      "177 of 200 epoch loss:   0.0233   with  lr:   0.0001\n",
      "178 of 200 epoch loss:   0.0210   with  lr:   0.0001\n",
      "179 of 200 epoch loss:   0.0225   with  lr:   0.0001\n",
      "180 of 200 epoch loss:   0.0249   with  lr:   0.0001\n",
      "181 of 200 epoch loss:   0.0212   with  lr:   0.0001\n",
      "182 of 200 epoch loss:   0.0203   with  lr:   0.0001\n",
      "183 of 200 epoch loss:   0.0217   with  lr:   0.0001\n",
      "184 of 200 epoch loss:   0.0219   with  lr:   0.0001\n",
      "185 of 200 epoch loss:   0.0215   with  lr:   0.0001\n",
      "186 of 200 epoch loss:   0.0253   with  lr:   0.0001\n",
      "187 of 200 epoch loss:   0.0252   with  lr:   0.0001\n",
      "188 of 200 epoch loss:   0.0250   with  lr:   0.0001\n",
      "189 of 200 epoch loss:   0.0236   with  lr:   0.0001\n",
      "190 of 200 epoch loss:   0.0222   with  lr:   0.0001\n",
      "191 of 200 epoch loss:   0.0209   with  lr:   0.0001\n",
      "192 of 200 epoch loss:   0.0231   with  lr:   0.0001\n",
      "193 of 200 epoch loss:   0.0211   with  lr:   0.0001\n",
      "194 of 200 epoch loss:   0.0184   with  lr:   0.0001\n",
      "195 of 200 epoch loss:   0.0204   with  lr:   0.0001\n",
      "196 of 200 epoch loss:   0.0227   with  lr:   0.0001\n",
      "197 of 200 epoch loss:   0.0209   with  lr:   0.0001\n",
      "198 of 200 epoch loss:   0.0188   with  lr:   0.0001\n",
      "199 of 200 epoch loss:   0.0223   with  lr:   0.0001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练 Seq2Seq 模型; \n",
    "model.train()\n",
    "LR = 1e-4\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1, last_epoch=-1)\n",
    "\n",
    "epoches = 200\n",
    "epoch_loss = 0\n",
    "epoch_loss_list = []\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_x.shape[0]):\n",
    "        pred = model(train_x[step], train_y[step], teacher_forcing_ratio=0.5)\n",
    "        # print(\"Train pred shape : {}\".format(pred.shape))\n",
    "        # print(\"Train train_y[step] shape : {}\".format(train_y[step].shape))\n",
    "        \n",
    "        loss = loss_func(pred, train_y[step].float())              # this calc the last element's loss between prediction and real.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.data.cpu()\n",
    "\n",
    "    if epoch_loss.item() < 1e-4:\n",
    "        print('Epoch [{}/{}], Loss: {:.5f}'.format(epoch+1, epoches, loss.item()))\n",
    "        print(\"The loss value is reached\")\n",
    "        break\n",
    "        \n",
    "    print(\"{} of {} epoch loss:   {:.4f}   with  lr:   {}\".format(epoch, epoches, epoch_loss.item(), optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    if (epoch+1) % 2000 ==0:\n",
    "        scheduler.step()\n",
    "        \n",
    "plt.plot(epoch_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6632718e-ce6d-497f-ab14-be276835cbbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model, 'e:\\\\Model_Seq2Seq.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "577462c4-1fca-480e-aeef-110a96855779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model = torch.load('e:\\\\Model_Seq2Seq.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9aaadf6-8d3d-4488-8ad3-a12104efc196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [-0.05030376 -0.03788231]\n",
      "Actual: [ 0. nan]\n"
     ]
    }
   ],
   "source": [
    "# 用模型预测数据\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_batch_count = test_x.shape[0]\n",
    "\n",
    "for step in range(test_batch_count):\n",
    "    pred = model(test_x[step], test_y[step], teacher_forcing_ratio=0)                       # 测试集上就不要teacher模式了。\n",
    "\n",
    "    loss = loss_func(pred, test_y[step])\n",
    "    \n",
    "    print(\"Prediction: {}\".format(pred[-1].cpu().detach().flatten().numpy()))\n",
    "    print(\"Actual: {}\".format(test_y[step][-1].cpu().detach().flatten().numpy()))\n",
    "\n",
    "# actual_line = test_y[step].cpu().detach().flatten().numpy()\n",
    "# pred_line   = pred.cpu().detach().flatten().numpy()\n",
    "# plt.plot(actual_line, 'r--')\n",
    "# plt.plot(pred_line, 'b-')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267f17c-8a01-4949-8287-f3368db264d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
