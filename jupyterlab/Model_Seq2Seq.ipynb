{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e96365c-9911-44d3-bf30-585acfbf60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 利用 LSTM 做一个 Seq2Seq 的预测，不考虑准确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fb6d43-f680-43cf-a074-a7cba41af8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling_data shape: (441, 60, 124)\n",
      "seq count: 441\n",
      "seq length: 60\n",
      "total batch count: 441\n",
      "batch size: 1\n",
      "rolling_data: torch.Size([441, 1, 60, 124])\n",
      "train_x: torch.Size([440, 1, 60, 122])\n",
      "train_y: torch.Size([440, 1, 2, 1])\n",
      "test_x:  torch.Size([1, 1, 60, 122])\n",
      "test_y:  torch.Size([1, 1, 2, 1])\n",
      "train_batch_count: 440\n",
      "test_batch_count:  1\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229_sc.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset.insert(1, 'future2',dataset.future)\n",
    "dataset['future2'] = dataset['future'].shift(-1)\n",
    "dataset = dataset.fillna(0)\n",
    "\n",
    "# print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "\n",
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 60\n",
    "BATCH_SIZE = 1                                                    # 注意：BATCH_SIZE是要能够整除seq_count的\n",
    "TEST_BATCH_COUNT = 1\n",
    "Y_DIM = 2                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "X_DIM = dataset.shape[1]-Y_DIM                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_DIM)                   # 数据一共是 seq_count x seq_len x (x_in_dim+y_dim) \n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(SEQ_LENGTH))\n",
    "\n",
    "\n",
    "total_batch_count = int(rolling_data.shape[0]/BATCH_SIZE)                                   # 把数据规划成 batch_count 个 batch\n",
    "\n",
    "\n",
    "print(\"total batch count: {}\".format(total_batch_count))\n",
    "print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "rolling_data = rolling_data.reshape(total_batch_count, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_DIM)  # 把数据转成 total_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "rolling_data = torch.tensor(rolling_data)\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "\n",
    "\n",
    "train_batch_count = total_batch_count - TEST_BATCH_COUNT\n",
    "test_batch_count = TEST_BATCH_COUNT\n",
    "\n",
    "train = rolling_data[:train_batch_count, :, :, :]\n",
    "test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_DIM:], train[:,:,-1:,0:Y_DIM]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_DIM:],  test[:,:,-1:,0:Y_DIM]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y-dim]  to [train_batch_count, batch_size, y-sequence_length, 1 dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y-dim]  to [test_batch_count, batch_size, y-sequence_length, 1 dim]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train_batch_count))\n",
    "print(\"test_batch_count:  {}\".format(test_batch_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ac5928d-7984-49aa-b1ae-57e902a703f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder & Decoder class\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size = self.hidden_dim, num_layers=self.num_layers, batch_first=True)\n",
    "        # print(\"Encoder self.input_dim  : {}\".format(self.input_dim))\n",
    "        # print(\"Encoder self.hidden_dim  : {}\".format(self.hidden_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"Encoder forward() x: {}\".format(x.shape))\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "        # print(\"Encoder outputs :{}\".format(outputs.shape))\n",
    "        # print(\"Encoder h_n     :{}\".format(h_n.shape))\n",
    "        # print(\"Encoder c_n     :{}\".format(c_n.shape))\n",
    "        return outputs, h_n, c_n\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.fc_in = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input : input batch data, size(input): [batch_size, feature_size]\n",
    "        # notice input only has two dimensions since the input is batchs\n",
    "        # of last coordinate of observed trajectory so the sequence length has been removed.\n",
    "        \n",
    "        # add sequence dimension to input, to allow use of nn.LSTM\n",
    "        # print(\"Decoder forward() input size : {}\".format(input.shape))\n",
    "        # print(\"Decoder forward() hidden size: {}\".format(hidden.shape))\n",
    "        # print(\"Decoder forward() cell size  : {}\".format(cell.shape))\n",
    "        \n",
    "        input = self.fc_in(input)\n",
    "\n",
    "        lstm_output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        \n",
    "        # print(\"Decoder forward() lstm_output: {}\".format(lstm_output.shape))\n",
    "        \n",
    "        prediction = self.fc_out(lstm_output)         # prediction is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b80326ab-dd8a-4bdb-96e4-a3480314b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model class\n",
    "\n",
    "ENC_INPUT_DIM   = X_DIM\n",
    "ENC_HIDDEN_DIM  = 768\n",
    "DEC_INPUT_DIM   = 1\n",
    "DEC_HIDDEN_DIM  = 768\n",
    "DEC_OUPUT_DIM   = 1\n",
    "NUM_LAYERS      = 1\n",
    "ENC_DROPOUT     = 0.1\n",
    "DEC_DROPOUT     = 0.1\n",
    "DEC_OUTPUT_LEN  = 2\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.decoder_fc_init= nn.Linear(encoder.input_dim, decoder.input_dim)\n",
    "        # self.decoder_fc_input= nn.Linear(decoder.output_dim, decoder.input_dim)\n",
    "        \n",
    "        assert (encoder.hidden_dim == decoder.hidden_dim), \"hidden dimension in encoder and decoder must be equal\"       \n",
    "        assert (encoder.num_layers == decoder.num_layers), \"hidden layer numbers in encoder and decoder must be equal\"\n",
    "        \n",
    "            \n",
    "    def forward(self, x, y):\n",
    "        # x is the input to the encoder.\n",
    "        # y is the output from the decoder\n",
    "        # x = [batch size, encoder_in_sequence_len, encoder_in_dim]               encoder_in_sequence_len=45, encoder_in_dim=122\n",
    "        # y = [batch size, encoder_in_sequence_len, decoder_out_dim]             decoder_out_sequence_len=45, decoder_out_dim=2\n",
    "        \n",
    "        # print(\"Seq2Seq forwar() x shape : {}\".format(x.shape))\n",
    "        # print(\"Seq2Seq forwar() y shape : {}\".format(y.shape))\n",
    "                \n",
    "        batch_size = x.shape[0]\n",
    "        encoder_in_seq_len = x.shape[1]\n",
    "        encoder_in_dim = x.shape[2]\n",
    "        \n",
    "        decoder_out_seq_len = y.shape[1]                                                # This is most important that define the output length\n",
    "        # decoder_out_dim = y.shape[2]\n",
    "        \n",
    "        # tensor to store decoder outputs of each time step\n",
    "        # outputs = torch.zeros(batch_size, decoder_out_seq_len, DEC_OUPUT_DIM).double().to(device)\n",
    "        outputs = torch.zeros(y.shape).to(device)\n",
    "        # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))\n",
    "        \n",
    "        encoder_output, hidden, cell = self.encoder(x)\n",
    "        # print(\"encoder_output shape: {}\".format(encoder_output.shape))                    # [batch_size, encode_input_seq_length, encoder_hidden_dim]\n",
    "        # print(\"encoder hidden shape: {}\".format(hidden.shape))                            # [encoder_hidden_layer_number, batch_size, encoder_hidden_dim]\n",
    "        # print(\"encoder cell shape :  {}\".format(cell.shape))                              # [encoder_hidden_layer_number, batch_size, encoder_hidden_dim]\n",
    "        \n",
    "\n",
    "        # first input to decoder may be last coordinates of x to predict the future: [last_x] --> [model] --> [future_y]\n",
    "        # this is last batch and last word of sequence.\n",
    "        # print(\"Seq2Seq forward() x shape : {}\".format(x.shape))                             # [batch_size, encode_input_seq_length, encoder_input_dim]\n",
    "        # print(\"Seq2Seq forward() x[-1,-1,:] shape : {}\".format(x[-1,-1,:].shape))           # [encoder_input_dim]\n",
    "        decoder_input = x[:, -1, :]                                                           # [batch_size, encoder_input_dim] Get last element of sequence of the batch in encoder\n",
    "        decoder_input = decoder_input.unsqueeze(1)                                            # [batch_size, 1, encoder_input_dim] Get last element of sequence of the batch in encoder\n",
    "        # print(\"decoder_input: {}\".format(decoder_input.shape))\n",
    "        decoder_input = self.decoder_fc_init(decoder_input)                                   # [batch_size, 1, decoder_input_dim] Conver to 1st element of sequence of the batch in decoder\n",
    "        \n",
    "        # decoder_input = torch.zeros(self.decoder.input_dim).double().to(device)\n",
    "        \n",
    "        # decoder_input = torch.zeros(batch_size, seq_len, OUPUT_DIM).double().to(device)\n",
    "        # print(\"Seq2Seq forward() encoder_output shape : {}\".format(encoder_output.shape))\n",
    "        \n",
    "        # print(\"Seq2Seq forward() decoder_input shape: {}\".format(decoder_input.shape))\n",
    "        # decoder_input = decoder_input.unsqueeze(0)\n",
    "        # decoder_input = decoder_input.unsqueeze(0)\n",
    "        # print(\"Seq2Seq forward() decoder_input shape: {}\".format(decoder_input.shape))\n",
    "\n",
    "        \n",
    "        # Becasue the input and target have different sequence length\n",
    "        # Get the target prediction one by one\n",
    "        for i in range(decoder_out_seq_len):\n",
    "            # run the decoder for one time step\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            # print(\"Seq2Seq forward() output shape: {}\".format(output.shape))\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each time step\n",
    "            outputs[:,i,:] = output\n",
    "            # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))            \n",
    "            # output is the same shape as input, [batch_size, feature size]\n",
    "            # so we can use output directly as next input\n",
    "            decoder_input = output\n",
    "            # print(\"Seq2Seq forward() decoder_input shape: {}\".format(output.shape))\n",
    "            \n",
    "            # 或者使用teacher_forcing来优化\n",
    "            # teacher_forcing_ratio=0.5\n",
    "            # teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # decoder_input = y[i] if teacher_forcing else output\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f9dd074-51e0-475f-a64c-f817959b185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "encoder = Encoder(input_dim=ENC_INPUT_DIM, hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, dropout=ENC_DROPOUT)\n",
    "decoder = Decoder(input_dim=DEC_INPUT_DIM, hidden_dim=DEC_HIDDEN_DIM, num_layers=NUM_LAYERS, output_dim=DEC_OUPUT_DIM, dropout=DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).double().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7692e6fe-3005-40a5-8f3e-01cfaeed148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 10 epoch loss: 4.546502\n",
      "1 of 10 epoch loss: 1.791893\n",
      "2 of 10 epoch loss: 1.620879\n",
      "3 of 10 epoch loss: 1.549675\n",
      "4 of 10 epoch loss: 1.495712\n",
      "5 of 10 epoch loss: 1.427790\n",
      "6 of 10 epoch loss: 1.439167\n",
      "7 of 10 epoch loss: 1.434969\n",
      "8 of 10 epoch loss: 1.381002\n",
      "9 of 10 epoch loss: 1.345864\n"
     ]
    }
   ],
   "source": [
    "# 训练 Seq2Seq 模型; \n",
    "LR = 1e-4\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "model.train()\n",
    "epoches = 10\n",
    "epoch_loss = 0\n",
    "epoch_loss_list = []\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        pred = model(train_x[step], train_y[step])\n",
    "        \n",
    "#         print(\"Train pred shape : {}\".format(pred.shape))\n",
    "#         print(\"Train train_y[step] shape : {}\".format(train_y[step].shape))\n",
    "        \n",
    "        loss = loss_func(pred, train_y[step].float())                                  # this calc the last element's loss between prediction and real.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.data.cpu()\n",
    "\n",
    "    print(\"{} of {} epoch loss: {:.6f}\".format(epoch, epoches, epoch_loss))\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # if (epoch+1)%40 == 0:\n",
    "    #     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9aaadf6-8d3d-4488-8ad3-a12104efc196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Loss average:0.000000\n",
      "Prediction: tensor([[[-0.0113],\n",
      "         [-0.0135]]], device='cuda:0', grad_fn=<CopySlices>) ---- Actual: tensor([[[-0.1300],\n",
      "         [ 0.0000]]], device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgI0lEQVR4nO3deZgU5dX38e8BNdHEBRQBIQoiRFEQzQRFghuraMRENPK4jBHEKOKCS0hIxOASjOY1cQ9BDVHjFhdwCQgoQQWRQRQBhWFR2YRB0EeDGpbz/HEXLyP2OEsv1d31+1xXX11Vfff0KWaYM/dSp8zdERGR5KoXdwAiIhIvJQIRkYRTIhARSTglAhGRhFMiEBFJuB3iDqAu9tprL2/RokXcYYiIFJRZs2atdfdG2x8vyETQokULysrK4g5DRKSgmNn7qY5raEhEJOGUCEREEk6JQEQk4ZQIREQSTolARCThMpIIzKyXmS0ws0VmNjTF698ys0ej12eYWYtKr/0qOr7AzHpmIh4REam5tBOBmdUH7gROANoC/cys7XbN+gPr3f0A4Fbgpui9bYEzgIOBXsBd0dcTEZEcyUSPoCOwyN2XuPt/gUeAPtu16QOMibb/CXQ1M4uOP+LuX7r7UmBR9PVERKSyDz6ATz/NypfORCJoBiyrtL88OpayjbtvAj4B9qzhewEws4FmVmZmZRUVFRkIW0SkAGzZAnfeCQcfDNdck5WPKJjJYncf5e4l7l7SqNHXrpAWESk+CxbAMcfAxRdDp05w6aVZ+ZhMlJhYAXyv0n7z6FiqNsvNbAdgd+CjGr5XRCR5li6FQw+FXXaBv/0NzjkHzLLyUZnoEcwEWptZSzPbiTD5O267NuOA0mi7L/Cih3tkjgPOiFYVtQRaA69nICYRkcK0dm14btkSbr4Z5s+H0tKsJQHIQCKIxvwvBiYA7wCPufs8MxthZidHze4F9jSzRcAQYGj03nnAY8B8YDwwyN03pxuTiEjB+eILGDYM9tsP5s0LxwYPhiZNsv7RGak+6u7PA89vd+yaSttfAKdV8d4bgBsyEYeISEGaNg3694d334Vzz4WmTXP68QUzWSwiUnTcYcgQ+NGP4PPPYcIEuP9+aNgwp2EoEYiIxMUMdt45rAqaOxd69IgljIK8MY2ISMFatw6uuALOPBO6dYPrr8/qRHBNqEcgIpIrTzwBbdvCAw9smxCOOQmAEoGISPatWgWnngp9+8I++0BZWdYuDqsLJQIRkWx75hl47jkYORJefx06dIg7oq/QHIGISDa8914oEdGzJwwYAN27h4vE8pB6BCIimbRlC9x+OxxyCJx/PmzcCPXq5W0SACUCEZHMeecd6NIFLrkkPL/8Muy4Y9xRVUtDQyIimbB0KRx2GHznO/D3v8NZZ+XFiqCaSFQimDgRVq4MvbT69at+5OL1Avn5EJHqVFRAo0Zh6OePfwwrgxo3jjuqWklUIrj1VvjXv+KOIjDLXqLJVTKL8/Xq2ijZStZ9/jmMGAF//nNYCXTIITBoUNxR1UmiEsH998OGDbB5c9WPLVuy+3q2P2PjRvjyy8x//UL0Tck2H5JZoSf0RCfbl18OK4EWLgzF4pqlvLFiwUhUIiiw3lpeqZwc8j0Z5kOyrctnFKJsJ9u4k93XXq/n1L9/NPWffZr6jTtRf+TD1Cs5nPpvZeccc5VsE5UIpO7q1QuPAlgAUbC2Twz5nAyz/RkbN4by/Jn++ukz4PzwWE10Z5Xs2vp/b2tyeOMN+P73M/sZSgQieULJNvvqlGzXrmfzjTex+cST2XzEUWze5GzeYrEl5AYNMv/vokQgIolRq2TrDo8/HkpEr18P3ZtBh6MIvYLiokQgIrK9lSvDCqCnn4Yf/AAmTYL27eOOKmt0ZbGIyPaefRbGjw83j3/ttaJOAqAegYhIsGRJWA7aq1dYGtqzZ7iRfAKoRyAiybZ5M/zpT9CuHVxwwbYicQlJApBmIjCzhmY20czKo+eU89lmVhq1KTez0ujYLmb2nJm9a2bzzGxkOrGIiNTavHnQuTNcfjkceyy88koil22l2yMYCkx299bAZFKsqjWzhsBw4AigIzC8UsK4xd0PBA4DOpvZCWnGIyJSM0uWwOGHw6JF8OCDYV7ge9+LO6pYpJsI+gBjou0xwCkp2vQEJrr7OndfD0wEern7Bnd/CcDd/wu8ATRPMx4RkW+2enV43n//MCQ0f364kXxi62Wknwgau/uqaPtDIFURh2bAskr7y6Nj/5+Z7QH8mNCrEBHJvA0b4OqroUULePvtcOzCC2HvvWMNKx9Uu2rIzCYBTVK8NKzyjru7mXltAzCzHYCHgdvcfck3tBsIDATYd999a/sxIpJk//53WAm0aBEMHAj6HfIV1SYCd+9W1WtmttrMmrr7KjNrCqxJ0WwFcGyl/ebAlEr7o4Byd/9TNXGMitpSUlJS64QjIgnkHu4Wdscd0KoVvPgiHHdc3FHlnXSHhsYBpdF2KTA2RZsJQA8zaxBNEveIjmFm1wO7A5elGYeIyNeZheI8Q4bAnDlKAlVINxGMBLqbWTnQLdrHzErMbDSAu68DrgNmRo8R7r7OzJoThpfaAm+Y2ZtmNiDNeEQk6dauDbeJfOGFsD9iRLhz2C67xBtXHkvrymJ3/wjomuJ4GTCg0v59wH3btVlOMVZvEpF4uMOjj8LgwfDJJ3DkkdCjR9xRFQRdWSwihW/FCjjlFOjXL9w7eNasUDVUakSJQEQK33PPwcSJYQho+vRQLkJqTEXnRKQwLV4MCxZA795haWivXloWWkfqEYhIYdm8Ofzl365duCBsa5E4JYE6UyIQkcIxdy506gRXXgndusG0aYksEpdpGhoSkcKwtUjcHnvAI4/A6acnuj5QJikRiEh++/BDaNIkFIm7/XY49VTYa6+4oyoqGhoSkfy0YQNccUUoEjdnTjh2wQVKAlmgHoGI5J+XXgorgZYsgV/8IiQDyRr1CEQkf7jDoEFw/PFhJdCUKXD33bDbbnFHVtSUCEQkf5iFoZ+rroK33oJjjok7okTQ0JCIxGvNGrj0Ujj3XOjZE373u7gjShz1CEQkHu7w0EPQti08+WSYD5BYKBGISO4tWwYnnRTKRbduDbNnh6uEJRZKBCKSe+PHh4ngP/0JXnkl9AokNpojEJHcKC+HhQvhxBOhf3844QRo3jzuqAT1CEQk2zZtgptvhvbtw9LQrUXilATyhhKBiGTPnDmhSNzVV4cy0SoSl5c0NCQi2bFkCZSUhJvHP/YY9O2rInF5SolARDJr5UrYZ59QJO7OO+GnP4U994w7KvkGGhoSkcz47DO47LKQALYWiTv/fCWBAqAegYikb+JEGDgQ3nsvTAi3bBl3RFILafcIzKyhmU00s/LouUEV7UqjNuVmVpri9XFmNjfdeEQkh9zDhWA9esBOO8HUqXDHHbDrrnFHJrWQiaGhocBkd28NTI72v8LMGgLDgSOAjsDwygnDzH4KfJaBWEQkl8ygaVMYOhTefBO6dIk7IqmDTCSCPsCYaHsMcEqKNj2Bie6+zt3XAxOBXgBm9l1gCHB9BmIRkWxbvTrcJnL8+LB/zTXw+9/DzjvHG5fUWSYSQWN3XxVtfwg0TtGmGbCs0v7y6BjAdcAfgQ3f9CFmNtDMysysrKKiIs2QRaTW3OHvf4eDDoKxY8N8gBSFGiUCM5tkZnNTPPpUbufuDnhNP9zMOgCt3P2p6tq6+yh3L3H3kkaNGtX0I0QkEz74AHr3htLSkAjeeivcOUyKQo1WDbl7t6peM7PVZtbU3VeZWVNgTYpmK4BjK+03B6YAnYASM3svimVvM5vi7sciIvnjhRfg5ZfhttvCqqB6WnleTDLx3RwHbF0FVAqMTdFmAtDDzBpEk8Q9gAnufre77+PuLYAfAQuVBETyxIIF8MwzYbt//7A/eLCSQBHKxHd0JNDdzMqBbtE+ZlZiZqMB3H0dYS5gZvQYER0TkXyzcSOMHAmHHgqXXBL2zaBZs+rfKwXJwrB+YSkpKfGysrK4wxApPrNnh7/+Z88OtYFuvx2aNIk7KskQM5vl7iXbH9eVxSISLF4MHTuGkhBPPBFqBEkiaLBPJOlWrAjPrVrBX/4C77yjJJAwSgQiSfXZZ2EOYP/9w3JQgPPOC2WjJVE0NCSSRBMmwAUXhOsDBg8OvQFJLCUCkSRxDwngr3+FAw8M1wZ07hx3VBIzDQ2JJIlZuFfwsGFhZZCSgKAegUjxW7UKLr4YBgyAE04IReJEKlGPQKRYucP990PbtvDcc7BsWfXvkURSIhApRkuXQs+eYRVQu3bh1pEDB8YdleQpJQKRYvTiizB9erh5/JQp0KZN3BFJHtMcgUixeOcdKC+Hk08OPYHevcPdw0SqoR6BSKHbuBFuuAE6dIDLLttWJE5JQGpIiUCkkL3xBvzwh/Cb38App4ThoB13jDsqKTAaGhIpVFuLxDVqBE89FRKBSB2oRyBSaLYuA23VCkaPhvnzlQQkLUoEIoXif/8XLrooJIA33wzHzj1XReIkbRoaEikEzz8fbha/fHmYEG7dOu6IpIgoEYjkM/dQGuK++8IVwtOmwZFHxh2VFBkNDYnkM7Nwv4BrrgkrhJQEJAvUIxDJNytXwqBBoSdw4omhUqhIFqlHIJIv3OHee8MQ0PjxISGI5EBaicDMGprZRDMrj55TLl8ws9KoTbmZlVY6vpOZjTKzhWb2rpmdmk48IgVryRLo3j30Ajp0CEXizj8/7qgkIdLtEQwFJrt7a2BytP8VZtYQGA4cAXQEhldKGMOANe7eBmgL/DvNeEQK05Qp8PrrcM89oWCcVgVJDqWbCPoAY6LtMcApKdr0BCa6+zp3Xw9MBHpFr50H/B7A3be4+9o04xEpHPPmwdNPh+2f/xwWLgy3kaynEVvJrXR/4hq7+6po+0OgcYo2zYDKd8RYDjQzsz2i/evM7A0ze9zMUr0fADMbaGZlZlZWUVGRZtgiMfrvf2HECDjsMBgyZFuRuCZN4o5MEqraRGBmk8xsbopHn8rt3N0Br8Vn7wA0B6a5++HAdOCWqhq7+yh3L3H3kkaNGtXiY0TyyMyZUFICw4dD374wY4aKxEnsql0+6u7dqnrNzFabWVN3X2VmTYE1KZqtAI6ttN8cmAJ8BGwAnoyOPw70r1nYIgVo8WLo1Cn85T9uHPz4x3FHJAKkPzQ0Dti6CqgUGJuizQSgh5k1iCaJewAToh7EM2xLEl2B+WnGI5J/3n8/PLdqFa4QnjdPSUDySrqJYCTQ3czKgW7RPmZWYmajAdx9HXAdMDN6jIiOAfwSuNbM5gBnA1ekGY9I/vjkk1Af6IADthWJO+cc2H33WMMS2V5aVxa7+0eEv+S3P14GDKi0fx9wX4p27wNHpxODSF567rmwAmjVKrj8ct0zWPKa1qmJZJJ7WAp60kmhPPT06XDLLbDLLnFHJlIlJQKRTDILF4Ndey3MmhXuICaS51R0TiRdy5eHG8YMHBh6Ar/+ddwRidSKegQidbVlC4waBQcfDJMmwZpUq6dF8p8SgUhdLFoEXbuGCeGSEpg7F847L+6oROpEQ0MidfHKK+FGMX/9K/TvH+YGRAqUEoFITb39dugJ/OQnUFoKvXvD3nvHHZVI2jQ0JFKdL78MtYEOPxyuugo2bQo9ACUBKRJKBCLfZMYM+MEPQrXQM86A116DHdSRluKin2iRqixeDJ07Q9Om8Oyz4f7BIkVIPQKR7S1dGp5btYK//S0UiVMSkCKmRCCy1ccfh/sEt2kDs2eHY2edBbvtFmtYItmmoSERgLFj4cILYfVquPJKOPDAuCMSyRklAkk297AU9IEHoH37cMOYkpK4oxLJKQ0NSTJ5dFdVM2jbFq67DsrKlAQkkZQIJHmWLQvF4Z55JuwPHQq/+Y3uHSyJpUQgybFlC9x9dygSN2UKrF0bd0QieUFzBJIM5eUwYABMnQrduoWqoS1bxh2VSF5QIpBkePVVmDMn3Dz+3HNVJE6kEiUCKV5vvRV6An37hpVBJ54IjRrFHZVI3tEcgRSfL7+E3/42rAD61a+2FYlTEhBJSYlAisu0adChA1x/PZx5ZigapyJxIt8o7URgZg3NbKKZlUfPDapoVxq1KTez0krH+5nZ22Y2x8zGm9le6cYkCbVoEXTpAhs2wPjxoU5Qw4ZxRyWS9zLRIxgKTHb31sDkaP8rzKwhMBw4AugIDDezBma2A/Bn4Dh3bw/MAS7OQEySJEuWhOcDDoAHHwy3jezZM96YRApIJhJBH2BMtD0GOCVFm57ARHdf5+7rgYlAL8Cix3fMzIDdgJUZiEmSYP36cJ/gykXi+vWDXXeNNy6RApOJwdPG7r4q2v4QaJyiTTNgWaX95UAzd99oZhcCbwP/AcqBQak+xMwGAgMB9t133wyELQXtqafgoougogKuvhoOOijuiEQKVo16BGY2yczmpnj0qdzO3R3wmn64me0IXAgcBuxDGBr6Vaq27j7K3UvcvaSRVn8kl3soDf3Tn0KTJvD663DjjfDtb8cdmUjBqlGPwN27VfWama02s6buvsrMmgJrUjRbARxbab85MAXoEH39xdHXeowUcwwiuIcloGahSmjbtuH+waoPJJK2TMwRjAO2rgIqBcamaDMB6BFNEDcAekTHVgBtzWzrn/jdgXcyEJMUk/ffhxNOCCWiIQwF/frXSgIiGZKJRDAS6G5m5UC3aB8zKzGz0QDuvg64DpgZPUZEE8crgd8BU81sDqGHcGMGYpJisGUL3HFHKBL36qvhDmIiknHmXuMh/bxRUlLiZWVlcYch2bRgAfTvHxJAz57wl7/AfvvFHZVIQTOzWe7+tZtu6JJLyU8zZsD8+eGisHPOUZE4kSxSIpD8MXt2uDr4tNPg7LNDkbg994w7KpGip1pDEr8vvgiTvz/8IQwbtq1InJKASE4oEUi8Xn01FIn7/e/DEJCKxInknP7HSXwWLYKjj4Z994UXXoDu3eOOSCSR1COQ3CsvD88HHAD/+Ae8/baSgEiMlAgkd9atC3cKO+ggeOONcOxnP4PvfjfeuEQSTkNDkn3u8MQTMGhQSAZDh4YSESKSF5QIJLvcw53CHn4YDj8cJkwIk8MikjeUCCQ7KheJO+yw8Mt/yBCtCBLJQ5ojkMxbujSUhRgb1R+86qpQKE5JQCQvKRFI5mzeDLfdBoccAtOnw6efxh2RiNSA/kSTzHjnnVAkbvr0UDL6nnvC9QEikveUCCQzZs4MFUMfeCBMDqtInEjBUCKQups1CxYvhtNPD0XiTjoJGjaMOyoRqSXNEUjtff45/PKXcMQR8NvfbisSpyQgUpCUCKR2/v1vOPRQ+MMf4Oc/V5E4kSKg/8FSc+XlcNxx0LIlTJoEXbvGHZGIZIB6BFK9hQvDc+vW8OijMGeOkoBIEVEikKqtXRsmgSsXiTvtNPjOd+KNS0QySkND8nXu8NhjMHgwrF8fJoQPPjjuqEQkS9LqEZhZQzObaGbl0XODKtqNN7OPzezZ7Y63NLMZZrbIzB41s53SiUcywB3OOCM89tsv9ASuvRa+9a24IxORLEl3aGgoMNndWwOTo/1UbgbOTnH8JuBWdz8AWA/0TzMeqSv38GwGHTvCzTeHq4TbtYs3LhHJunQTQR9gTLQ9BjglVSN3nwx8pfCMmRlwPPDP6t4vWbZkSbhD2NYicVdcAVdeqWWhIgmRbiJo7O6rou0Pgca1eO+ewMfuvinaXw40q6qxmQ00szIzK6uoqKhbtPJVmzfDrbeGInGvvw7/+U/cEYlIDKr9k8/MJgFNUrw0rPKOu7uZeaYC2567jwJGAZSUlGTtcxJj3rxQJG7GjFAa4u67oXnzuKMSkRhUmwjcvVtVr5nZajNr6u6rzKwpsKYWn/0RsIeZ7RD1CpoDK2rxfknH7NmhTtA//hEmhlUkTiSx0h0aGgeURtulwNiavtHdHXgJ6FuX90sdzJwJjzwSts88M1wo1q+fkoBIwqWbCEYC3c2sHOgW7WNmJWY2emsjM3sZeBzoambLzaxn9NIvgSFmtogwZ3BvmvFIKhs2hMnfI48MS0G3FolrkHK1r4gkTFrLQtz9I+BrtQbcvQwYUGm/SxXvXwJ0TCcGqcaUKTBgQBgGuuACuOkmrQYSka/Qb4RiVl4Oxx8P++8PL74YCsaJiGxHtYaK0bvvhufWreHxx0OROCUBEamCEkExqaiA//mfUBdoa5G4U0+FXXaJNy4RyWsaGioG7mE10CWXwCefwPDh4SIxEZEaUCIodO7hnsH//Ge4deS996pSqIjUihJBoXIPS0DN4KijoHPnUDa6fv24IxORAqM5gkK0aFFYDfTUU2H/8svhssuUBESkTpQICsmmTXDLLaE09OzZ8OWXcUckIkVAQ0OF4u23Q5G4mTOhTx+46y7YZ5+4oxKRIqBEUCjeegveey/cPP6001QfSEQyRkND+WzGDHj44bC9tUjc6acrCYhIRikR5KP//AeGDIFOnWDEiG1F4vbYI+7IRKQIKRHkm8mTw2TwrbfCL34RegUqEiciWaTfMPlk4cJw7+BWrULV0GOOiTsiEUkA9Qjywfz54blNG3jiiVAkTklARHJEiSBOq1fDz34WhoK2Fon7yU9g553jjUtEEkWJIA7u8OCD0LYtPP10mBBu1y7uqEQkoTRHkGvu0LcvPPlkWBV0771w0EFxRyUiCaZEkCuVi8R16RLmAAYNUn0gEYmdhoZyYeHC8Iv/ySfD/mWXhXsHKAmISB5QIsimTZvgD3+AQw8NtYI2bow7IhGRr9HQULbMmQPnnQezZoWVQHfeCU2bxh2ViMjXpNUjMLOGZjbRzMqj5wZVtBtvZh+b2bPbHX/IzBaY2Vwzu8/Mdkwnnrwydy4sWxZuHv/EE0oCIpK30h0aGgpMdvfWwORoP5WbgbNTHH8IOBBoB+wMDEgznnhNmwYPPRS2+/ULcwN9+6pInIjktXQTQR9gTLQ9BjglVSN3nwx8muL48x4BXgeapxlPPD77LEz+/uhHcMMN24rE7b573JGJiFQr3UTQ2N1XRdsfAo3r8kWiIaGzgfHf0GagmZWZWVlFRUVdPiY7XngBDjkE7rgjLAdVkTgRKTDV/sYys0lAkxQvDau84+5uZl7HOO4Cprr7y1U1cPdRwCiAkpKSun5OZi1cCL16hRpBU6eGHoGISIGpNhG4e7eqXjOz1WbW1N1XmVlTYE1tAzCz4UAj4ILavjc2c+eGXkCbNuEG8j17wre/HXdUIiJ1ku7Q0DigNNouBcbW5s1mNgDoCfRz9y1pxpJ9H34YbhPZvn1YFgrh/sFKAiJSwNJNBCOB7mZWDnSL9jGzEjMbvbWRmb0MPA50NbPlZtYzeukewrzCdDN708yuSTOe7HCHMWNCkbhnngkTwu3bxx2ViEhGpDWr6e4fAV1THC+j0lJQd+9Sxfvzf1bVPVwQNnYsdO4Mo0fDgQfGHZWISMbk/y/iuGzZAvXqhWWgxx0H3brBRReFYyIiRUS/1VJ59104+uhwRTDApZfCxRcrCYhIUdJvtso2boQbbwxF4ubPD70CEZEip6GhrWbPDkXi3nwzrAy6/XZoXKfr40RECooSwVbvvhuWhz75ZJgcFhFJiGQPDb3yCjzwQNg+44xwpbCSgIgkTDITwaefhsnfLl1g5MhtReJ23TXuyEREci55iWD8+FAe4q67QsVQFYkTkYRL1m/AhQuhd2/4/vfDsNBRR8UdkYhI7JKVCNq0CVcId++u+kAiIpFkJQKAH/847ghERPJK8uYIRETkK5QIREQSTolARCThlAhERBJOiUBEJOGUCEREEk6JQEQk4ZQIREQSztw97hhqzcwqgPfr+Pa9gLUZDKcQ6JyTQedc/NI93/3cvdH2BwsyEaTDzMrcvSTuOHJJ55wMOufil63z1dCQiEjCKRGIiCRcEhPBqLgDiIHOORl0zsUvK+ebuDkCERH5qiT2CEREpBIlAhGRhCvaRGBmvcxsgZktMrOhKV7/lpk9Gr0+w8xaxBBmRtXgnIeY2Xwzm2Nmk81svzjizKTqzrlSu1PNzM2soJca1uR8zez06Ps8z8z+kesYM60GP9f7mtlLZjY7+tnuHUecmWRm95nZGjObW8XrZma3Rf8mc8zs8LQ+0N2L7gHUBxYD+wM7AW8BbbdrcxFwT7R9BvBo3HHn4JyPA3aJti9MwjlH7XYFpgKvASVxx53l73FrYDbQINrfO+64c3DOo4ALo+22wHtxx52B8z4aOByYW8XrvYF/AQYcCcxI5/OKtUfQEVjk7kvc/b/AI0Cf7dr0AcZE2/8EupqZ5TDGTKv2nN39JXffEO2+BjTPcYyZVpPvM8B1wE3AF7kMLgtqcr7nA3e6+3oAd1+T4xgzrSbn7MBu0fbuwMocxpcV7j4VWPcNTfoAf/fgNWAPM2ta188r1kTQDFhWaX95dCxlG3ffBHwC7JmT6LKjJudcWX/CXxSFrNpzjrrM33P353IZWJbU5HvcBmhjZq+a2Wtm1itn0WVHTc75WuAsM1sOPA8Mzk1osart//dvlLyb1wtmdhZQAhwTdyzZZGb1gP8HnBtzKLm0A2F46FhCj2+qmbVz94/jDCrL+gF/c/c/mlkn4AEzO8Tdt8QdWKEo1h7BCuB7lfabR8dStjGzHQhdyo9yEl121OScMbNuwDDgZHf/MkexZUt157wrcAgwxczeI4yljivgCeOafI+XA+PcfaO7LwUWEhJDoarJOfcHHgNw9+nAtwnF2YpZjf6/11SxJoKZQGsza2lmOxEmg8dt12YcUBpt9wVe9GgWpkBVe85mdhjwF0ISKPSxY6jmnN39E3ffy91buHsLwrzIye5eFk+4aavJz/XThN4AZrYXYahoSQ5jzLSanPMHQFcAMzuIkAgqchpl7o0DzolWDx0JfOLuq+r6xYpyaMjdN5nZxcAEwqqD+9x9npmNAMrcfRxwL6ELuYgwKXNGfBGnr4bnfDPwXeDxaF78A3c/Obag01TDcy4aNTzfCUAPM5sPbAaucveC7enW8JyvAP5qZpcTJo7PLfA/6jCzhwkJfa9o7mM4sCOAu99DmAvpDSwCNgA/T+vzCvzfS0RE0lSsQ0MiIlJDSgQiIgmnRCAiknBKBCIiCadEICKScEoEIiIJp0QgIpJw/wda4JDrR/EO5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 用模型预测数据\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "for step in range(test_batch_count):\n",
    "    pred = model(test_x[step], test_y[step])\n",
    "\n",
    "    loss = loss_func(pred, test_y[step])\n",
    "    \n",
    "    if (step+1) < test_batch_count:                       # 最后一个测试数据不需要统计，因为没有真实值。\n",
    "        test_loss += loss.cpu()\n",
    "    \n",
    "    if test_batch_count == 1:\n",
    "        print(\"Prediction Loss average:{:.6f}\".format(test_loss/test_batch_count))\n",
    "    else:\n",
    "        print(\"Prediction Loss average:{:.6f}\".format(test_loss/(test_batch_count-1)))\n",
    "        \n",
    "    print(\"Prediction: {} ---- Actual: {}\".format(pred, test_y[step]))\n",
    "\n",
    "actual_line = test_y[step].cpu().detach().flatten().numpy()\n",
    "pred_line   = pred.cpu().detach().flatten().numpy()\n",
    "plt.plot(actual_line, 'r--')\n",
    "plt.plot(pred_line, 'b-')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267f17c-8a01-4949-8287-f3368db264d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
