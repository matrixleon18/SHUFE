{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7635ca9d-4939-4a35-a263-2feced7b671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 LSTM 做一个 Seq2Seq 的预测，不考虑准确性\n",
    "## Architecture \n",
    "## General:        [x1, x2, ... , xn]   ==> [Seq2Seq Model] ==> [Prediction1, Prediction2] \n",
    "## Genral Data:    [122-D, ... , 122-D] ==> [Seq2Seq Model] ==> [1-D, 1-D] \n",
    "## Detail :        [x1, x2, ... , xn]   ==> [Encoder] ==> [h_n, c_n] + [xn]    ==> [Decoder] ==> [Prediont1, Prediction2]\n",
    "## Detail Data:    [122-D, ...., 122-D] ==> [Encoder] ==> [h_n, c_n] + [122-D] ==> [Decoder] ==> [1-D, 1-D]\n",
    "## Decoder :       xn, [h_n, c_n] ==> FC(xn), [h_n, c_n] ==> [1-D, h_n, c_n] ==> LSTM(1-D, h_n, c_n) ==> pred1, h1, c1 ==> LSTM(pred1, h, c) ==> pred2, h2, c2 ==> LSTM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fb6d43-f680-43cf-a074-a7cba41af8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling_data shape: (441, 60, 130)\n",
      "seq count: 441\n",
      "seq length: 60\n",
      "batch size: 1\n",
      "train_x: torch.Size([440, 1, 60, 128])\n",
      "train_y: torch.Size([440, 1, 2, 1])\n",
      "test_x:  torch.Size([1, 1, 60, 128])\n",
      "test_y:  torch.Size([1, 1, 2, 1])\n",
      "train_batch_count: 440\n",
      "test_batch_count:  1\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset.insert(1, 'future2',dataset.future)\n",
    "dataset['future2'] = dataset['future'].shift(-1)\n",
    "dataset = dataset.fillna(0)\n",
    "\n",
    "# print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "\n",
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 60\n",
    "BATCH_SIZE = 1                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_COUNT = 1\n",
    "Y_SEQ_LEN = 2                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "Y_DIM = 1\n",
    "X_DIM = dataset.shape[1]-Y_SEQ_LEN                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                   # 数据一共是 seq_count x seq_len x (x_in_dim+Y_SEQ_LEN) \n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(SEQ_LENGTH))\n",
    "print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "\n",
    "train = rolling_data[:-1].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "test  = rolling_data[-BATCH_SIZE:].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)  # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "train = torch.tensor(train)\n",
    "test  = torch.tensor(test)\n",
    "\n",
    "# train = rolling_data[:train_batch_count, :, :, :]\n",
    "# test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_SEQ_LEN:], train[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_SEQ_LEN:],  test[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y_seq_len]  to  [test_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train.shape[0]))\n",
    "print(\"test_batch_count:  {}\".format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac5928d-7984-49aa-b1ae-57e902a703f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder & Decoder class\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size = self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        # print(\"Encoder self.input_dim  : {}\".format(self.input_dim))\n",
    "        # print(\"Encoder self.hidden_dim  : {}\".format(self.hidden_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"Encoder forward() x: {}\".format(x.shape))\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "        # print(\"Encoder outputs :{}\".format(outputs.shape))\n",
    "        # print(\"Encoder h_n     :{}\".format(h_n.shape))\n",
    "        # print(\"Encoder c_n     :{}\".format(c_n.shape))\n",
    "        return outputs, h_n, c_n\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.fc_in = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input : input batch data, size(input): [batch_size, feature_size]\n",
    "        # notice input only has two dimensions since the input is batchs\n",
    "        # of last coordinate of observed trajectory so the sequence length has been removed.\n",
    "        \n",
    "        # add sequence dimension to input, to allow use of nn.LSTM\n",
    "        # print(\"Decoder forward() input size : {}\".format(input.shape))\n",
    "        # print(\"Decoder forward() hidden size: {}\".format(hidden.shape))\n",
    "        # print(\"Decoder forward() cell size  : {}\".format(cell.shape))\n",
    "        \n",
    "        input = self.fc_in(input)\n",
    "\n",
    "        lstm_output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        \n",
    "        # print(\"Decoder forward() lstm_output: {}\".format(lstm_output.shape))\n",
    "        \n",
    "        prediction = self.fc_out(lstm_output)         # prediction is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80326ab-dd8a-4bdb-96e4-a3480314b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model class\n",
    "\n",
    "DEC_INPUT_DIM   = 1\n",
    "HIDDEN_DIM      = 768\n",
    "NUM_LAYERS      = 5\n",
    "ENC_DROPOUT     = 0.5\n",
    "DEC_DROPOUT     = 0.5\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.decoder_fc_init= nn.Linear(encoder.input_dim, decoder.input_dim)\n",
    "        # self.decoder_fc_input= nn.Linear(decoder.output_dim, decoder.input_dim)\n",
    "        \n",
    "        assert (encoder.hidden_dim == decoder.hidden_dim), \"hidden dimension in encoder and decoder must be equal\"       \n",
    "        assert (encoder.num_layers == decoder.num_layers), \"hidden layer numbers in encoder and decoder must be equal\"\n",
    "        \n",
    "            \n",
    "    def forward(self, x, y, teacher_forcing_ratio = 0.5):\n",
    "        # x is the input to the encoder.\n",
    "        # y is the output from the decoder\n",
    "        # x = [batch size, encoder_in_sequence_len,  encoder_in_dim]               encoder_in_sequence_len=60, encoder_in_dim=122\n",
    "        # y = [batch size, decoder_out_sequence_len, decoder_out_dim]              decoder_out_sequence_len=2, decoder_out_dim=1    \n",
    "        # print(\"Seq2Seq forwar() x shape : {}\".format(x.shape))\n",
    "        # print(\"Seq2Seq forwar() y shape : {}\".format(y.shape))\n",
    "\n",
    "        decoder_out_seq_len = y.shape[1]                                                     # This is most important that define the output length\n",
    "        \n",
    "        # tensor to store decoder outputs of each time step; this outputs will calc loss with y, so its shape is same as y\n",
    "        outputs = torch.zeros(y.shape).to(device)\n",
    "        # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))\n",
    "        \n",
    "        _, hidden, cell = self.encoder(x)\n",
    "        # print(\"encoder hidden shape: {}\".format(hidden.shape))                            # [encoder_hidden_layer_number, batch_size, encoder_hidden_dim]\n",
    "        # print(\"encoder cell shape :  {}\".format(cell.shape))                              # [encoder_hidden_layer_number, batch_size, encoder_hidden_dim]\n",
    "        \n",
    "        # first input to decoder may be last coordinates of x to predict the future: [last_x]+[h_n,c_n] --> [model] --> [future_y]\n",
    "        decoder_input = x[:, -1, :]                                                           # [batch_size, encoder_input_dim] Get last elements of sequences of the batch from input x\n",
    "        decoder_input = decoder_input.unsqueeze(1)                                            # [batch_size, 1, encoder_input_dim] Get last element of sequence of the batch in encoder\n",
    "        # print(\"decoder_input: {}\".format(decoder_input.shape))\n",
    "        decoder_input = self.decoder_fc_init(decoder_input)                                   # [batch_size, 1, decoder_input_dim] Conver to 1st element of sequence of the batch in decoder\n",
    "        # print(\"decoder_input: {}\".format(decoder_input.shape))\n",
    "        \n",
    "        # Becasue the input and target have different sequence length, Get the target prediction one by one [Prev_prediction]+[h_n,c_n] --> [model] --> [Prediction]\n",
    "        for i in range(decoder_out_seq_len):\n",
    "            # run the decoder for one time step\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            # print(\"Seq2Seq forward() output shape: {}\".format(output.shape))\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each time step\n",
    "            outputs[:,i,:] = output[:,0]\n",
    "            # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))            \n",
    "\n",
    "            # assign this prediction as next prediction's input\n",
    "            decoder_input = output\n",
    "            # print(\"Seq2Seq forward() decoder_input shape: {}\".format(output.shape))\n",
    "            \n",
    "            # 或者使用teacher_forcing来优化\n",
    "            teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            decoder_input = y[:, i, :].unsqueeze(1) if teacher_forcing else output\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9dd074-51e0-475f-a64c-f817959b185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "encoder = Encoder(input_dim=X_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, dropout=ENC_DROPOUT)\n",
    "decoder = Decoder(input_dim=DEC_INPUT_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, output_dim=Y_DIM, dropout=DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).double().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7692e6fe-3005-40a5-8f3e-01cfaeed148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 300 epoch loss: 3.806984\n",
      "1 of 300 epoch loss: 3.854328\n",
      "2 of 300 epoch loss: 3.756763\n",
      "3 of 300 epoch loss: 3.735371\n",
      "4 of 300 epoch loss: 3.710433\n",
      "5 of 300 epoch loss: 3.710085\n",
      "6 of 300 epoch loss: 3.647564\n",
      "7 of 300 epoch loss: 3.598686\n",
      "8 of 300 epoch loss: 3.597121\n",
      "9 of 300 epoch loss: 3.679762\n",
      "10 of 300 epoch loss: 3.647173\n",
      "11 of 300 epoch loss: 3.591777\n",
      "12 of 300 epoch loss: 3.514430\n",
      "13 of 300 epoch loss: 3.476100\n",
      "14 of 300 epoch loss: 3.452982\n",
      "15 of 300 epoch loss: 3.475161\n",
      "16 of 300 epoch loss: 3.360468\n",
      "17 of 300 epoch loss: 3.401718\n",
      "18 of 300 epoch loss: 3.461853\n",
      "19 of 300 epoch loss: 3.429134\n",
      "20 of 300 epoch loss: 3.428367\n",
      "21 of 300 epoch loss: 3.357182\n",
      "22 of 300 epoch loss: 3.350182\n",
      "23 of 300 epoch loss: 3.305694\n",
      "24 of 300 epoch loss: 3.310776\n",
      "25 of 300 epoch loss: 3.154868\n",
      "26 of 300 epoch loss: 3.039492\n",
      "27 of 300 epoch loss: 3.077756\n",
      "28 of 300 epoch loss: 2.847634\n",
      "29 of 300 epoch loss: 3.208609\n",
      "30 of 300 epoch loss: 2.948700\n",
      "31 of 300 epoch loss: 2.795785\n",
      "32 of 300 epoch loss: 2.682173\n",
      "33 of 300 epoch loss: 2.778225\n",
      "34 of 300 epoch loss: 2.766966\n",
      "35 of 300 epoch loss: 2.604785\n",
      "36 of 300 epoch loss: 2.519367\n",
      "37 of 300 epoch loss: 2.430078\n",
      "38 of 300 epoch loss: 2.306665\n",
      "39 of 300 epoch loss: 2.483752\n",
      "40 of 300 epoch loss: 2.426998\n",
      "41 of 300 epoch loss: 2.369624\n",
      "42 of 300 epoch loss: 2.202292\n",
      "43 of 300 epoch loss: 2.128090\n",
      "44 of 300 epoch loss: 2.098583\n",
      "45 of 300 epoch loss: 2.066688\n",
      "46 of 300 epoch loss: 1.925942\n",
      "47 of 300 epoch loss: 1.927125\n",
      "48 of 300 epoch loss: 1.807008\n",
      "49 of 300 epoch loss: 1.803065\n",
      "50 of 300 epoch loss: 1.752383\n",
      "51 of 300 epoch loss: 1.718239\n",
      "52 of 300 epoch loss: 1.772857\n",
      "53 of 300 epoch loss: 1.730747\n",
      "54 of 300 epoch loss: 1.715805\n",
      "55 of 300 epoch loss: 1.698849\n",
      "56 of 300 epoch loss: 1.586344\n",
      "57 of 300 epoch loss: 1.561313\n",
      "58 of 300 epoch loss: 1.585389\n",
      "59 of 300 epoch loss: 1.596419\n",
      "60 of 300 epoch loss: 1.564603\n",
      "61 of 300 epoch loss: 1.494540\n",
      "62 of 300 epoch loss: 1.571160\n",
      "63 of 300 epoch loss: 1.524366\n",
      "64 of 300 epoch loss: 1.519554\n",
      "65 of 300 epoch loss: 1.580274\n",
      "66 of 300 epoch loss: 1.456423\n",
      "67 of 300 epoch loss: 1.485876\n",
      "68 of 300 epoch loss: 1.455147\n",
      "69 of 300 epoch loss: 1.472984\n",
      "70 of 300 epoch loss: 1.450355\n",
      "71 of 300 epoch loss: 1.440604\n",
      "72 of 300 epoch loss: 1.386095\n",
      "73 of 300 epoch loss: 1.442323\n",
      "74 of 300 epoch loss: 1.405438\n",
      "75 of 300 epoch loss: 1.427468\n",
      "76 of 300 epoch loss: 1.373039\n",
      "77 of 300 epoch loss: 1.362766\n",
      "78 of 300 epoch loss: 1.338633\n",
      "79 of 300 epoch loss: 1.381087\n",
      "80 of 300 epoch loss: 1.350325\n",
      "81 of 300 epoch loss: 1.352682\n",
      "82 of 300 epoch loss: 1.358054\n",
      "83 of 300 epoch loss: 1.325707\n",
      "84 of 300 epoch loss: 1.401193\n",
      "85 of 300 epoch loss: 1.338425\n",
      "86 of 300 epoch loss: 1.315310\n",
      "87 of 300 epoch loss: 1.299321\n",
      "88 of 300 epoch loss: 1.293875\n",
      "89 of 300 epoch loss: 1.349627\n",
      "90 of 300 epoch loss: 1.304682\n",
      "91 of 300 epoch loss: 1.285476\n",
      "92 of 300 epoch loss: 1.279514\n",
      "93 of 300 epoch loss: 1.279798\n",
      "94 of 300 epoch loss: 1.300792\n",
      "95 of 300 epoch loss: 1.358680\n",
      "96 of 300 epoch loss: 1.311142\n",
      "97 of 300 epoch loss: 1.300053\n",
      "98 of 300 epoch loss: 1.252932\n",
      "99 of 300 epoch loss: 1.346650\n",
      "100 of 300 epoch loss: 1.277081\n",
      "101 of 300 epoch loss: 1.301710\n",
      "102 of 300 epoch loss: 1.339001\n",
      "103 of 300 epoch loss: 1.267303\n",
      "104 of 300 epoch loss: 1.322103\n",
      "105 of 300 epoch loss: 1.267846\n",
      "106 of 300 epoch loss: 1.314681\n",
      "107 of 300 epoch loss: 1.309471\n",
      "108 of 300 epoch loss: 1.335899\n",
      "109 of 300 epoch loss: 1.317007\n",
      "110 of 300 epoch loss: 1.268271\n",
      "111 of 300 epoch loss: 1.327595\n",
      "112 of 300 epoch loss: 1.259548\n",
      "113 of 300 epoch loss: 1.280947\n",
      "114 of 300 epoch loss: 1.318520\n",
      "115 of 300 epoch loss: 1.296392\n",
      "116 of 300 epoch loss: 1.257666\n",
      "117 of 300 epoch loss: 1.322328\n",
      "118 of 300 epoch loss: 1.344090\n",
      "119 of 300 epoch loss: 1.263238\n",
      "120 of 300 epoch loss: 1.319753\n",
      "121 of 300 epoch loss: 1.303201\n",
      "122 of 300 epoch loss: 1.251621\n",
      "123 of 300 epoch loss: 1.298740\n",
      "124 of 300 epoch loss: 1.237288\n",
      "125 of 300 epoch loss: 1.347528\n",
      "126 of 300 epoch loss: 1.321715\n",
      "127 of 300 epoch loss: 1.292648\n",
      "128 of 300 epoch loss: 1.268030\n",
      "129 of 300 epoch loss: 1.305061\n",
      "130 of 300 epoch loss: 1.233636\n",
      "131 of 300 epoch loss: 1.331485\n",
      "132 of 300 epoch loss: 1.315410\n",
      "133 of 300 epoch loss: 1.304998\n",
      "134 of 300 epoch loss: 1.311051\n",
      "135 of 300 epoch loss: 1.288571\n",
      "136 of 300 epoch loss: 1.287111\n",
      "137 of 300 epoch loss: 1.271159\n",
      "138 of 300 epoch loss: 1.274128\n",
      "139 of 300 epoch loss: 1.299491\n",
      "140 of 300 epoch loss: 1.262910\n",
      "141 of 300 epoch loss: 1.313390\n",
      "142 of 300 epoch loss: 1.273849\n",
      "143 of 300 epoch loss: 1.317424\n",
      "144 of 300 epoch loss: 1.263685\n",
      "145 of 300 epoch loss: 1.272767\n",
      "146 of 300 epoch loss: 1.268937\n",
      "147 of 300 epoch loss: 1.283614\n",
      "148 of 300 epoch loss: 1.263265\n",
      "149 of 300 epoch loss: 1.278743\n",
      "150 of 300 epoch loss: 1.300792\n",
      "151 of 300 epoch loss: 1.283303\n",
      "152 of 300 epoch loss: 1.292167\n",
      "153 of 300 epoch loss: 1.201973\n",
      "154 of 300 epoch loss: 1.286407\n",
      "155 of 300 epoch loss: 1.264363\n",
      "156 of 300 epoch loss: 1.288213\n",
      "157 of 300 epoch loss: 1.325470\n",
      "158 of 300 epoch loss: 1.267581\n",
      "159 of 300 epoch loss: 1.291521\n",
      "160 of 300 epoch loss: 1.268016\n",
      "161 of 300 epoch loss: 1.271179\n",
      "162 of 300 epoch loss: 1.271165\n",
      "163 of 300 epoch loss: 1.267914\n",
      "164 of 300 epoch loss: 1.224226\n",
      "165 of 300 epoch loss: 1.253828\n",
      "166 of 300 epoch loss: 1.282846\n",
      "167 of 300 epoch loss: 1.264568\n",
      "168 of 300 epoch loss: 1.309763\n",
      "169 of 300 epoch loss: 1.214903\n",
      "170 of 300 epoch loss: 1.263737\n",
      "171 of 300 epoch loss: 1.237279\n",
      "172 of 300 epoch loss: 1.290196\n",
      "173 of 300 epoch loss: 1.300561\n",
      "174 of 300 epoch loss: 1.230272\n",
      "175 of 300 epoch loss: 1.253398\n",
      "176 of 300 epoch loss: 1.281476\n",
      "177 of 300 epoch loss: 1.297050\n",
      "178 of 300 epoch loss: 1.284553\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13740/3845537634.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} of {} epoch loss: {:.6f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练 Seq2Seq 模型; \n",
    "LR = 1e-4\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "model.train()\n",
    "epoches = 300\n",
    "epoch_loss = 0\n",
    "epoch_loss_list = []\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_x.shape[0]):\n",
    "        pred = model(train_x[step], train_y[step], teacher_forcing_ratio=0.5)\n",
    "        # print(\"Train pred shape : {}\".format(pred.shape))\n",
    "        # print(\"Train train_y[step] shape : {}\".format(train_y[step].shape))\n",
    "        \n",
    "        loss = loss_func(pred, train_y[step].float())              # this calc the last element's loss between prediction and real.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.data.cpu()\n",
    "\n",
    "    print(\"{} of {} epoch loss: {:.6f}\".format(epoch, epoches, epoch_loss))\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    if (epoch+1)%40 == 0:\n",
    "        scheduler.step()\n",
    "        \n",
    "plt.plot(epoch_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9aaadf6-8d3d-4488-8ad3-a12104efc196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [-0.03312445 -0.07379392]\n",
      "Actual: [0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 用模型预测数据\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "for step in range(test_x.shape[0]):\n",
    "    pred = model(test_x[step], test_y[step], teacher_forcing_ratio=0)                       # 测试集上就不要teacher模式了。\n",
    "\n",
    "    loss = loss_func(pred, test_y[step])\n",
    "    \n",
    "    print(\"Prediction: {}\".format(pred[-1].cpu().detach().flatten().numpy()))\n",
    "    print(\"Actual: {}\".format(test_y[step][-1].cpu().detach().flatten().numpy()))\n",
    "\n",
    "actual_line = test_y[step].cpu().detach().flatten().numpy()\n",
    "pred_line   = pred.cpu().detach().flatten().numpy()\n",
    "plt.plot(actual_line, 'r--')\n",
    "plt.plot(pred_line, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267f17c-8a01-4949-8287-f3368db264d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
