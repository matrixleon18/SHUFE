{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7635ca9d-4939-4a35-a263-2feced7b671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 LSTM 做一个 Seq2Seq 的预测，不考虑准确性\n",
    "## Architecture \n",
    "## General:        [x1, x2, ... , xn]   ==> [Seq2Seq Model] ==> [Prediction1, Prediction2] \n",
    "## Genral Data:    [122-D, ... , 122-D] ==> [Seq2Seq Model] ==> [1-D, 1-D] \n",
    "## Detail :        [x1, x2, ... , xn]   ==> [Encoder] ==> [h_n, c_n] + [xn]    ==> [Decoder] ==> [Prediont1, Prediction2]\n",
    "## Detail Data:    [122-D, ...., 122-D] ==> [Encoder] ==> [h_n, c_n] + [122-D] ==> [Decoder] ==> [1-D, 1-D]\n",
    "## Decoder :       xn, [h_n, c_n] ==> FC(xn), [h_n, c_n] ==> [1-D, h_n, c_n] ==> LSTM(1-D, h_n, c_n) ==> pred1, h1, c1 ==> LSTM(pred1, h, c) ==> pred2, h2, c2 ==> LSTM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea989f2-dc0b-4e59-abdc-9eb17eefc4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling_data shape: (600, 60, 136)\n",
      "seq count: 600\n",
      "seq length: 60\n",
      "train_x: torch.Size([26, 23, 60, 134])\n",
      "train_y: torch.Size([26, 23, 2, 1])\n",
      "test_x:  torch.Size([2, 1, 60, 134])\n",
      "test_y:  torch.Size([2, 1, 2, 1])\n",
      "train_batch_count: 26\n",
      "test_batch_count:  2\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset.insert(1, 'future2',dataset.future)\n",
    "dataset['future2'] = dataset['future'].shift(-1)\n",
    "dataset = dataset.fillna(0)\n",
    "\n",
    "\n",
    "# print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "\n",
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 60\n",
    "TRAIN_BATCH_SIZE = 23                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_SIZE = 1                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_COUNT = 2\n",
    "Y_SEQ_LEN = 2                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "Y_DIM = 1\n",
    "X_DIM = dataset.shape[1]-Y_SEQ_LEN                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                   # 数据一共是 seq_count x seq_len x (x_in_dim+Y_SEQ_LEN) \n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(SEQ_LENGTH))\n",
    "# print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "test_seq_count = TEST_BATCH_COUNT * TEST_BATCH_SIZE\n",
    "\n",
    "\n",
    "# train = rolling_data[:-test_seq_count].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "# test  = rolling_data[-test_seq_count:].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "train = rolling_data[:-test_seq_count].reshape(-1, TRAIN_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                    # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "test  = rolling_data[-test_seq_count:].reshape(-1, TEST_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)      # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "TRAIN_BATCH_SIZE = train.shape[1]\n",
    "TRAIN_BATCH_COUNT = train.shape[0]\n",
    "TEST_BATCH_SIZE = test.shape[1]\n",
    "TEST_BATCH_COUNT = test.shape[0]\n",
    "\n",
    "train = torch.tensor(train)\n",
    "test  = torch.tensor(test)\n",
    "\n",
    "# train = rolling_data[:train_batch_count, :, :, :]\n",
    "# test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_SEQ_LEN:], train[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_SEQ_LEN:],  test[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y_seq_len]  to  [test_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train.shape[0]))\n",
    "print(\"test_batch_count:  {}\".format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac5928d-7984-49aa-b1ae-57e902a703f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder & Decoder class\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size = self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        # print(\"Encoder self.input_dim  : {}\".format(self.input_dim))\n",
    "        # print(\"Encoder self.hidden_dim  : {}\".format(self.hidden_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"Encoder forward() x: {}\".format(x.shape))\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "        # print(\"Encoder outputs :{}\".format(outputs.shape))\n",
    "        # print(\"Encoder h_n     :{}\".format(h_n.shape))\n",
    "        # print(\"Encoder c_n     :{}\".format(c_n.shape))\n",
    "        return outputs, h_n, c_n\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.fc_in = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input : input batch data, size(input): [batch_size, feature_size]\n",
    "        # notice input only has two dimensions since the input is batchs\n",
    "        # of last coordinate of observed trajectory so the sequence length has been removed.\n",
    "        \n",
    "        # add sequence dimension to input, to allow use of nn.LSTM\n",
    "        # print(\"Decoder forward() input size : {}\".format(input.shape))\n",
    "        # print(\"Decoder forward() hidden size: {}\".format(hidden.shape))\n",
    "        # print(\"Decoder forward() cell size  : {}\".format(cell.shape))\n",
    "        \n",
    "        input = self.fc_in(input)\n",
    "\n",
    "        lstm_output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        \n",
    "        # print(\"Decoder forward() lstm_output: {}\".format(lstm_output.shape))\n",
    "        \n",
    "        prediction = self.fc_out(lstm_output)         # prediction is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80326ab-dd8a-4bdb-96e4-a3480314b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model class\n",
    "\n",
    "DEC_INPUT_DIM   = 1\n",
    "HIDDEN_DIM      = 768\n",
    "NUM_LAYERS      = 5\n",
    "ENC_DROPOUT     = 0.1\n",
    "DEC_DROPOUT     = 0.1\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.decoder_fc_init= nn.Linear(encoder.input_dim, decoder.input_dim)\n",
    "        # self.decoder_fc_input= nn.Linear(decoder.output_dim, decoder.input_dim)\n",
    "        \n",
    "        assert (encoder.hidden_dim == decoder.hidden_dim), \"hidden dimension in encoder and decoder must be equal\"       \n",
    "        assert (encoder.num_layers == decoder.num_layers), \"hidden layer numbers in encoder and decoder must be equal\"\n",
    "        \n",
    "            \n",
    "    def forward(self, x, y, teacher_forcing_ratio = 0.5):\n",
    "        # x is the input to the encoder.\n",
    "        # y is the output from the decoder\n",
    "        # x = [batch size, encoder_in_sequence_len,  encoder_in_dim]               encoder_in_sequence_len=60, encoder_in_dim=122\n",
    "        # y = [batch size, decoder_out_sequence_len, decoder_out_dim]              decoder_out_sequence_len=2, decoder_out_dim=1    \n",
    "        # print(\"Seq2Seq forwar() x shape : {}\".format(x.shape))\n",
    "        # print(\"Seq2Seq forwar() y shape : {}\".format(y.shape))\n",
    "\n",
    "        decoder_out_seq_len = y.shape[1]                                                     # This is most important that define the output length\n",
    "        \n",
    "        # tensor to store decoder outputs of each time step; this outputs will calc loss with y, so its shape is same as y\n",
    "        outputs = torch.zeros(y.shape).to(device)\n",
    "        # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))\n",
    "        \n",
    "        _, hidden, cell = self.encoder(x)\n",
    "        # print(\"encoder hidden shape: {}\".format(hidden.shape))                            # [encoder_hidden_layer_number, batch_size, encoder_hidden_dim]\n",
    "        # print(\"encoder cell shape :  {}\".format(cell.shape))                              # [encoder_hidden_layer_number, batch_size, encoder_hidden_dim]\n",
    "        \n",
    "        # first input to decoder may be last coordinates of x to predict the future: [last_x]+[h_n,c_n] --> [model] --> [future_y]\n",
    "        decoder_input = x[:, -1, :]                                                           # [batch_size, encoder_input_dim] Get last elements of sequences of the batch from input x\n",
    "        decoder_input = decoder_input.unsqueeze(1)                                            # [batch_size, 1, encoder_input_dim] Get last element of sequence of the batch in encoder\n",
    "        # print(\"decoder_input: {}\".format(decoder_input.shape))\n",
    "        decoder_input = self.decoder_fc_init(decoder_input)                                   # [batch_size, 1, decoder_input_dim] Conver to 1st element of sequence of the batch in decoder\n",
    "        # print(\"decoder_input: {}\".format(decoder_input.shape))\n",
    "        \n",
    "        # Becasue the input and target have different sequence length, Get the target prediction one by one [Prev_prediction]+[h_n,c_n] --> [model] --> [Prediction]\n",
    "        for i in range(decoder_out_seq_len):\n",
    "            # run the decoder for one time step\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            # print(\"Seq2Seq forward() output shape: {}\".format(output.shape))\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each time step\n",
    "            outputs[:,i,:] = output[:,0]\n",
    "            # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))            \n",
    "\n",
    "            # assign this prediction as next prediction's input\n",
    "            decoder_input = output\n",
    "            # print(\"Seq2Seq forward() decoder_input shape: {}\".format(output.shape))\n",
    "            \n",
    "            # 或者使用teacher_forcing来优化\n",
    "            teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            decoder_input = y[:, i, :].unsqueeze(1) if teacher_forcing else output\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9dd074-51e0-475f-a64c-f817959b185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "encoder = Encoder(input_dim=X_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, dropout=ENC_DROPOUT)\n",
    "decoder = Decoder(input_dim=DEC_INPUT_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, output_dim=Y_DIM, dropout=DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).double().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7692e6fe-3005-40a5-8f3e-01cfaeed148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 200 epoch loss:   0.2164   with  lr:   0.0001\n",
      "1 of 200 epoch loss:   0.2142   with  lr:   0.0001\n",
      "2 of 200 epoch loss:   0.2143   with  lr:   0.0001\n",
      "3 of 200 epoch loss:   0.2140   with  lr:   0.0001\n",
      "4 of 200 epoch loss:   0.2145   with  lr:   0.0001\n",
      "5 of 200 epoch loss:   0.2137   with  lr:   0.0001\n",
      "6 of 200 epoch loss:   0.2149   with  lr:   0.0001\n",
      "7 of 200 epoch loss:   0.2143   with  lr:   0.0001\n",
      "8 of 200 epoch loss:   0.2116   with  lr:   0.0001\n",
      "9 of 200 epoch loss:   0.2094   with  lr:   0.0001\n",
      "10 of 200 epoch loss:   0.2077   with  lr:   0.0001\n",
      "11 of 200 epoch loss:   0.2090   with  lr:   0.0001\n",
      "12 of 200 epoch loss:   0.2052   with  lr:   0.0001\n",
      "13 of 200 epoch loss:   0.2048   with  lr:   0.0001\n",
      "14 of 200 epoch loss:   0.2023   with  lr:   0.0001\n",
      "15 of 200 epoch loss:   0.2065   with  lr:   0.0001\n",
      "16 of 200 epoch loss:   0.2010   with  lr:   0.0001\n",
      "17 of 200 epoch loss:   0.2000   with  lr:   0.0001\n",
      "18 of 200 epoch loss:   0.1951   with  lr:   0.0001\n",
      "19 of 200 epoch loss:   0.1961   with  lr:   0.0001\n",
      "20 of 200 epoch loss:   0.2020   with  lr:   0.0001\n",
      "21 of 200 epoch loss:   0.1985   with  lr:   0.0001\n",
      "22 of 200 epoch loss:   0.1942   with  lr:   0.0001\n",
      "23 of 200 epoch loss:   0.1926   with  lr:   0.0001\n",
      "24 of 200 epoch loss:   0.1968   with  lr:   0.0001\n",
      "25 of 200 epoch loss:   0.2830   with  lr:   0.0001\n",
      "26 of 200 epoch loss:   0.2025   with  lr:   0.0001\n",
      "27 of 200 epoch loss:   0.2000   with  lr:   0.0001\n",
      "28 of 200 epoch loss:   0.1965   with  lr:   0.0001\n",
      "29 of 200 epoch loss:   0.1949   with  lr:   0.0001\n",
      "30 of 200 epoch loss:   0.1939   with  lr:   0.0001\n",
      "31 of 200 epoch loss:   0.1910   with  lr:   0.0001\n",
      "32 of 200 epoch loss:   0.1909   with  lr:   0.0001\n",
      "33 of 200 epoch loss:   0.1874   with  lr:   0.0001\n",
      "34 of 200 epoch loss:   0.1857   with  lr:   0.0001\n",
      "35 of 200 epoch loss:   0.1842   with  lr:   0.0001\n",
      "36 of 200 epoch loss:   0.1833   with  lr:   0.0001\n",
      "37 of 200 epoch loss:   0.1807   with  lr:   0.0001\n",
      "38 of 200 epoch loss:   0.1880   with  lr:   0.0001\n",
      "39 of 200 epoch loss:   0.1989   with  lr:   0.0001\n",
      "40 of 200 epoch loss:   0.1891   with  lr:   0.0001\n",
      "41 of 200 epoch loss:   0.1857   with  lr:   0.0001\n",
      "42 of 200 epoch loss:   0.1791   with  lr:   0.0001\n",
      "43 of 200 epoch loss:   0.1776   with  lr:   0.0001\n",
      "44 of 200 epoch loss:   0.1783   with  lr:   0.0001\n",
      "45 of 200 epoch loss:   0.1814   with  lr:   0.0001\n",
      "46 of 200 epoch loss:   0.1909   with  lr:   0.0001\n",
      "47 of 200 epoch loss:   0.1744   with  lr:   0.0001\n",
      "48 of 200 epoch loss:   0.1700   with  lr:   0.0001\n",
      "49 of 200 epoch loss:   0.1672   with  lr:   0.0001\n",
      "50 of 200 epoch loss:   0.1850   with  lr:   0.0001\n",
      "51 of 200 epoch loss:   0.1750   with  lr:   0.0001\n",
      "52 of 200 epoch loss:   0.1650   with  lr:   0.0001\n",
      "53 of 200 epoch loss:   0.1621   with  lr:   0.0001\n",
      "54 of 200 epoch loss:   0.1609   with  lr:   0.0001\n",
      "55 of 200 epoch loss:   0.1626   with  lr:   0.0001\n",
      "56 of 200 epoch loss:   0.1659   with  lr:   0.0001\n",
      "57 of 200 epoch loss:   0.1670   with  lr:   0.0001\n",
      "58 of 200 epoch loss:   0.1873   with  lr:   0.0001\n",
      "59 of 200 epoch loss:   0.1795   with  lr:   0.0001\n",
      "60 of 200 epoch loss:   0.1691   with  lr:   0.0001\n",
      "61 of 200 epoch loss:   0.1602   with  lr:   0.0001\n",
      "62 of 200 epoch loss:   0.1548   with  lr:   0.0001\n",
      "63 of 200 epoch loss:   0.1652   with  lr:   0.0001\n",
      "64 of 200 epoch loss:   0.1586   with  lr:   0.0001\n",
      "65 of 200 epoch loss:   0.1434   with  lr:   0.0001\n",
      "66 of 200 epoch loss:   0.1355   with  lr:   0.0001\n",
      "67 of 200 epoch loss:   0.1327   with  lr:   0.0001\n",
      "68 of 200 epoch loss:   0.1331   with  lr:   0.0001\n",
      "69 of 200 epoch loss:   0.1383   with  lr:   0.0001\n",
      "70 of 200 epoch loss:   0.1465   with  lr:   0.0001\n",
      "71 of 200 epoch loss:   0.1464   with  lr:   0.0001\n",
      "72 of 200 epoch loss:   0.1510   with  lr:   0.0001\n",
      "73 of 200 epoch loss:   0.1366   with  lr:   0.0001\n",
      "74 of 200 epoch loss:   0.1465   with  lr:   0.0001\n",
      "75 of 200 epoch loss:   0.1404   with  lr:   0.0001\n",
      "76 of 200 epoch loss:   0.1527   with  lr:   0.0001\n",
      "77 of 200 epoch loss:   0.1701   with  lr:   0.0001\n",
      "78 of 200 epoch loss:   0.1570   with  lr:   0.0001\n",
      "79 of 200 epoch loss:   0.1453   with  lr:   0.0001\n",
      "80 of 200 epoch loss:   0.1540   with  lr:   0.0001\n",
      "81 of 200 epoch loss:   0.1454   with  lr:   0.0001\n",
      "82 of 200 epoch loss:   0.1349   with  lr:   0.0001\n",
      "83 of 200 epoch loss:   0.1374   with  lr:   0.0001\n",
      "84 of 200 epoch loss:   0.1216   with  lr:   0.0001\n",
      "85 of 200 epoch loss:   0.1210   with  lr:   0.0001\n",
      "86 of 200 epoch loss:   0.1176   with  lr:   0.0001\n",
      "87 of 200 epoch loss:   0.1216   with  lr:   0.0001\n",
      "88 of 200 epoch loss:   0.1327   with  lr:   0.0001\n",
      "89 of 200 epoch loss:   0.1269   with  lr:   0.0001\n",
      "90 of 200 epoch loss:   0.1163   with  lr:   0.0001\n",
      "91 of 200 epoch loss:   0.1150   with  lr:   0.0001\n",
      "92 of 200 epoch loss:   0.1157   with  lr:   0.0001\n",
      "93 of 200 epoch loss:   0.1236   with  lr:   0.0001\n",
      "94 of 200 epoch loss:   0.1251   with  lr:   0.0001\n",
      "95 of 200 epoch loss:   0.1129   with  lr:   0.0001\n",
      "96 of 200 epoch loss:   0.1151   with  lr:   0.0001\n",
      "97 of 200 epoch loss:   0.1295   with  lr:   0.0001\n",
      "98 of 200 epoch loss:   0.1195   with  lr:   0.0001\n",
      "99 of 200 epoch loss:   0.1197   with  lr:   0.0001\n",
      "100 of 200 epoch loss:   0.1170   with  lr:   0.0001\n",
      "101 of 200 epoch loss:   0.1127   with  lr:   0.0001\n",
      "102 of 200 epoch loss:   0.1150   with  lr:   0.0001\n",
      "103 of 200 epoch loss:   0.1133   with  lr:   0.0001\n",
      "104 of 200 epoch loss:   0.1053   with  lr:   0.0001\n",
      "105 of 200 epoch loss:   0.1202   with  lr:   0.0001\n",
      "106 of 200 epoch loss:   0.1245   with  lr:   0.0001\n",
      "107 of 200 epoch loss:   0.1229   with  lr:   0.0001\n",
      "108 of 200 epoch loss:   0.1227   with  lr:   0.0001\n",
      "109 of 200 epoch loss:   0.1096   with  lr:   0.0001\n",
      "110 of 200 epoch loss:   0.1049   with  lr:   0.0001\n",
      "111 of 200 epoch loss:   0.1011   with  lr:   0.0001\n",
      "112 of 200 epoch loss:   0.1038   with  lr:   0.0001\n",
      "113 of 200 epoch loss:   0.1090   with  lr:   0.0001\n",
      "114 of 200 epoch loss:   0.1059   with  lr:   0.0001\n",
      "115 of 200 epoch loss:   0.1027   with  lr:   0.0001\n",
      "116 of 200 epoch loss:   0.1055   with  lr:   0.0001\n",
      "117 of 200 epoch loss:   0.1008   with  lr:   0.0001\n",
      "118 of 200 epoch loss:   0.1084   with  lr:   0.0001\n",
      "119 of 200 epoch loss:   0.1183   with  lr:   0.0001\n",
      "120 of 200 epoch loss:   0.0999   with  lr:   0.0001\n",
      "121 of 200 epoch loss:   0.1047   with  lr:   0.0001\n",
      "122 of 200 epoch loss:   0.0924   with  lr:   0.0001\n",
      "123 of 200 epoch loss:   0.1022   with  lr:   0.0001\n",
      "124 of 200 epoch loss:   0.1108   with  lr:   0.0001\n",
      "125 of 200 epoch loss:   0.0985   with  lr:   0.0001\n",
      "126 of 200 epoch loss:   0.0925   with  lr:   0.0001\n",
      "127 of 200 epoch loss:   0.1250   with  lr:   0.0001\n",
      "128 of 200 epoch loss:   0.1192   with  lr:   0.0001\n",
      "129 of 200 epoch loss:   0.1032   with  lr:   0.0001\n",
      "130 of 200 epoch loss:   0.0932   with  lr:   0.0001\n",
      "131 of 200 epoch loss:   0.0876   with  lr:   0.0001\n",
      "132 of 200 epoch loss:   0.0887   with  lr:   0.0001\n",
      "133 of 200 epoch loss:   0.0898   with  lr:   0.0001\n",
      "134 of 200 epoch loss:   0.0959   with  lr:   0.0001\n",
      "135 of 200 epoch loss:   0.0955   with  lr:   0.0001\n",
      "136 of 200 epoch loss:   0.0998   with  lr:   0.0001\n",
      "137 of 200 epoch loss:   0.0967   with  lr:   0.0001\n",
      "138 of 200 epoch loss:   0.0871   with  lr:   0.0001\n",
      "139 of 200 epoch loss:   0.0870   with  lr:   0.0001\n",
      "140 of 200 epoch loss:   0.0913   with  lr:   0.0001\n",
      "141 of 200 epoch loss:   0.1009   with  lr:   0.0001\n",
      "142 of 200 epoch loss:   0.0965   with  lr:   0.0001\n",
      "143 of 200 epoch loss:   0.0881   with  lr:   0.0001\n",
      "144 of 200 epoch loss:   0.0842   with  lr:   0.0001\n",
      "145 of 200 epoch loss:   0.0855   with  lr:   0.0001\n",
      "146 of 200 epoch loss:   0.0856   with  lr:   0.0001\n",
      "147 of 200 epoch loss:   0.0872   with  lr:   0.0001\n",
      "148 of 200 epoch loss:   0.0883   with  lr:   0.0001\n",
      "149 of 200 epoch loss:   0.0823   with  lr:   0.0001\n",
      "150 of 200 epoch loss:   0.1003   with  lr:   0.0001\n",
      "151 of 200 epoch loss:   0.1019   with  lr:   0.0001\n",
      "152 of 200 epoch loss:   0.0918   with  lr:   0.0001\n",
      "153 of 200 epoch loss:   0.0924   with  lr:   0.0001\n",
      "154 of 200 epoch loss:   0.0772   with  lr:   0.0001\n",
      "155 of 200 epoch loss:   0.0872   with  lr:   0.0001\n",
      "156 of 200 epoch loss:   0.0820   with  lr:   0.0001\n",
      "157 of 200 epoch loss:   0.0787   with  lr:   0.0001\n",
      "158 of 200 epoch loss:   0.0715   with  lr:   0.0001\n",
      "159 of 200 epoch loss:   0.0758   with  lr:   0.0001\n",
      "160 of 200 epoch loss:   0.0853   with  lr:   0.0001\n",
      "161 of 200 epoch loss:   0.0864   with  lr:   0.0001\n",
      "162 of 200 epoch loss:   0.0795   with  lr:   0.0001\n",
      "163 of 200 epoch loss:   0.0804   with  lr:   0.0001\n",
      "164 of 200 epoch loss:   0.0739   with  lr:   0.0001\n",
      "165 of 200 epoch loss:   0.0756   with  lr:   0.0001\n",
      "166 of 200 epoch loss:   0.0753   with  lr:   0.0001\n",
      "167 of 200 epoch loss:   0.0868   with  lr:   0.0001\n",
      "168 of 200 epoch loss:   0.0790   with  lr:   0.0001\n",
      "169 of 200 epoch loss:   0.0735   with  lr:   0.0001\n",
      "170 of 200 epoch loss:   0.0738   with  lr:   0.0001\n",
      "171 of 200 epoch loss:   0.0806   with  lr:   0.0001\n",
      "172 of 200 epoch loss:   0.0749   with  lr:   0.0001\n",
      "173 of 200 epoch loss:   0.0656   with  lr:   0.0001\n",
      "174 of 200 epoch loss:   0.0697   with  lr:   0.0001\n",
      "175 of 200 epoch loss:   0.0678   with  lr:   0.0001\n",
      "176 of 200 epoch loss:   0.0630   with  lr:   0.0001\n",
      "177 of 200 epoch loss:   0.0609   with  lr:   0.0001\n",
      "178 of 200 epoch loss:   0.0713   with  lr:   0.0001\n",
      "179 of 200 epoch loss:   0.0676   with  lr:   0.0001\n",
      "180 of 200 epoch loss:   0.0669   with  lr:   0.0001\n",
      "181 of 200 epoch loss:   0.0701   with  lr:   0.0001\n",
      "182 of 200 epoch loss:   0.0660   with  lr:   0.0001\n",
      "183 of 200 epoch loss:   0.0822   with  lr:   0.0001\n",
      "184 of 200 epoch loss:   0.0719   with  lr:   0.0001\n",
      "185 of 200 epoch loss:   0.0664   with  lr:   0.0001\n",
      "186 of 200 epoch loss:   0.0599   with  lr:   0.0001\n",
      "187 of 200 epoch loss:   0.0617   with  lr:   0.0001\n",
      "188 of 200 epoch loss:   0.0667   with  lr:   0.0001\n",
      "189 of 200 epoch loss:   0.0606   with  lr:   0.0001\n",
      "190 of 200 epoch loss:   0.0579   with  lr:   0.0001\n",
      "191 of 200 epoch loss:   0.0544   with  lr:   0.0001\n",
      "192 of 200 epoch loss:   0.0615   with  lr:   0.0001\n",
      "193 of 200 epoch loss:   0.0569   with  lr:   0.0001\n",
      "194 of 200 epoch loss:   0.0743   with  lr:   0.0001\n",
      "195 of 200 epoch loss:   0.1003   with  lr:   0.0001\n",
      "196 of 200 epoch loss:   0.0825   with  lr:   0.0001\n",
      "197 of 200 epoch loss:   0.0695   with  lr:   0.0001\n",
      "198 of 200 epoch loss:   0.0662   with  lr:   0.0001\n",
      "199 of 200 epoch loss:   0.0582   with  lr:   0.0001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练 Seq2Seq 模型; \n",
    "LR = 1e-4\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "model.train()\n",
    "epoches = 200\n",
    "epoch_loss = 0\n",
    "epoch_loss_list = []\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_x.shape[0]):\n",
    "        pred = model(train_x[step], train_y[step], teacher_forcing_ratio=0.5)\n",
    "        # print(\"Train pred shape : {}\".format(pred.shape))\n",
    "        # print(\"Train train_y[step] shape : {}\".format(train_y[step].shape))\n",
    "        \n",
    "        loss = loss_func(pred, train_y[step].float())              # this calc the last element's loss between prediction and real.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.data.cpu()\n",
    "\n",
    "    if epoch_loss.item() < 1e-4:\n",
    "        print('Epoch [{}/{}], Loss: {:.5f}'.format(epoch+1, epoches, loss.item()))\n",
    "        print(\"The loss value is reached\")\n",
    "        break\n",
    "        \n",
    "    print(\"{} of {} epoch loss:   {:.4f}   with  lr:   {}\".format(epoch, epoches, epoch_loss.item(), optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    if (epoch+1) % 2000 ==0:\n",
    "        scheduler.step()\n",
    "        \n",
    "plt.plot(epoch_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6632718e-ce6d-497f-ab14-be276835cbbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model, 'e:\\\\Model_Seq2Seq.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "577462c4-1fca-480e-aeef-110a96855779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model = torch.load('e:\\\\Model_Seq2Seq.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9aaadf6-8d3d-4488-8ad3-a12104efc196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.02871585 0.05555358]\n",
      "Actual: [0.06 0.  ]\n",
      "Prediction: [0.01565927 0.03599594]\n",
      "Actual: [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 用模型预测数据\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_batch_count = test_x.shape[0]\n",
    "\n",
    "for step in range(test_batch_count):\n",
    "    pred = model(test_x[step], test_y[step], teacher_forcing_ratio=0)                       # 测试集上就不要teacher模式了。\n",
    "\n",
    "    loss = loss_func(pred, test_y[step])\n",
    "    \n",
    "    print(\"Prediction: {}\".format(pred[-1].cpu().detach().flatten().numpy()))\n",
    "    print(\"Actual: {}\".format(test_y[step][-1].cpu().detach().flatten().numpy()))\n",
    "\n",
    "# actual_line = test_y[step].cpu().detach().flatten().numpy()\n",
    "# pred_line   = pred.cpu().detach().flatten().numpy()\n",
    "# plt.plot(actual_line, 'r--')\n",
    "# plt.plot(pred_line, 'b-')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267f17c-8a01-4949-8287-f3368db264d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
