{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7635ca9d-4939-4a35-a263-2feced7b671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 LSTM 做一个 Seq2Seq 的预测，不考虑准确性\n",
    "## Architecture \n",
    "## General:        [x1, x2, ... , xn]   ==> [Seq2Seq Model] ==> [Prediction1, Prediction2] \n",
    "## Genral Data:    [122-D, ... , 122-D] ==> [Seq2Seq Model] ==> [1-D, 1-D] \n",
    "## Detail :        [x1, x2, ... , xn]   ==> [Encoder] ==> [h_n, c_n] + [xn]    ==> [Decoder] ==> [Prediont1, Prediction2]\n",
    "## Detail Data:    [122-D, ...., 122-D] ==> [Encoder] ==> [h_n, c_n] + [122-D] ==> [Decoder] ==> [1-D, 1-D]\n",
    "## Decoder :       xn, [h_n, c_n] ==> FC(xn), [h_n, c_n] ==> [1-D, h_n, c_n] ==> LSTM(1-D, h_n, c_n) ==> pred1, h1, c1 ==> LSTM(pred1, h, c) ==> pred2, h2, c2 ==> LSTM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fb6d43-f680-43cf-a074-a7cba41af8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling_data shape: (441, 60, 124)\n",
      "seq count: 441\n",
      "seq length: 60\n",
      "total batch count: 441\n",
      "batch size: 1\n",
      "rolling_data: torch.Size([441, 1, 60, 124])\n",
      "train_x: torch.Size([440, 1, 60, 122])\n",
      "train_y: torch.Size([440, 1, 2, 1])\n",
      "test_x:  torch.Size([1, 1, 60, 122])\n",
      "test_y:  torch.Size([1, 1, 2, 1])\n",
      "train_batch_count: 440\n",
      "test_batch_count:  1\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset.insert(1, 'future2',dataset.future)\n",
    "dataset['future2'] = dataset['future'].shift(-1)\n",
    "dataset = dataset.fillna(0)\n",
    "\n",
    "# print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "\n",
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 60\n",
    "BATCH_SIZE = 1                                                    # 注意：BATCH_SIZE是要能够整除seq_count的\n",
    "TEST_BATCH_COUNT = 1\n",
    "Y_SEQ_LEN = 2                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "Y_DIM = 1\n",
    "X_DIM = dataset.shape[1]-Y_SEQ_LEN                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                   # 数据一共是 seq_count x seq_len x (x_in_dim+Y_SEQ_LEN) \n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(SEQ_LENGTH))\n",
    "\n",
    "\n",
    "total_batch_count = int(rolling_data.shape[0]/BATCH_SIZE)                                   # 把数据规划成 batch_count 个 batch\n",
    "\n",
    "\n",
    "print(\"total batch count: {}\".format(total_batch_count))\n",
    "print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "rolling_data = rolling_data.reshape(total_batch_count, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)  # 把数据转成 total_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "rolling_data = torch.tensor(rolling_data)\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "\n",
    "\n",
    "train_batch_count = total_batch_count - TEST_BATCH_COUNT\n",
    "test_batch_count = TEST_BATCH_COUNT\n",
    "\n",
    "train = rolling_data[:train_batch_count, :, :, :]\n",
    "test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_SEQ_LEN:], train[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_SEQ_LEN:],  test[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y_seq_len]  to  [test_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train_batch_count))\n",
    "print(\"test_batch_count:  {}\".format(test_batch_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac5928d-7984-49aa-b1ae-57e902a703f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder & Decoder class\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size = self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        # print(\"Encoder self.input_dim  : {}\".format(self.input_dim))\n",
    "        # print(\"Encoder self.hidden_dim  : {}\".format(self.hidden_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"Encoder forward() x: {}\".format(x.shape))\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "        # print(\"Encoder outputs :{}\".format(outputs.shape))\n",
    "        # print(\"Encoder h_n     :{}\".format(h_n.shape))\n",
    "        # print(\"Encoder c_n     :{}\".format(c_n.shape))\n",
    "        return outputs, h_n, c_n\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.fc_in = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input : input batch data, size(input): [batch_size, feature_size]\n",
    "        # notice input only has two dimensions since the input is batchs\n",
    "        # of last coordinate of observed trajectory so the sequence length has been removed.\n",
    "        \n",
    "        # add sequence dimension to input, to allow use of nn.LSTM\n",
    "        # print(\"Decoder forward() input size : {}\".format(input.shape))\n",
    "        # print(\"Decoder forward() hidden size: {}\".format(hidden.shape))\n",
    "        # print(\"Decoder forward() cell size  : {}\".format(cell.shape))\n",
    "        \n",
    "        input = self.fc_in(input)\n",
    "\n",
    "        lstm_output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        \n",
    "        # print(\"Decoder forward() lstm_output: {}\".format(lstm_output.shape))\n",
    "        \n",
    "        prediction = self.fc_out(lstm_output)         # prediction is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80326ab-dd8a-4bdb-96e4-a3480314b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model class\n",
    "\n",
    "DEC_INPUT_DIM   = 1\n",
    "HIDDEN_DIM      = 768\n",
    "NUM_LAYERS      = 3\n",
    "ENC_DROPOUT     = 0.1\n",
    "DEC_DROPOUT     = 0.1\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.decoder_fc_init= nn.Linear(encoder.input_dim, decoder.input_dim)\n",
    "        # self.decoder_fc_input= nn.Linear(decoder.output_dim, decoder.input_dim)\n",
    "        \n",
    "        assert (encoder.hidden_dim == decoder.hidden_dim), \"hidden dimension in encoder and decoder must be equal\"       \n",
    "        assert (encoder.num_layers == decoder.num_layers), \"hidden layer numbers in encoder and decoder must be equal\"\n",
    "        \n",
    "            \n",
    "    def forward(self, x, y):\n",
    "        # x is the input to the encoder.\n",
    "        # y is the output from the decoder\n",
    "        # x = [batch size, encoder_in_sequence_len,  encoder_in_dim]               encoder_in_sequence_len=60, encoder_in_dim=122\n",
    "        # y = [batch size, decoder_out_sequence_len, decoder_out_dim]              decoder_out_sequence_len=2, decoder_out_dim=1    \n",
    "        # print(\"Seq2Seq forwar() x shape : {}\".format(x.shape))\n",
    "        # print(\"Seq2Seq forwar() y shape : {}\".format(y.shape))\n",
    "\n",
    "        decoder_out_seq_len = y.shape[1]                                                     # This is most important that define the output length\n",
    "        \n",
    "        # tensor to store decoder outputs of each time step; this outputs will calc loss with y, so its shape is same as y\n",
    "        outputs = torch.zeros(y.shape).to(device)\n",
    "        # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))\n",
    "        \n",
    "        _, hidden, cell = self.encoder(x)\n",
    "        # print(\"encoder hidden shape: {}\".format(hidden.shape))                            # [encoder_hidden_layer_number, batch_size, encoder_hidden_dim]\n",
    "        # print(\"encoder cell shape :  {}\".format(cell.shape))                              # [encoder_hidden_layer_number, batch_size, encoder_hidden_dim]\n",
    "        \n",
    "        # first input to decoder may be last coordinates of x to predict the future: [last_x]+[h_n,c_n] --> [model] --> [future_y]\n",
    "        decoder_input = x[:, -1, :]                                                           # [batch_size, encoder_input_dim] Get last elements of sequences of the batch from input x\n",
    "        decoder_input = decoder_input.unsqueeze(1)                                            # [batch_size, 1, encoder_input_dim] Get last element of sequence of the batch in encoder\n",
    "        # print(\"decoder_input: {}\".format(decoder_input.shape))\n",
    "        decoder_input = self.decoder_fc_init(decoder_input)                                   # [batch_size, 1, decoder_input_dim] Conver to 1st element of sequence of the batch in decoder\n",
    "        # print(\"decoder_input: {}\".format(decoder_input.shape))\n",
    "        \n",
    "        # Becasue the input and target have different sequence length, Get the target prediction one by one [Prev_prediction]+[h_n,c_n] --> [model] --> [Prediction]\n",
    "        for i in range(decoder_out_seq_len):\n",
    "            # run the decoder for one time step\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            # print(\"Seq2Seq forward() output shape: {}\".format(output.shape))\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each time step\n",
    "            outputs[:,i,:] = output\n",
    "            # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))            \n",
    "\n",
    "            # assign this prediction as next prediction's input\n",
    "            decoder_input = output\n",
    "            # print(\"Seq2Seq forward() decoder_input shape: {}\".format(output.shape))\n",
    "            \n",
    "            # 或者使用teacher_forcing来优化\n",
    "            # teacher_forcing_ratio=0.5\n",
    "            # teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # decoder_input = y[i] if teacher_forcing else output\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9dd074-51e0-475f-a64c-f817959b185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "encoder = Encoder(input_dim=X_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, dropout=ENC_DROPOUT)\n",
    "decoder = Decoder(input_dim=DEC_INPUT_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, output_dim=Y_DIM, dropout=DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).double().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692e6fe-3005-40a5-8f3e-01cfaeed148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 Seq2Seq 模型; \n",
    "LR = 1e-4\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1)\n",
    "\n",
    "\n",
    "model.train()\n",
    "epoches = 10\n",
    "epoch_loss = 0\n",
    "epoch_loss_list = []\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        pred = model(train_x[step], train_y[step])\n",
    "        # print(\"Train pred shape : {}\".format(pred.shape))\n",
    "        # print(\"Train train_y[step] shape : {}\".format(train_y[step].shape))\n",
    "        \n",
    "        loss = loss_func(pred, train_y[step].float())                                  # this calc the last element's loss between prediction and real.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.data.cpu()\n",
    "\n",
    "    print(\"{} of {} epoch loss: {:.6f}\".format(epoch, epoches, epoch_loss))\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # if (epoch+1)%10 == 0:\n",
    "    #     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aaadf6-8d3d-4488-8ad3-a12104efc196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用模型预测数据\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "for step in range(test_batch_count):\n",
    "    pred = model(test_x[step], test_y[step])\n",
    "\n",
    "    loss = loss_func(pred, test_y[step])\n",
    "    \n",
    "    if (step+1) < test_batch_count:                       # 最后一个测试数据不需要统计，因为没有真实值。\n",
    "        test_loss += loss.cpu()\n",
    "    \n",
    "    if test_batch_count == 1:\n",
    "        print(\"Prediction Loss average:{:.6f}\".format(test_loss/test_batch_count))\n",
    "    else:\n",
    "        print(\"Prediction Loss average:{:.6f}\".format(test_loss/(test_batch_count-1)))\n",
    "        \n",
    "    print(\"Prediction: {} ---- Actual: {}\".format(pred, test_y[step]))\n",
    "\n",
    "actual_line = test_y[step].cpu().detach().flatten().numpy()\n",
    "pred_line   = pred.cpu().detach().flatten().numpy()\n",
    "plt.plot(actual_line, 'r--')\n",
    "plt.plot(pred_line, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267f17c-8a01-4949-8287-f3368db264d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
