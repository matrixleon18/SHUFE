{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e96365c-9911-44d3-bf30-585acfbf60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 利用 LSTM 做一个 Seq2Seq 的预测，不考虑准确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3786622-0609-4097-bd47-9d70f64ba1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 118)\n",
      "rolling_data: (106, 45, 118)\n",
      "seq count: 106\n",
      "seq length: 45\n",
      "total batch count: 106\n",
      "batch size: 1\n",
      "rolling_data: torch.Size([106, 1, 45, 118])\n",
      "train_x: torch.Size([105, 1, 45, 117])\n",
      "train_y: torch.Size([105, 1, 45, 1])\n",
      "test_x:  torch.Size([1, 1, 45, 117])\n",
      "test_y:  torch.Size([1, 1, 45, 1])\n",
      "train_batch_count: 105\n",
      "test_batch_count:  1\n"
     ]
    }
   ],
   "source": [
    "# GPU -- 准备来做一个GPU版本的LSTM做股票预测\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"000600.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date','prediction'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset['updown'] = dataset['updown']\n",
    "print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "# 方法2，将数据按照batch_size的窗口进行滑动，每个窗口数据做一组\n",
    "\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "seq_len = 45\n",
    "batch_size = 1                                                    # 注意：batch_size是要能够整除seq_count的\n",
    "\n",
    "# 把数据切换成 batch_size 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(seq_len):\n",
    "    if i.shape[0] == seq_len:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, seq_len, 118)                 # 数据一共是 seq_count x seq_len x in_dim\n",
    "\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                 # 所以一共有 seq_count 列数据，每一行的数据是118维 （包括y）\n",
    "print(\"seq length: {}\".format(seq_len))\n",
    "\n",
    "\n",
    "total_batch_count = int(rolling_data.shape[0]/batch_size)                          # 把数据规划成 batch_count 个 batch\n",
    "\n",
    "print(\"total batch count: {}\".format(total_batch_count))\n",
    "print(\"batch size: {}\".format(batch_size))\n",
    "\n",
    "rolling_data = rolling_data.reshape(total_batch_count, batch_size, seq_len, 118)  # 把数据转成 total_batch_count x batch_size x seq_len x in_dim 格式\n",
    "rolling_data = torch.tensor(rolling_data)\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "\n",
    "\n",
    "train_batch_count = total_batch_count - 1\n",
    "test_batch_count = total_batch_count - train_batch_count\n",
    "\n",
    "train = rolling_data[:train_batch_count, :, :, :]\n",
    "test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,1:], train[:,:,:,0:1]\n",
    "test_x,  test_y  = test[:,:,:, 1:],  test[:,:,:,0:1]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train_batch_count))\n",
    "print(\"test_batch_count:  {}\".format(test_batch_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac5928d-7984-49aa-b1ae-57e902a703f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size = self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        # print(\"Encoder self.input_dim  : {}\".format(self.input_dim))\n",
    "        # print(\"Encoder self.hidden_dim  : {}\".format(self.hidden_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "        # print(\"Encoder outputs :{}\".format(outputs.shape))\n",
    "        # print(\"Encoder h_n     :{}\".format(h_n.shape))\n",
    "        # print(\"Encoder c_n     :{}\".format(c_n.shape))\n",
    "        return outputs, h_n, c_n\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.fc_in = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input : input batch data, size(input): [batch_size, feature_size]\n",
    "        # notice input only has two dimensions since the input is batchs\n",
    "        # of last coordinate of observed trajectory so the sequence length has been removed.\n",
    "        \n",
    "        # add sequence dimension to input, to allow use of nn.LSTM\n",
    "        # print(\"Decoder forward() input size : {}\".format(input.shape))\n",
    "        # print(\"Decoder forward() hidden size: {}\".format(hidden.shape))\n",
    "        # print(\"Decoder forward() cell size  : {}\".format(cell.shape))\n",
    "        \n",
    "        input = self.fc_in(input)\n",
    "\n",
    "        lstm_output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        \n",
    "        # print(\"Decoder forward() lstm_output: {}\".format(lstm_output.shape))\n",
    "        \n",
    "        prediction = self.fc_out(lstm_output)         # prediction is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80326ab-dd8a-4bdb-96e4-a3480314b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain the model\n",
    "\n",
    "ENC_INPUT_DIM   = 117\n",
    "ENC_HIDDEN_DIM  = 768\n",
    "DEC_INPUT_DIM   = 1\n",
    "DEC_HIDDEN_DIM  = 768\n",
    "DEC_OUPUT_DIM   = 1\n",
    "NUM_LAYERS      = 2\n",
    "ENC_DROPOUT     = 0.5\n",
    "DEC_DROPOUT     = 0.5\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "#         self.decoder_fc_init= nn.Linear(encoder.input_dim, decoder.input_dim)\n",
    "        \n",
    "#         self.decoder_fc_input= nn.Linear(decoder.output_dim, decoder.input_dim)\n",
    "        \n",
    "        assert (encoder.hidden_dim == decoder.hidden_dim), \"hidden dimension in encoder and decoder must be equal\"       \n",
    "        assert (encoder.num_layers == decoder.num_layers), \"hidden layer numbers in encoder and decoder must be equal\"\n",
    "        \n",
    "            \n",
    "    def forward(self, x, y):\n",
    "        # x is the input to the encoder.\n",
    "        # y is the output from the decoder\n",
    "        # x = [batch size, encoder_in_sequence_len, encoder_in_dim]               encoder_in_sequence_len=45, encoder_in_dim=117\n",
    "        # y = [batch size, decoder_out_sequence_len, decoder_out_dim]             decoder_out_sequence_len=45, decoder_out_dim=1\n",
    "        \n",
    "        # print(\"Seq2Seq forwar() x shape : {}\".format(x.shape))\n",
    "        # print(\"Seq2Seq forwar() y shape : {}\".format(y.shape))\n",
    "                \n",
    "        batch_size = x.shape[0]\n",
    "        encoder_in_seq_len = x.shape[1]\n",
    "        encoder_in_dim = x.shape[2]\n",
    "        \n",
    "        decoder_in_dim = y.shape[2]\n",
    "        decoder_out_seq_len = y.shape[1]\n",
    "        \n",
    "        # tensor to store decoder outputs of each time step\n",
    "        outputs = torch.zeros(decoder_out_seq_len).double().to(device)\n",
    "        # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))\n",
    "        \n",
    "        encoder_output, hidden, cell = self.encoder(x)\n",
    "        \n",
    "        # print(\"Seq2Seq forward() x[-1,-1,:] shape : {}\".format(x[-1,-1,:].shape))\n",
    "        # first input to decoder may be last coordinates of x\n",
    "        # this is last batch and last word of sequence.\n",
    "        decoder_input = torch.zeros(self.decoder.input_dim).double().to(device)\n",
    "        \n",
    "        # decoder_input = torch.zeros(batch_size, seq_len, OUPUT_DIM).double().to(device)\n",
    "        # print(\"Seq2Seq forward() encoder_output shape : {}\".format(encoder_output.shape))\n",
    "        \n",
    "        # print(\"Seq2Seq forward() decoder_input shape: {}\".format(decoder_input.shape))\n",
    "        decoder_input = decoder_input.unsqueeze(0)\n",
    "        decoder_input = decoder_input.unsqueeze(0)\n",
    "        # print(\"Seq2Seq forward() decoder_input shape: {}\".format(decoder_input.shape))\n",
    "\n",
    "        \n",
    "        # Becasue the input and target have different sequence length\n",
    "        # Get the target prediction one by one\n",
    "        for i in range(decoder_out_seq_len):\n",
    "            # run the decoder for one time step\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            # print(\"Seq2Seq forward() output shape: {}\".format(output.shape))\n",
    "            \n",
    "            # place predictions in a tensor holding predictions for each time step\n",
    "            outputs[i] = output\n",
    "            \n",
    "            # output is the same shape as input, [batch_size, feature size]\n",
    "            # so we can use output directly as next input\n",
    "            decoder_input = output\n",
    "            # print(\"Seq2Seq forward() decoder_input shape: {}\".format(output.shape))\n",
    "            \n",
    "            # 或者使用teacher_forcing来优化\n",
    "            # teacher_forcing_ratio=0.5\n",
    "            # teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # decoder_input = y[i] if teacher_forcing else output\n",
    "            \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9dd074-51e0-475f-a64c-f817959b185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = Encoder(input_dim=ENC_INPUT_DIM, hidden_dim=ENC_HIDDEN_DIM, num_layers=NUM_LAYERS, dropout=ENC_DROPOUT)\n",
    "decoder = Decoder(input_dim=DEC_INPUT_DIM, hidden_dim=DEC_HIDDEN_DIM, num_layers=NUM_LAYERS, output_dim=DEC_OUPUT_DIM, dropout=DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbaec60-2485-436c-8a28-2e19af78111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(\"The model has {:,} trainable parameters\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7692e6fe-3005-40a5-8f3e-01cfaeed148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar  3 15:29:00 2022\n",
      "9 of 100 epoch loss: 0.000313\n",
      "19 of 100 epoch loss: 0.000317\n",
      "29 of 100 epoch loss: 0.000339\n",
      "39 of 100 epoch loss: 0.000320\n",
      "49 of 100 epoch loss: 0.000326\n",
      "59 of 100 epoch loss: 0.000307\n",
      "69 of 100 epoch loss: 0.000289\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(pred, train_y[i])\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, norm_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\python38\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "\n",
    "# 训练 LSTM 模型; \n",
    "i = 0\n",
    "\n",
    "epoches = 100\n",
    "\n",
    "decoder_start = torch.zeros(batch_size, seq_len, 768).double().to(device)\n",
    "\n",
    "print(time.ctime())\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        model = model.double()\n",
    "\n",
    "        pred = model(train_x[i], train_y[i])\n",
    "        \n",
    "        pred = pred.unsqueeze(0).unsqueeze(2)\n",
    "        \n",
    "        # print(\"Train pred shape : {}\".format(pred.shape))\n",
    "        # print(\"Train train_y[i] shape : {}\".format(train_y[i].shape))\n",
    "        \n",
    "        loss = loss_func(pred, train_y[i])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1)%train_batch_count == 0:\n",
    "            i=0\n",
    "        else:\n",
    "            i=i+1\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(\"{} of {} epoch loss: {:.6f}\".format(epoch, epoches, loss.item()))\n",
    "        # print(\"Prediction: {}\".format(pred.data))\n",
    "        # print(\"Actual Res: {}\".format(train_y[i].data))\n",
    "\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f474f02-88a3-4801-8374-75eb7ae216cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
