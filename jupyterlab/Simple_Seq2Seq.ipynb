{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e96365c-9911-44d3-bf30-585acfbf60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 利用 LSTM 做一个 Seq2Seq 的预测，不考虑准确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3786622-0609-4097-bd47-9d70f64ba1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 118)\n",
      "rolling_data: (106, 45, 118)\n",
      "seq count: 106\n",
      "seq length: 45\n",
      "total batch count: 106\n",
      "batch size: 1\n",
      "rolling_data: torch.Size([106, 1, 45, 118])\n",
      "train_x: torch.Size([105, 1, 45, 117])\n",
      "train_y: torch.Size([105, 1, 45, 1])\n",
      "test_x:  torch.Size([1, 1, 45, 117])\n",
      "test_y:  torch.Size([1, 1, 45, 1])\n",
      "train_batch_count: 105\n",
      "test_batch_count:  1\n"
     ]
    }
   ],
   "source": [
    "# GPU -- 准备来做一个GPU版本的LSTM做股票预测\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"000600.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date','prediction'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset['updown'] = dataset['updown']\n",
    "print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "# 方法2，将数据按照batch_size的窗口进行滑动，每个窗口数据做一组\n",
    "\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "seq_len = 45\n",
    "batch_size = 1                                                    # 注意：batch_size是要能够整除seq_count的\n",
    "\n",
    "# 把数据切换成 batch_size 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(seq_len):\n",
    "    if i.shape[0] == seq_len:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, seq_len, 118)                 # 数据一共是 seq_count x seq_len x in_dim\n",
    "\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                 # 所以一共有 seq_count 列数据，每一行的数据是118维 （包括y）\n",
    "print(\"seq length: {}\".format(seq_len))\n",
    "\n",
    "\n",
    "total_batch_count = int(rolling_data.shape[0]/batch_size)                          # 把数据规划成 batch_count 个 batch\n",
    "\n",
    "print(\"total batch count: {}\".format(total_batch_count))\n",
    "print(\"batch size: {}\".format(batch_size))\n",
    "\n",
    "rolling_data = rolling_data.reshape(total_batch_count, batch_size, seq_len, 118)  # 把数据转成 total_batch_count x batch_size x seq_len x in_dim 格式\n",
    "rolling_data = torch.tensor(rolling_data)\n",
    "print(\"rolling_data: {}\".format(rolling_data.shape))\n",
    "\n",
    "\n",
    "train_batch_count = total_batch_count - 1\n",
    "test_batch_count = total_batch_count - train_batch_count\n",
    "\n",
    "train = rolling_data[:train_batch_count, :, :, :]\n",
    "test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,1:], train[:,:,:,0:1]\n",
    "test_x,  test_y  = test[:,:,:, 1:],  test[:,:,:,0:1]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train_batch_count))\n",
    "print(\"test_batch_count:  {}\".format(test_batch_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac5928d-7984-49aa-b1ae-57e902a703f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size = self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        # print(\"Encoder self.input_dim  : {}\".format(self.input_dim))\n",
    "        # print(\"Encoder self.hidden_dim  : {}\".format(self.hidden_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "        # print(\"Encoder outputs :{}\".format(outputs.shape))\n",
    "        # print(\"Encoder h_n     :{}\".format(h_n.shape))\n",
    "        # print(\"Encoder c_n     :{}\".format(c_n.shape))\n",
    "        return outputs, h_n, c_n\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input : input batch data, size(input): [batch_size, feature_size]\n",
    "        # notice input only has two dimensions since the input is batchs\n",
    "        # of last coordinate of observed trajectory so the sequence length has been removed.\n",
    "        \n",
    "        # add sequence dimension to input, to allow use of nn.LSTM\n",
    "        # print(\"Decoder forward() input size : {}\".format(input.shape))\n",
    "        # print(\"Decoder forward() hidden size: {}\".format(hidden.shape))\n",
    "        # print(\"Decoder forward() cell size  : {}\".format(cell.shape))\n",
    "\n",
    "        lstm_output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        \n",
    "        # print(\"Decoder forward() lstm_output: {}\".format(lstm_output.shape))\n",
    "        \n",
    "        prediction = self.fc_out(lstm_output)         # prediction is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80326ab-dd8a-4bdb-96e4-a3480314b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain the model\n",
    "\n",
    "INPUT_DIM = 117\n",
    "HIDDEN_DIM = 768\n",
    "OUPUT_DIM = 1\n",
    "NUM_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.decoder_fc_init= nn.Linear(encoder.input_dim, decoder.input_dim)\n",
    "        \n",
    "        self.decoder_fc_input= nn.Linear(decoder.output_dim, decoder.input_dim)\n",
    "        \n",
    "        assert (encoder.hidden_dim == decoder.hidden_dim), \"hidden dimension in encoder and decoder must be equal\"       \n",
    "        assert (encoder.num_layers == decoder.num_layers), \"hidden layer numbers in encoder and decoder must be equal\"\n",
    "        \n",
    "            \n",
    "    def forward(self, x, y):\n",
    "        # x is the input to the encoder.\n",
    "        # y is the output from the decoder\n",
    "        # x = [batch size, input_sequence_len, input_in_dim]              input_sequence_len=45, input_in_dim=117\n",
    "        # y = [batch size, target_sequence_len, feature size]             target_sequence_len=5, target_in_dim=768\n",
    "        \n",
    "        # print(\"Seq2Seq forwar() x shape : {}\".format(x.shape))\n",
    "        # print(\"Seq2Seq forwar() y shape : {}\".format(y.shape))\n",
    "                \n",
    "        batch_size = x.shape[0]\n",
    "        encoder_in_seq_len = x.shape[1]\n",
    "        encoder_in_dim = x.shape[2]\n",
    "        \n",
    "        decoder_in_dim = y.shape[2]\n",
    "        target_seq_len = y.shape[1]\n",
    "        \n",
    "        # tensor to store decoder outputs of each time step\n",
    "        outputs = torch.zeros(target_seq_len).double().to(device)\n",
    "        # print(\"Seq2Seq forward() outputs shape: {}\".format(outputs.shape))\n",
    "        \n",
    "        encoder_output, hidden, cell = self.encoder(x)\n",
    "        \n",
    "        # print(\"Seq2Seq forward() x[-1,-1,:] shape : {}\".format(x[-1,-1,:].shape))\n",
    "        # first input to decoder may be last coordinates of x\n",
    "        # this is last batch and last word of sequence.\n",
    "        decoder_input = self.decoder_fc_init(x[-1, -1, :])\n",
    "        \n",
    "        # decoder_input = torch.zeros(batch_size, seq_len, OUPUT_DIM).double().to(device)\n",
    "        # print(\"Seq2Seq forward() encoder_output shape : {}\".format(encoder_output.shape))\n",
    "        \n",
    "        # print(\"Seq2Seq forward() decoder_input shape: {}\".format(decoder_input.shape))\n",
    "        \n",
    "        decoder_input = decoder_input.unsqueeze(0)\n",
    "        decoder_input = decoder_input.unsqueeze(0)\n",
    "        # print(\"Seq2Seq forward() decoder_input shape: {}\".format(decoder_input.shape))\n",
    "\n",
    "        \n",
    "        # Becasue the input and target have different sequence length\n",
    "        # Get the target prediction one by one\n",
    "        for i in range(target_seq_len):\n",
    "            # run the decoder for one time step\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            # print(\"Seq2Seq forward() output shape: {}\".format(output.shape))\n",
    "            \n",
    "            # place predictions in a tensor holding predictions for each time step\n",
    "            outputs[i] = output\n",
    "            \n",
    "            # output is the same shape as input, [batch_size, feature size]\n",
    "            # so we can use output directly as next input\n",
    "            decoder_input = self.decoder_fc_input(output)\n",
    "            # print(\"Seq2Seq forward() decoder_input shape: {}\".format(output.shape))\n",
    "            \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9dd074-51e0-475f-a64c-f817959b185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = Encoder(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, dropout=ENC_DROPOUT)\n",
    "decoder = Decoder(input_dim=HIDDEN_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, output_dim=OUPUT_DIM, dropout=DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbaec60-2485-436c-8a28-2e19af78111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(\"The model has {:,} trainable parameters\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692e6fe-3005-40a5-8f3e-01cfaeed148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar  3 14:26:40 2022\n",
      "9 of 100 epoch loss: 0.000345\n",
      "Prediction: tensor([[[ 0.0012],\n",
      "         [-0.0001],\n",
      "         [ 0.0039],\n",
      "         [ 0.0079],\n",
      "         [-0.0046],\n",
      "         [-0.0002],\n",
      "         [-0.0030],\n",
      "         [-0.0006],\n",
      "         [-0.0047],\n",
      "         [-0.0110],\n",
      "         [-0.0017],\n",
      "         [ 0.0018],\n",
      "         [ 0.0031],\n",
      "         [-0.0055],\n",
      "         [-0.0042],\n",
      "         [ 0.0012],\n",
      "         [-0.0007],\n",
      "         [-0.0003],\n",
      "         [-0.0019],\n",
      "         [-0.0047],\n",
      "         [-0.0060],\n",
      "         [-0.0041],\n",
      "         [-0.0011],\n",
      "         [ 0.0041],\n",
      "         [ 0.0009],\n",
      "         [ 0.0028],\n",
      "         [-0.0042],\n",
      "         [-0.0071],\n",
      "         [ 0.0005],\n",
      "         [ 0.0007],\n",
      "         [-0.0036],\n",
      "         [ 0.0027],\n",
      "         [-0.0009],\n",
      "         [-0.0031],\n",
      "         [-0.0029],\n",
      "         [-0.0024],\n",
      "         [-0.0068],\n",
      "         [ 0.0002],\n",
      "         [ 0.0016],\n",
      "         [-0.0088],\n",
      "         [ 0.0013],\n",
      "         [-0.0079],\n",
      "         [ 0.0026],\n",
      "         [-0.0024],\n",
      "         [-0.0011]]], device='cuda:0', dtype=torch.float64)\n",
      "Actual Res: tensor([[[-0.0217],\n",
      "         [-0.0081],\n",
      "         [-0.0102],\n",
      "         [-0.0041],\n",
      "         [-0.0021],\n",
      "         [ 0.0166],\n",
      "         [-0.0081],\n",
      "         [ 0.0041],\n",
      "         [ 0.0082],\n",
      "         [-0.0020],\n",
      "         [ 0.0346],\n",
      "         [-0.0118],\n",
      "         [-0.0815],\n",
      "         [-0.0022],\n",
      "         [-0.0217],\n",
      "         [ 0.0089],\n",
      "         [-0.0044],\n",
      "         [-0.0022],\n",
      "         [-0.0199],\n",
      "         [-0.0158],\n",
      "         [-0.0023],\n",
      "         [-0.0207],\n",
      "         [ 0.0047],\n",
      "         [ 0.0327],\n",
      "         [ 0.0068],\n",
      "         [ 0.0157],\n",
      "         [-0.0066],\n",
      "         [-0.0089],\n",
      "         [-0.0067],\n",
      "         [ 0.0090],\n",
      "         [-0.0022],\n",
      "         [ 0.0157],\n",
      "         [ 0.0022],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [-0.0155],\n",
      "         [ 0.0157],\n",
      "         [-0.0088],\n",
      "         [-0.0067],\n",
      "         [ 0.0247],\n",
      "         [-0.0066],\n",
      "         [ 0.0022],\n",
      "         [-0.0088],\n",
      "         [-0.0133],\n",
      "         [-0.0090]]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "\n",
    "# 训练 LSTM 模型; \n",
    "i = 0\n",
    "\n",
    "epoches = 100\n",
    "\n",
    "decoder_start = torch.zeros(batch_size, seq_len, 768).double().to(device)\n",
    "\n",
    "print(time.ctime())\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        model = model.double()\n",
    "\n",
    "        pred = model(train_x[i], train_y[i])\n",
    "        \n",
    "        pred = pred.unsqueeze(0).unsqueeze(2)\n",
    "        \n",
    "        # print(\"Train pred shape : {}\".format(pred.shape))\n",
    "        # print(\"Train train_y[i] shape : {}\".format(train_y[i].shape))\n",
    "        \n",
    "        loss = loss_func(pred, train_y[i])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1)%train_batch_count == 0:\n",
    "            i=0\n",
    "        else:\n",
    "            i=i+1\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(\"{} of {} epoch loss: {:.6f}\".format(epoch, epoches, loss.item()))\n",
    "        # print(\"Prediction: {}\".format(pred.data))\n",
    "        # print(\"Actual Res: {}\".format(train_y[i].data))\n",
    "\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f474f02-88a3-4801-8374-75eb7ae216cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
