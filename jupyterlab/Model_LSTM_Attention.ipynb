{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df055ebf-71d4-4fdd-95a3-b391fdfc40bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 有两层 LSTM 的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2f70f0-3a72-4250-8005-9dffb5f17e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling_data shape: (441, 60, 135)\n",
      "seq count: 441\n",
      "seq length: 60\n",
      "batch size: 440\n",
      "train_x: torch.Size([1, 440, 60, 134])\n",
      "train_y: torch.Size([1, 440, 1, 1])\n",
      "test_x:  torch.Size([1, 440, 60, 134])\n",
      "test_y:  torch.Size([1, 440, 1, 1])\n",
      "train_batch_count: 1\n",
      "test_batch_count:  1\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.tail())\n",
    "dataset = dataset.fillna(0)\n",
    "\n",
    "# print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "\n",
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 60\n",
    "BATCH_SIZE = 440                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_COUNT = 1\n",
    "Y_SEQ_LEN = 1                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "Y_DIM = 1\n",
    "X_DIM = dataset.shape[1]-Y_SEQ_LEN                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                   # 数据一共是 seq_count x seq_len x (x_in_dim+Y_SEQ_LEN) \n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(SEQ_LENGTH))\n",
    "print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "\n",
    "train = rolling_data[:-1].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "test  = rolling_data[-BATCH_SIZE:].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)  # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "train = torch.tensor(train)\n",
    "test  = torch.tensor(test)\n",
    "\n",
    "# train = rolling_data[:train_batch_count, :, :, :]\n",
    "# test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_SEQ_LEN:], train[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_SEQ_LEN:],  test[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y_seq_len]  to  [test_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train.shape[0]))\n",
    "print(\"test_batch_count:  {}\".format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2328f7e3-40ed-49ef-890d-4b521c9d8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 LSTM 模型\n",
    "np.random.seed(1027)\n",
    "torch.manual_seed(1027)\n",
    "torch.cuda.manual_seed(1027)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TIME_STEP = SEQ_LENGTH                                        # 一般这个单独设定，这里为了简单，还是直接就等于seq_len的方便。其实也就是等于最长的那个sequence length\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size, attention_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=int(hidden_layer_size/2), num_layers=num_layers, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=int(hidden_layer_size/2), hidden_size=hidden_layer_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear_1 = nn.Linear(hidden_layer_size, int(hidden_layer_size/4))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(hidden_layer_size, output_size)\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.h10 = torch.zeros(NUM_LAYERS, BATCH_SIZE, int(hidden_layer_size/2)*num_layers).double().to(device)\n",
    "        self.c10 = torch.zeros(NUM_LAYERS, BATCH_SIZE, int(hidden_layer_size/2)*num_layers).double().to(device)\n",
    "        self.h20 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size*num_layers).double().to(device)\n",
    "        self.c20 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size*num_layers).double().to(device)\n",
    "        \n",
    "        self.attention_size = attention_size\n",
    "        # w_omega means W_w\n",
    "        # u_omega means U_w\n",
    "        self.w_omega = Variable(torch.zeros(self.hidden_size * self.layer_size, self.attention_size).cuda())\n",
    "        self.u_omega = Variable(torch.zeros(self.attention_size).cuda()))\n",
    "        \n",
    "        self.init_weights2()\n",
    "\n",
    "    def init_weights1(self):\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "        for name, param in self.lstm2.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "    def init_weights2(self):\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)    \n",
    "        for name, param in self.lstm2.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)    \n",
    "                \n",
    "    def init_weights3(self):\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "        for name, param in self.lstm2.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "    \n",
    "    \n",
    "    def attention_net(self, lstm_output):\n",
    "        # referece: https://blog.csdn.net/qsmx666/article/details/107118550\n",
    "        print(\"lstm_output: {}\".format(lstm_output.size()))\n",
    "        #print(lstm_output.size()) = (batch_size, squence_length, hidden_layer_size*num_layers)\n",
    "\n",
    "        lstm_output_reshape = torch.Tensor.reshape(lstm_output, [-1, self.hidden_size*self.num_layers])\n",
    "        print(\"lstm_output_reshape: {}\".format(lstm_output_reshape.size()))\n",
    "        #print(output_reshape.size()) = (batch_size * squence_length, hidden_size*num_layers)\n",
    "\n",
    "        attn_tanh = torch.tanh(torch.mm(lstm_output_reshape, self.w_omega))\n",
    "        print(\"attn_tanh: {}\".format(attn_tanh.size()))\n",
    "        #print(attn_tanh.size()) = (squence_length * batch_size, attention_size)\n",
    "\n",
    "        attn_hidden_layer = torch.mm(attn_tanh, torch.Tensor.reshape(self.u_omega, [-1, 1]))\n",
    "        print(\"attn_hidden_layer: {}\".format(attn_hidden_layer.size()))\n",
    "        #print(attn_hidden_layer.size()) = (squence_length * batch_size, 1)\n",
    "\n",
    "        exps = torch.Tensor.reshape(torch.exp(attn_hidden_layer), [-1, self.sequence_length])\n",
    "        print(\"exps: {}\".format(exps.size()))\n",
    "        #print(exps.size()) = (batch_size, squence_length)\n",
    "\n",
    "        alphas = exps / torch.Tensor.reshape(torch.sum(exps, 1), [-1, 1])\n",
    "        print(\"alphas: {}\".format(alphas.size()))\n",
    "        #print(alphas.size()) = (batch_size, squence_length)\n",
    "\n",
    "        alphas_reshape = torch.Tensor.reshape(alphas, [-1, self.sequence_length, 1])\n",
    "        print(\"alphas_reshape: {}\".format(alphas_reshape.size()))\n",
    "        #print(alphas_reshape.size()) = (batch_size, squence_length, 1)\n",
    "\n",
    "        # state = lstm_output.permute(1, 0, 2)\n",
    "        print(\"state: {}\".format(state.size()))\n",
    "        #print(state.size()) = (batch_size, squence_length, hidden_size*layer_size)\n",
    "\n",
    "        attn_output = torch.sum(state * alphas_reshape, 1)\n",
    "        print(\"attn_output: {}\".format(attn_output.size()))\n",
    "        #print(attn_output.size()) = (batch_size, hidden_size*layer_size)\n",
    "\n",
    "        return attn_output\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # layer 1\n",
    "        # x = self.linear_1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch        \n",
    "        # LSTM layer\n",
    "        # lstm_out, (h_n, c_n) = self.lstm(x, (self.h0.detach(), self.c0.detach()))\n",
    "        \n",
    "        lstm1_out, (h1_n, c1_n) = self.lstm1(x, (self.h10, self.c10))\n",
    "        \n",
    "        lstm1_out = self.dropout(lstm1_out)\n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm2(lstm1_out, (self.h20, self.c20))\n",
    "        \n",
    "        attn_output = self.attention_net(lstm_out)\n",
    "        \n",
    "        predictions = self.linear_2(attn_output)\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87b288f-1086-49d4-81ec-2a42bf499c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 LSTM 模型\n",
    "np.random.seed(1027)\n",
    "torch.manual_seed(1027)\n",
    "torch.cuda.manual_seed(1027)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TIME_STEP = SEQ_LENGTH                                        # 一般这个单独设定，这里为了简单，还是直接就等于seq_len的方便。其实也就是等于最长的那个sequence length\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=int(hidden_layer_size/2), num_layers=num_layers, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=int(hidden_layer_size/2), hidden_size=hidden_layer_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear_1 = nn.Linear(hidden_layer_size, int(hidden_layer_size/4))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.h10 = torch.zeros(NUM_LAYERS, BATCH_SIZE, int(hidden_layer_size/2)).double().to(device)\n",
    "        self.c10 = torch.zeros(NUM_LAYERS, BATCH_SIZE, int(hidden_layer_size/2)).double().to(device)\n",
    "        self.h20 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size).double().to(device)\n",
    "        self.c20 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size).double().to(device)\n",
    "        \n",
    "        self.init_weights2()\n",
    "\n",
    "    def init_weights1(self):\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "        for name, param in self.lstm2.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "    def init_weights2(self):\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)    \n",
    "        for name, param in self.lstm2.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)    \n",
    "                \n",
    "    def init_weights3(self):\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "        for name, param in self.lstm2.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "    def forward(self, x):\n",
    "\n",
    "        # layer 1\n",
    "        # x = self.linear_1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch        \n",
    "        # LSTM layer\n",
    "        # lstm_out, (h_n, c_n) = self.lstm(x, (self.h0.detach(), self.c0.detach()))\n",
    "        \n",
    "        lstm1_out, (h1_n, c1_n) = self.lstm1(x, (self.h10, self.c10))\n",
    "        \n",
    "        lstm1_out = self.dropout(lstm1_out)\n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm2(lstm1_out, (self.h20, self.c20))\n",
    "\n",
    "        # lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        predictions = self.linear_2(lstm_out)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33bcc6e-17bd-45ef-a37c-0c4fc5266e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 实例化 LSTM 模型\n",
    "model = LSTMModel(input_size=X_DIM, hidden_layer_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1)\n",
    "model = model.double().to(device)\n",
    "LR = 1e-4\n",
    "loss_func = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c81486-f58a-4a42-b0ba-176f469dbaa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练 LSTM 模型;  ---- 这里的损失函数是计算Sequence最后一个元素的预测数据和真实数据差异\n",
    "model.train()\n",
    "epoches = 10000\n",
    "epoch_loss = 0\n",
    "epoch_loss_list = []\n",
    "train_batch_count = train_x.shape[0]\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        pred = model(train_x[step])\n",
    "        loss = loss_func(pred[:,-1], train_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data.cpu()\n",
    "        \n",
    "    if epoch_loss.item() < 1e-4:\n",
    "        print('Epoch [{}/{}], Loss: {:.5f}'.format(epoch+1, epoches, loss.item()))\n",
    "        print(\"The loss value is reached\")\n",
    "        break\n",
    "\n",
    "    print(\"{} of {} epoch loss: {:.4f} with lr: {}\".format(epoch, epoches, epoch_loss.item(), optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    if (epoch+1) % 2000 ==0:\n",
    "        scheduler.step()\n",
    "    # print(\"learning rate: {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    # for p in optimizer.param_groups:\n",
    "    #     p['lr'] *= 0.99\n",
    "    \n",
    "plt.plot(epoch_loss_list)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50c0b44-f62f-4c56-90cf-0a55d1fb6d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model, 'e:\\\\Model_LSTM2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b617d8d5-f726-4a0a-9ad8-46f2af1e1a12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model = torch.load('e:\\\\Model_LSTM2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76b4797e-cea6-470f-8cdc-1133bc9de018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Loss average:0.000619\n",
      "Prediction: -0.02\n",
      "Actual:     0.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 用模型预测数据\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_batch_count = test_x.shape[0]\n",
    "\n",
    "for step in range(test_batch_count):\n",
    "    pred = model(test_x[step])\n",
    "\n",
    "    loss = loss_func(pred[-1,-1], test_y[step][-1,-1])                # Compare the all sequences' last element in one batch\n",
    "    \n",
    "    test_loss += loss.cpu()\n",
    "    \n",
    "print(\"Prediction Loss average:{:.6f}\".format(test_loss.data/(step+1)))\n",
    "print(\"Prediction: {:.2f}\".format(float(pred[-1,-1].data)))\n",
    "print(\"Actual:     {:.2f}\".format(float(test_y[step][-1,-1].data)))\n",
    "\n",
    "actual_line = test_y[step][-1].cpu().detach().flatten().numpy()        # Only plot the last sequence of test batch\n",
    "pred_line   = pred[-1].cpu().detach().flatten().numpy()                # Only plot the last sequence of test batch\n",
    "plt.plot(actual_line, 'r--')\n",
    "plt.plot(pred_line, 'b-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b83f3b-d110-4968-a685-13beebc4dec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
