{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e9707-a755-443b-8483-00b9df2ea38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 单层 LSTM 的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a0bfe-88e2-4398-9334-cc77d8bb8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c3f73-0613-4605-9691-ba3dfbbfbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1027)\n",
    "torch.manual_seed(1027)\n",
    "torch.cuda.manual_seed(1027)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe160cab-111b-4904-abb0-8204a8a10669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "dataset = dataset.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de255091-c7b4-4495-a035-b80b06317681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "TRAIN_VALIDATION_RATIO = 0.7\n",
    "TRAIN_BATCH_SIZE = 60                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_SIZE = 1\n",
    "SEQ_LENGTH = 15\n",
    "Y_SEQ_LEN = 1                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "Y_DIM = 1\n",
    "X_DIM = dataset.shape[1]-Y_SEQ_LEN                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                   # 数据一共是 seq_count x seq_len x (x_in_dim+Y_SEQ_LEN) \n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(SEQ_LENGTH))\n",
    "\n",
    "# TEST_BATCH_COUNT  = (rolling_data.shape[0])%TRAIN_BATCH_SIZE\n",
    "TEST_BATCH_SIZE  = (rolling_data.shape[0])%TRAIN_BATCH_SIZE\n",
    "TEST_BATCH_COUNT = 1\n",
    "TRAIN_BATCH_COUNT = int((rolling_data.shape[0]//TRAIN_BATCH_SIZE) * TRAIN_VALIDATION_RATIO)\n",
    "VALID_BATCH_COUNT = int((rolling_data.shape[0]//TRAIN_BATCH_SIZE) - TRAIN_BATCH_COUNT)\n",
    "\n",
    "print(\"TRAIN_BATCH_COUNT : {}\".format(TRAIN_BATCH_COUNT))\n",
    "print(\"VALID_BATCH_COUNT : {}\".format(VALID_BATCH_COUNT))\n",
    "print(\"TEST_BATCH_COUNT  : {}\".format(TEST_BATCH_COUNT))\n",
    "\n",
    "# train = rolling_data[:-test_seq_count].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "# test  = rolling_data[-test_seq_count:].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "train = rolling_data[:TRAIN_BATCH_COUNT*TRAIN_BATCH_SIZE].reshape(TRAIN_BATCH_COUNT, TRAIN_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_DIM*Y_SEQ_LEN)                    # 把数据转成 tain_batch_count x TRAIN_BATCH_SIZE x seq_len x in_dim 格式\n",
    "valid = rolling_data[TRAIN_BATCH_COUNT*TRAIN_BATCH_SIZE:-TEST_BATCH_COUNT*TEST_BATCH_SIZE].reshape(VALID_BATCH_COUNT, TRAIN_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_DIM*Y_SEQ_LEN)     # 把数据转成 tain_batch_count x TRAIN_BATCH_SIZE x seq_len x in_dim 格式\n",
    "test  = rolling_data[-TEST_BATCH_COUNT*TEST_BATCH_SIZE:].reshape(TEST_BATCH_COUNT, TEST_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_DIM*Y_SEQ_LEN)                     # 把数据转成 test_batch_count x TEST_BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "TRAIN_BATCH_COUNT = train.shape[0]\n",
    "TRAIN_BATCH_SIZE = train.shape[1]\n",
    "VALID_BATCH_COUNT = valid.shape[0]\n",
    "VALID_BATCH_SIZE = valid.shape[1]\n",
    "TEST_BATCH_COUNT = test.shape[0]\n",
    "TEST_BATCH_SIZE = test.shape[1]\n",
    "\n",
    "train = torch.tensor(train)\n",
    "valid = torch.tensor(valid)\n",
    "test  = torch.tensor(test)\n",
    "\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_SEQ_LEN:], train[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "valid_x, valid_y = valid[:,:,:,Y_SEQ_LEN:], valid[:,:,-1:,0:Y_SEQ_LEN]           # [valid_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_SEQ_LEN:],  test[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "valid_y = valid_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y_seq_len]  to  [test_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "valid_x = valid_x.to(device)\n",
    "valid_y = valid_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"valid_x: {}\".format(valid_x.shape))\n",
    "print(\"valid_y: {}\".format(valid_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train.shape[0]))\n",
    "print(\"valid_batch_count: {}\".format(valid.shape[0]))\n",
    "print(\"test_batch_count:  {}\".format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033e44d-4dfa-4a7f-8fa6-b96783b5ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 LSTM 模型\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_layer_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        # self.h0 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size).double().to(device)\n",
    "        # self.c0 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size).double().to(device)\n",
    "        \n",
    "        self.init_weights2()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "\n",
    "    def init_weights2(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)    \n",
    "    \n",
    "    def init_weights3(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "    def forward(self, x, hidden, cell):\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, (hidden, cell))\n",
    "        \n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        predictions = self.linear_2(lstm_out)\n",
    "        \n",
    "        return predictions, h_n, c_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847eb8d0-f5f3-4943-ad9e-b9f2a7ba5c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 LSTM 模型 ---- 这里的损失函数是计算Sequence最后一个元素的预测数据和真实数据差异\n",
    "\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "model = LSTMModel(input_size=X_DIM, hidden_layer_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1).double().to(device)\n",
    "LR = 1e-3\n",
    "loss_func = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d9251-af68-4657-a16a-e406fb4a1f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练 LSTM 模型;  ---- 这里的损失函数是计算Sequence最后一个元素的预测数据和真实数据差异\n",
    "model.train()\n",
    "epoches = 50\n",
    "train_epoch_loss = 0\n",
    "train_epoch_loss_list = []\n",
    "valid_smallest_loss = 1\n",
    "valid_smallest_epoch = 0\n",
    "valid_epoch_loss = 0\n",
    "valid_epoch_loss_list = []\n",
    "\n",
    "train_batch_count = train_x.shape[0]\n",
    "valid_batch_count = valid_x.shape[0]\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, TRAIN_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, TRAIN_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    batch_loss = []\n",
    "    train_epoch_loss = 0\n",
    "\n",
    "    train_batch_list = list(range(0,train_batch_count))\n",
    "    random.shuffle(train_batch_list)\n",
    "    # for step in range(train_batch_count):\n",
    "    for step in train_batch_list:\n",
    "        pred, hn, cn = model(train_x[step], h0, c0)\n",
    "        # h0, c0 = hn.detach(), cn.detach()\n",
    "        loss = loss_func(pred[:,-1], train_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.cpu().data.numpy())\n",
    "    # print(batch_loss)\n",
    "    train_epoch_loss = np.mean(batch_loss)\n",
    "\n",
    "    batch_loss = []\n",
    "    valid_epoch_loss = 0\n",
    "    valid_pred_value_list = []\n",
    "    valid_real_value_list = []\n",
    "    for step in range(valid_batch_count):\n",
    "        pred, hn, cn = model(valid_x[step], h0, c0)\n",
    "        loss = loss_func(pred[:,-1,-1], valid_y[step][:,-1,-1])\n",
    "        valid_pred_value_list.append(pred[:,-1,-1].cpu().detach().flatten().numpy() )\n",
    "        valid_real_value_list.append(valid_y[step,:,-1,-1].cpu().detach().flatten().numpy() )\n",
    "        batch_loss.append(loss.cpu().data.numpy())\n",
    "    # print(batch_loss)\n",
    "    valid_epoch_loss = np.mean(batch_loss)\n",
    "        \n",
    "    print(\"{} of {} epoch   train_loss: {:.4f}   valid_loss: {:.4f}\".format(epoch, epoches, train_epoch_loss, valid_epoch_loss))\n",
    "\n",
    "    valid_epoch_loss_list.append(valid_epoch_loss)\n",
    "    train_epoch_loss_list.append(train_epoch_loss)\n",
    "\n",
    "plt.plot(train_epoch_loss_list, 'r-')\n",
    "plt.plot(valid_epoch_loss_list, 'b-')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69845d-ab88-441a-8ad9-e92ddf92884f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(valid_y[step,:,-1,-1].cpu().detach().flatten().numpy(), 'r--')\n",
    "plt.plot(pred[:,-1,-1].cpu().detach().flatten().numpy(), 'b-')\n",
    "plt.show()\n",
    "print(valid_y[step,:,-1,-1].cpu().detach().flatten().numpy())\n",
    "print(pred[:,-1,-1].cpu().detach().flatten().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f9272-9fd8-40a2-ae45-f92f793bbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用模型预测数据\n",
    "model.eval()\n",
    "# model.train()\n",
    "valid_pred_value_list = []\n",
    "valid_real_value_list = []\n",
    "h0 = torch.zeros(NUM_LAYERS, TRAIN_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, TRAIN_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "        \n",
    "for step in range(valid_x.shape[0]):\n",
    "    pred, hn, cn = model(valid_x[step], h0, c0)\n",
    "    loss = loss_func(pred[:,-1,-1], valid_y[step][:,-1,-1])\n",
    "    valid_pred_value_list = pred[:,-1,-1].cpu().detach().flatten().numpy()\n",
    "    valid_real_value_list = valid_y[step,:,-1,-1].cpu().detach().flatten().numpy()\n",
    "\n",
    "plt.plot(valid_y[step,:,-1,-1].cpu().detach().flatten().numpy(), 'r--')\n",
    "plt.plot(pred[:,-1,-1].cpu().detach().flatten().numpy(), 'b-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210219c-d586-4268-ba8b-8bd73db5b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_y[step,:,-1,-1].cpu().detach().flatten().numpy(), 'r--')\n",
    "print(pred[:,-1,-1].cpu().detach().flatten().numpy(), 'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a22ac-9ea0-4ebd-b1a7-4b62964784e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用模型预测数据\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "for step in range(test_x.shape[0]):\n",
    "    pred, hn, cn = model(test_x[step], h0, c0)\n",
    "    \n",
    "    loss = loss_func(pred[:,-1,-1], test_y[step][:,-1,-1])               # Compare the all sequences' last element in one batch\n",
    "    \n",
    "    if test_x.shape[1] == 1:\n",
    "        actual_line.append(test_y[step][-1,-1].item())\n",
    "        pred_line.append(pred[-1,-1].item())\n",
    "    elif test_x.shape[1] > 1:\n",
    "        actual_line = test_y[step].cpu().detach().flatten().numpy()        # Only plot the last sequence of test batch\n",
    "        pred_line   = pred[:,-1].cpu().detach().flatten().numpy()                # Only plot the last sequence of test batch\n",
    "        \n",
    "print(\"Test Loss : {:.6f}\".format(loss.data))\n",
    "print(\"Prediction: {:.2f}\".format(float(pred[-1,-1].data)))\n",
    "print(\"Actual:     {:.2f}\".format(float(test_y[step][-1,-1].data)))\n",
    "\n",
    "\n",
    "plt.plot(test_y[step,:,-1,-1].cpu().detach().flatten().numpy(), 'r--')\n",
    "plt.plot(pred[:,-1,-1].cpu().detach().flatten().numpy(), 'b-')\n",
    "plt.show()\n",
    "print(test_y[step,:,-1,-1])\n",
    "print(pred[:,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b848f-3934-4484-8587-fb5400cd7b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa93b9-0c68-4771-8c31-4840a9e16c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
