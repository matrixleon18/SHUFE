{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514a643-3f57-45f8-89c3-f2a95e32aee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bfb1014-0453-4e7b-b73e-f6c7ea7b7f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling_data shape: (600, 60, 135)\n",
      "seq count: 600\n",
      "seq length: 60\n",
      "train_x: torch.Size([26, 23, 60, 134])\n",
      "train_y: torch.Size([26, 23, 1, 1])\n",
      "test_x:  torch.Size([2, 1, 60, 134])\n",
      "test_y:  torch.Size([2, 1, 1, 1])\n",
      "train_batch_count: 26\n",
      "test_batch_count:  2\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "\n",
    "dataset = dataset.fillna(0)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "\n",
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 60\n",
    "TRAIN_BATCH_SIZE = 23                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_SIZE = 1                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_COUNT = 2\n",
    "Y_SEQ_LEN = 1                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "Y_DIM = 1\n",
    "X_DIM = dataset.shape[1]-Y_SEQ_LEN                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                   # 数据一共是 seq_count x seq_len x (x_in_dim+Y_SEQ_LEN) \n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(SEQ_LENGTH))\n",
    "# print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "test_seq_count = TEST_BATCH_COUNT * TEST_BATCH_SIZE\n",
    "\n",
    "\n",
    "# train = rolling_data[:-test_seq_count].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "# test  = rolling_data[-test_seq_count:].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "train = rolling_data[:-test_seq_count].reshape(-1, TRAIN_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                    # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "test  = rolling_data[-test_seq_count:].reshape(-1, TEST_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)      # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "TRAIN_BATCH_SIZE = train.shape[1]\n",
    "TRAIN_BATCH_COUNT = train.shape[0]\n",
    "TEST_BATCH_SIZE = test.shape[1]\n",
    "TEST_BATCH_COUNT = test.shape[0]\n",
    "\n",
    "train = torch.tensor(train)\n",
    "test  = torch.tensor(test)\n",
    "\n",
    "# train = rolling_data[:train_batch_count, :, :, :]\n",
    "# test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_SEQ_LEN:], train[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_SEQ_LEN:],  test[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y_seq_len]  to  [test_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train.shape[0]))\n",
    "print(\"test_batch_count:  {}\".format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3033e44d-4dfa-4a7f-8fa6-b96783b5ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 LSTM 模型\n",
    "\n",
    "np.random.seed(1027)\n",
    "torch.manual_seed(1027)\n",
    "torch.cuda.manual_seed(1027)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TIME_STEP = SEQ_LENGTH                                        # 一般这个单独设定，这里为了简单，还是直接就等于seq_len的方便。其实也就是等于最长的那个sequence length\n",
    "INPUT_SIZE = dataset.shape[1]-1\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_layer_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        # self.h0 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size).double().to(device)\n",
    "        # self.c0 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size).double().to(device)\n",
    "        \n",
    "        self.init_weights2()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "\n",
    "    def init_weights2(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)    \n",
    "    \n",
    "    def init_weights3(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "    def forward(self, x, hidden, cell):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # layer 1\n",
    "        # x = self.linear_1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # lstm_out, (h_n, c_n) = self.lstm(x, (self.h0.detach(), self.c0.detach()))\n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, (hidden, cell))\n",
    "        \n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        predictions = self.linear_2(lstm_out)\n",
    "        return predictions, h_n, c_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "847eb8d0-f5f3-4943-ad9e-b9f2a7ba5c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 LSTM 模型 ---- 这里的损失函数是计算Sequence最后一个元素的预测数据和真实数据差异\n",
    "model = LSTMModel(input_size=INPUT_SIZE, hidden_layer_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1)\n",
    "model = model.double().to(device)\n",
    "LR = 1e-4\n",
    "loss_func = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "533d9251-af68-4657-a16a-e406fb4a1f39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 200 epoch loss: 0.6898 with lr: 0.0001\n",
      "1 of 200 epoch loss: 0.7018 with lr: 0.0001\n",
      "2 of 200 epoch loss: 0.5776 with lr: 0.0001\n",
      "3 of 200 epoch loss: 0.3096 with lr: 0.0001\n",
      "4 of 200 epoch loss: 0.2375 with lr: 0.0001\n",
      "5 of 200 epoch loss: 0.2208 with lr: 0.0001\n",
      "6 of 200 epoch loss: 0.2275 with lr: 0.0001\n",
      "7 of 200 epoch loss: 0.2270 with lr: 0.0001\n",
      "8 of 200 epoch loss: 0.2198 with lr: 0.0001\n",
      "9 of 200 epoch loss: 0.2133 with lr: 0.0001\n",
      "10 of 200 epoch loss: 0.2204 with lr: 0.0001\n",
      "11 of 200 epoch loss: 0.2300 with lr: 0.0001\n",
      "12 of 200 epoch loss: 0.2202 with lr: 0.0001\n",
      "13 of 200 epoch loss: 0.2053 with lr: 0.0001\n",
      "14 of 200 epoch loss: 0.2278 with lr: 0.0001\n",
      "15 of 200 epoch loss: 0.2200 with lr: 0.0001\n",
      "16 of 200 epoch loss: 0.2168 with lr: 0.0001\n",
      "17 of 200 epoch loss: 0.2271 with lr: 0.0001\n",
      "18 of 200 epoch loss: 0.2208 with lr: 0.0001\n",
      "19 of 200 epoch loss: 0.2440 with lr: 0.0001\n",
      "20 of 200 epoch loss: 0.2374 with lr: 0.0001\n",
      "21 of 200 epoch loss: 0.2056 with lr: 0.0001\n",
      "22 of 200 epoch loss: 0.2245 with lr: 0.0001\n",
      "23 of 200 epoch loss: 0.2143 with lr: 0.0001\n",
      "24 of 200 epoch loss: 0.2177 with lr: 0.0001\n",
      "25 of 200 epoch loss: 0.2176 with lr: 0.0001\n",
      "26 of 200 epoch loss: 0.2401 with lr: 0.0001\n",
      "27 of 200 epoch loss: 0.2584 with lr: 0.0001\n",
      "28 of 200 epoch loss: 0.2646 with lr: 0.0001\n",
      "29 of 200 epoch loss: 0.2929 with lr: 0.0001\n",
      "30 of 200 epoch loss: 0.2408 with lr: 0.0001\n",
      "31 of 200 epoch loss: 0.2261 with lr: 0.0001\n",
      "32 of 200 epoch loss: 0.1971 with lr: 0.0001\n",
      "33 of 200 epoch loss: 0.2221 with lr: 0.0001\n",
      "34 of 200 epoch loss: 0.2162 with lr: 0.0001\n",
      "35 of 200 epoch loss: 0.2028 with lr: 0.0001\n",
      "36 of 200 epoch loss: 0.2168 with lr: 0.0001\n",
      "37 of 200 epoch loss: 0.1835 with lr: 0.0001\n",
      "38 of 200 epoch loss: 0.1946 with lr: 0.0001\n",
      "39 of 200 epoch loss: 0.2079 with lr: 0.0001\n",
      "40 of 200 epoch loss: 0.1922 with lr: 0.0001\n",
      "41 of 200 epoch loss: 0.2085 with lr: 0.0001\n",
      "42 of 200 epoch loss: 0.1779 with lr: 0.0001\n",
      "43 of 200 epoch loss: 0.1913 with lr: 0.0001\n",
      "44 of 200 epoch loss: 0.1768 with lr: 0.0001\n",
      "45 of 200 epoch loss: 0.1934 with lr: 0.0001\n",
      "46 of 200 epoch loss: 0.1909 with lr: 0.0001\n",
      "47 of 200 epoch loss: 0.1998 with lr: 0.0001\n",
      "48 of 200 epoch loss: 0.1891 with lr: 0.0001\n",
      "49 of 200 epoch loss: 0.1857 with lr: 0.0001\n",
      "50 of 200 epoch loss: 0.2004 with lr: 0.0001\n",
      "51 of 200 epoch loss: 0.1778 with lr: 0.0001\n",
      "52 of 200 epoch loss: 0.1764 with lr: 0.0001\n",
      "53 of 200 epoch loss: 0.1641 with lr: 0.0001\n",
      "54 of 200 epoch loss: 0.1749 with lr: 0.0001\n",
      "55 of 200 epoch loss: 0.1738 with lr: 0.0001\n",
      "56 of 200 epoch loss: 0.1788 with lr: 0.0001\n",
      "57 of 200 epoch loss: 0.1810 with lr: 0.0001\n",
      "58 of 200 epoch loss: 0.1705 with lr: 0.0001\n",
      "59 of 200 epoch loss: 0.1805 with lr: 0.0001\n",
      "60 of 200 epoch loss: 0.1671 with lr: 0.0001\n",
      "61 of 200 epoch loss: 0.1678 with lr: 0.0001\n",
      "62 of 200 epoch loss: 0.1563 with lr: 0.0001\n",
      "63 of 200 epoch loss: 0.1653 with lr: 0.0001\n",
      "64 of 200 epoch loss: 0.1637 with lr: 0.0001\n",
      "65 of 200 epoch loss: 0.1723 with lr: 0.0001\n",
      "66 of 200 epoch loss: 0.1579 with lr: 0.0001\n",
      "67 of 200 epoch loss: 0.1464 with lr: 0.0001\n",
      "68 of 200 epoch loss: 0.1555 with lr: 0.0001\n",
      "69 of 200 epoch loss: 0.1520 with lr: 0.0001\n",
      "70 of 200 epoch loss: 0.1670 with lr: 0.0001\n",
      "71 of 200 epoch loss: 0.1549 with lr: 0.0001\n",
      "72 of 200 epoch loss: 0.1631 with lr: 0.0001\n",
      "73 of 200 epoch loss: 0.1651 with lr: 0.0001\n",
      "74 of 200 epoch loss: 0.1424 with lr: 0.0001\n",
      "75 of 200 epoch loss: 0.1674 with lr: 0.0001\n",
      "76 of 200 epoch loss: 0.1486 with lr: 0.0001\n",
      "77 of 200 epoch loss: 0.1575 with lr: 0.0001\n",
      "78 of 200 epoch loss: 0.1430 with lr: 0.0001\n",
      "79 of 200 epoch loss: 0.1493 with lr: 0.0001\n",
      "80 of 200 epoch loss: 0.1467 with lr: 0.0001\n",
      "81 of 200 epoch loss: 0.1408 with lr: 0.0001\n",
      "82 of 200 epoch loss: 0.1447 with lr: 0.0001\n",
      "83 of 200 epoch loss: 0.1449 with lr: 0.0001\n",
      "84 of 200 epoch loss: 0.1412 with lr: 0.0001\n",
      "85 of 200 epoch loss: 0.1328 with lr: 0.0001\n",
      "86 of 200 epoch loss: 0.1309 with lr: 0.0001\n",
      "87 of 200 epoch loss: 0.1346 with lr: 0.0001\n",
      "88 of 200 epoch loss: 0.1435 with lr: 0.0001\n",
      "89 of 200 epoch loss: 0.1334 with lr: 0.0001\n",
      "90 of 200 epoch loss: 0.1541 with lr: 0.0001\n",
      "91 of 200 epoch loss: 0.1480 with lr: 0.0001\n",
      "92 of 200 epoch loss: 0.1466 with lr: 0.0001\n",
      "93 of 200 epoch loss: 0.1225 with lr: 0.0001\n",
      "94 of 200 epoch loss: 0.1367 with lr: 0.0001\n",
      "95 of 200 epoch loss: 0.1279 with lr: 0.0001\n",
      "96 of 200 epoch loss: 0.1243 with lr: 0.0001\n",
      "97 of 200 epoch loss: 0.1267 with lr: 0.0001\n",
      "98 of 200 epoch loss: 0.1255 with lr: 0.0001\n",
      "99 of 200 epoch loss: 0.1168 with lr: 0.0001\n",
      "100 of 200 epoch loss: 0.1230 with lr: 0.0001\n",
      "101 of 200 epoch loss: 0.1230 with lr: 0.0001\n",
      "102 of 200 epoch loss: 0.1303 with lr: 0.0001\n",
      "103 of 200 epoch loss: 0.1318 with lr: 0.0001\n",
      "104 of 200 epoch loss: 0.1283 with lr: 0.0001\n",
      "105 of 200 epoch loss: 0.1286 with lr: 0.0001\n",
      "106 of 200 epoch loss: 0.1207 with lr: 0.0001\n",
      "107 of 200 epoch loss: 0.1160 with lr: 0.0001\n",
      "108 of 200 epoch loss: 0.1120 with lr: 0.0001\n",
      "109 of 200 epoch loss: 0.1178 with lr: 0.0001\n",
      "110 of 200 epoch loss: 0.1268 with lr: 0.0001\n",
      "111 of 200 epoch loss: 0.1093 with lr: 0.0001\n",
      "112 of 200 epoch loss: 0.1123 with lr: 0.0001\n",
      "113 of 200 epoch loss: 0.1085 with lr: 0.0001\n",
      "114 of 200 epoch loss: 0.1143 with lr: 0.0001\n",
      "115 of 200 epoch loss: 0.1136 with lr: 0.0001\n",
      "116 of 200 epoch loss: 0.1126 with lr: 0.0001\n",
      "117 of 200 epoch loss: 0.1083 with lr: 0.0001\n",
      "118 of 200 epoch loss: 0.1119 with lr: 0.0001\n",
      "119 of 200 epoch loss: 0.1186 with lr: 0.0001\n",
      "120 of 200 epoch loss: 0.1156 with lr: 0.0001\n",
      "121 of 200 epoch loss: 0.1338 with lr: 0.0001\n",
      "122 of 200 epoch loss: 0.1182 with lr: 0.0001\n",
      "123 of 200 epoch loss: 0.1186 with lr: 0.0001\n",
      "124 of 200 epoch loss: 0.1173 with lr: 0.0001\n",
      "125 of 200 epoch loss: 0.1071 with lr: 0.0001\n",
      "126 of 200 epoch loss: 0.1085 with lr: 0.0001\n",
      "127 of 200 epoch loss: 0.1076 with lr: 0.0001\n",
      "128 of 200 epoch loss: 0.1126 with lr: 0.0001\n",
      "129 of 200 epoch loss: 0.1156 with lr: 0.0001\n",
      "130 of 200 epoch loss: 0.0997 with lr: 0.0001\n",
      "131 of 200 epoch loss: 0.1155 with lr: 0.0001\n",
      "132 of 200 epoch loss: 0.1032 with lr: 0.0001\n",
      "133 of 200 epoch loss: 0.1032 with lr: 0.0001\n",
      "134 of 200 epoch loss: 0.1122 with lr: 0.0001\n",
      "135 of 200 epoch loss: 0.1050 with lr: 0.0001\n",
      "136 of 200 epoch loss: 0.1073 with lr: 0.0001\n",
      "137 of 200 epoch loss: 0.1056 with lr: 0.0001\n",
      "138 of 200 epoch loss: 0.1013 with lr: 0.0001\n",
      "139 of 200 epoch loss: 0.1032 with lr: 0.0001\n",
      "140 of 200 epoch loss: 0.1072 with lr: 0.0001\n",
      "141 of 200 epoch loss: 0.1155 with lr: 0.0001\n",
      "142 of 200 epoch loss: 0.1004 with lr: 0.0001\n",
      "143 of 200 epoch loss: 0.1019 with lr: 0.0001\n",
      "144 of 200 epoch loss: 0.1004 with lr: 0.0001\n",
      "145 of 200 epoch loss: 0.1104 with lr: 0.0001\n",
      "146 of 200 epoch loss: 0.1082 with lr: 0.0001\n",
      "147 of 200 epoch loss: 0.1042 with lr: 0.0001\n",
      "148 of 200 epoch loss: 0.1084 with lr: 0.0001\n",
      "149 of 200 epoch loss: 0.0963 with lr: 0.0001\n",
      "150 of 200 epoch loss: 0.0880 with lr: 0.0001\n",
      "151 of 200 epoch loss: 0.1017 with lr: 0.0001\n",
      "152 of 200 epoch loss: 0.0982 with lr: 0.0001\n",
      "153 of 200 epoch loss: 0.0993 with lr: 0.0001\n",
      "154 of 200 epoch loss: 0.0898 with lr: 0.0001\n",
      "155 of 200 epoch loss: 0.0919 with lr: 0.0001\n",
      "156 of 200 epoch loss: 0.0903 with lr: 0.0001\n",
      "157 of 200 epoch loss: 0.0928 with lr: 0.0001\n",
      "158 of 200 epoch loss: 0.0937 with lr: 0.0001\n",
      "159 of 200 epoch loss: 0.0881 with lr: 0.0001\n",
      "160 of 200 epoch loss: 0.0946 with lr: 0.0001\n",
      "161 of 200 epoch loss: 0.1046 with lr: 0.0001\n",
      "162 of 200 epoch loss: 0.0906 with lr: 0.0001\n",
      "163 of 200 epoch loss: 0.1030 with lr: 0.0001\n",
      "164 of 200 epoch loss: 0.0887 with lr: 0.0001\n",
      "165 of 200 epoch loss: 0.0953 with lr: 0.0001\n",
      "166 of 200 epoch loss: 0.0951 with lr: 0.0001\n",
      "167 of 200 epoch loss: 0.0916 with lr: 0.0001\n",
      "168 of 200 epoch loss: 0.0927 with lr: 0.0001\n",
      "169 of 200 epoch loss: 0.0882 with lr: 0.0001\n",
      "170 of 200 epoch loss: 0.0887 with lr: 0.0001\n",
      "171 of 200 epoch loss: 0.0874 with lr: 0.0001\n",
      "172 of 200 epoch loss: 0.0857 with lr: 0.0001\n",
      "173 of 200 epoch loss: 0.0786 with lr: 0.0001\n",
      "174 of 200 epoch loss: 0.0859 with lr: 0.0001\n",
      "175 of 200 epoch loss: 0.0862 with lr: 0.0001\n",
      "176 of 200 epoch loss: 0.0852 with lr: 0.0001\n",
      "177 of 200 epoch loss: 0.0823 with lr: 0.0001\n",
      "178 of 200 epoch loss: 0.0961 with lr: 0.0001\n",
      "179 of 200 epoch loss: 0.0848 with lr: 0.0001\n",
      "180 of 200 epoch loss: 0.0790 with lr: 0.0001\n",
      "181 of 200 epoch loss: 0.0844 with lr: 0.0001\n",
      "182 of 200 epoch loss: 0.0811 with lr: 0.0001\n",
      "183 of 200 epoch loss: 0.0839 with lr: 0.0001\n",
      "184 of 200 epoch loss: 0.0798 with lr: 0.0001\n",
      "185 of 200 epoch loss: 0.0856 with lr: 0.0001\n",
      "186 of 200 epoch loss: 0.0794 with lr: 0.0001\n",
      "187 of 200 epoch loss: 0.0771 with lr: 0.0001\n",
      "188 of 200 epoch loss: 0.0768 with lr: 0.0001\n",
      "189 of 200 epoch loss: 0.0796 with lr: 0.0001\n",
      "190 of 200 epoch loss: 0.0852 with lr: 0.0001\n",
      "191 of 200 epoch loss: 0.0837 with lr: 0.0001\n",
      "192 of 200 epoch loss: 0.0809 with lr: 0.0001\n",
      "193 of 200 epoch loss: 0.0775 with lr: 0.0001\n",
      "194 of 200 epoch loss: 0.0798 with lr: 0.0001\n",
      "195 of 200 epoch loss: 0.0762 with lr: 0.0001\n",
      "196 of 200 epoch loss: 0.0796 with lr: 0.0001\n",
      "197 of 200 epoch loss: 0.0750 with lr: 0.0001\n",
      "198 of 200 epoch loss: 0.0804 with lr: 0.0001\n",
      "199 of 200 epoch loss: 0.0711 with lr: 0.0001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练 LSTM 模型;  ---- 这里的损失函数是计算Sequence最后一个元素的预测数据和真实数据差异\n",
    "model.train()\n",
    "epoches = 200\n",
    "epoch_loss = 0\n",
    "epoch_loss_list = []\n",
    "train_batch_count = train_x.shape[0]\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, TRAIN_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, TRAIN_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        pred, hn, cn = model(train_x[step], h0, c0)\n",
    "        # h0, c0 = hn.detach(), cn.detach()\n",
    "        loss = loss_func(pred[:,-1], train_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data.cpu()\n",
    "        \n",
    "    if epoch_loss.item() < 1e-4:\n",
    "        print('Epoch [{}/{}], Loss: {:.5f}'.format(epoch+1, epoches, loss.item()))\n",
    "        print(\"The loss value is reached\")\n",
    "        break\n",
    "\n",
    "    print(\"{} of {} epoch loss: {:.4f} with lr: {}\".format(epoch, epoches, epoch_loss.item(), optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    if (epoch+1) % 2000 ==0:\n",
    "        scheduler.step()\n",
    "    # print(\"learning rate: {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    # for p in optimizer.param_groups:\n",
    "    #     p['lr'] *= 0.99\n",
    "    \n",
    "plt.plot(epoch_loss_list)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab98543-e67b-4112-918f-6731252e65f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model, 'e:\\\\Model_LSTM1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5e146b-9f2a-4822-8d7e-c71ee74a6730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model = torch.load('e:\\\\Model_LSTM1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602f9272-9fd8-40a2-ae45-f92f793bbe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Loss average:0.010214\n",
      "Prediction: -0.09\n",
      "Actual:     0.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 用模型预测数据\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_batch_count = test_x.shape[0]\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "actual_line=[]\n",
    "pred_line=[]\n",
    "\n",
    "for step in range(test_batch_count):\n",
    "    pred, hn, cn = model(test_x[step], h0, c0)\n",
    "    \n",
    "    h0, c0 = hn.detach(), cn.detach()\n",
    "\n",
    "    loss = loss_func(pred[:,-1], test_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "    \n",
    "    test_loss += loss.cpu()\n",
    "    \n",
    "    actual_line.append(test_y[step][-1,-1].item())\n",
    "    pred_line.append(pred[-1,-1].item())\n",
    "        \n",
    "print(\"Prediction Loss average:{:.6f}\".format(test_loss.data/(step+1)))\n",
    "print(\"Prediction: {:.2f}\".format(float(pred[-1,-1].data)))\n",
    "print(\"Actual:     {:.2f}\".format(float(test_y[step][-1,-1].data)))\n",
    "\n",
    "# actual_line = test_y[step][-1].cpu().detach().flatten().numpy()        # Only plot the last sequence of test batch\n",
    "# pred_line   = pred[-1].cpu().detach().flatten().numpy()                # Only plot the last sequence of test batch\n",
    "plt.plot(actual_line, 'r--')\n",
    "plt.plot(pred_line, 'b-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9631a87-28ac-4b26-9b2b-24d900ffec9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.18 GiB (GPU 0; 8.00 GiB total capacity; 4.53 GiB already allocated; 1021.50 MiB free; 5.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4864\\4134883276.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# Compare the all sequences' last element in one batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.18 GiB (GPU 0; 8.00 GiB total capacity; 4.53 GiB already allocated; 1021.50 MiB free; 5.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 下面Train除了最后一个Sequence，再用最后一个Sequence来预测\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "\n",
    "dataset = dataset.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 60\n",
    "TRAIN_BATCH_SIZE = 599                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_SIZE = 1                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_COUNT = 1\n",
    "Y_SEQ_LEN = 1                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "Y_DIM = 1\n",
    "X_DIM = dataset.shape[1]-Y_SEQ_LEN                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                   # 数据一共是 seq_count x seq_len x (x_in_dim+Y_SEQ_LEN) \n",
    "\n",
    "test_seq_count = TEST_BATCH_COUNT * TEST_BATCH_SIZE\n",
    "\n",
    "train = rolling_data[:-test_seq_count].reshape(1, -1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                    # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "test  = rolling_data[-test_seq_count:].reshape(-1, TEST_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)      # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "TRAIN_BATCH_SIZE = train.shape[1]\n",
    "TRAIN_BATCH_COUNT = train.shape[0]\n",
    "TEST_BATCH_SIZE = test.shape[1]\n",
    "TEST_BATCH_COUNT = test.shape[0]\n",
    "\n",
    "train = torch.tensor(train)\n",
    "test  = torch.tensor(test)\n",
    "\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_SEQ_LEN:], train[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_SEQ_LEN:],  test[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y_seq_len]  to  [test_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "\n",
    "\n",
    "model = LSTMModel(input_size=X_DIM, hidden_layer_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1)\n",
    "model = model.double().to(device)\n",
    "LR = 1e-5\n",
    "loss_func = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "epoches = 200\n",
    "epoch_loss = 0\n",
    "epoch_loss_list = []\n",
    "train_batch_count = train_x.shape[0]\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, 599, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, 599, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        pred, hn, cn = model(train_x[step], h0, c0)\n",
    "        # h0, c0 = hn.detach(), cn.detach()\n",
    "        loss = loss_func(pred[:,-1], train_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data.cpu()\n",
    "        \n",
    "    if epoch_loss.item() < 1e-4:\n",
    "        print('Epoch [{}/{}], Loss: {:.5f}'.format(epoch+1, epoches, loss.item()))\n",
    "        print(\"The loss value is reached\")\n",
    "        break\n",
    "\n",
    "    print(\"{} of {} epoch loss: {:.4f} with lr: {}\".format(epoch, epoches, epoch_loss.item(), optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "\n",
    "    \n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_batch_count = test_x.shape[0]\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "actual_line=[]\n",
    "pred_line=[]\n",
    "\n",
    "for step in range(test_batch_count):\n",
    "    pred, hn, cn = model(test_x[step], h0, c0)\n",
    "    \n",
    "    h0, c0 = hn.detach(), cn.detach()\n",
    "\n",
    "    loss = loss_func(pred[:,-1], test_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "    \n",
    "    test_loss += loss.cpu()\n",
    "    \n",
    "    actual_line.append(test_y[step][-1,-1].item())\n",
    "    pred_line.append(pred[-1,-1].item())\n",
    "        \n",
    "print(\"Prediction Loss average:{:.6f}\".format(test_loss.data/(step+1)))\n",
    "print(\"Prediction: {:.2f}\".format(float(pred[-1,-1].data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3c66f-e2ca-4818-9856-1c396c9319c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
