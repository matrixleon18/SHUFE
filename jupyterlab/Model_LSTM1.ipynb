{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514a643-3f57-45f8-89c3-f2a95e32aee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfb1014-0453-4e7b-b73e-f6c7ea7b7f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling_data shape: (600, 60, 135)\n",
      "seq count: 600\n",
      "seq length: 60\n",
      "train_x: torch.Size([1, 593, 60, 134])\n",
      "train_y: torch.Size([1, 593, 1, 1])\n",
      "test_x:  torch.Size([7, 1, 60, 134])\n",
      "test_y:  torch.Size([7, 1, 1, 1])\n",
      "train_batch_count: 1\n",
      "test_batch_count:  7\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 GPU 优先\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "\n",
    "dataset = dataset.fillna(0)\n",
    "# print(dataset.columns)\n",
    "# print(dataset.shape)\n",
    "# print(dataset.tail())\n",
    "\n",
    "\n",
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 60\n",
    "TRAIN_BATCH_SIZE = 593                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_SIZE = 1                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_COUNT = 7\n",
    "Y_SEQ_LEN = 1                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "Y_DIM = 1\n",
    "X_DIM = dataset.shape[1]-Y_SEQ_LEN                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                   # 数据一共是 seq_count x seq_len x (x_in_dim+Y_SEQ_LEN) \n",
    "\n",
    "print(\"rolling_data shape: {}\".format(rolling_data.shape))\n",
    "print(\"seq count: {}\".format(rolling_data.shape[0]))                                       # 所以一共有 seq_count 列数据，每一行的数据是123维 （包括y）\n",
    "print(\"seq length: {}\".format(SEQ_LENGTH))\n",
    "# print(\"batch size: {}\".format(BATCH_SIZE))\n",
    "\n",
    "test_seq_count = TEST_BATCH_COUNT * TEST_BATCH_SIZE\n",
    "\n",
    "\n",
    "# train = rolling_data[:-test_seq_count].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "# test  = rolling_data[-test_seq_count:].reshape(-1, BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)           # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "train = rolling_data[:-test_seq_count].reshape(-1, TRAIN_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                    # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "test  = rolling_data[-test_seq_count:].reshape(-1, TEST_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)      # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "TRAIN_BATCH_SIZE = train.shape[1]\n",
    "TRAIN_BATCH_COUNT = train.shape[0]\n",
    "TEST_BATCH_SIZE = test.shape[1]\n",
    "TEST_BATCH_COUNT = test.shape[0]\n",
    "\n",
    "train = torch.tensor(train)\n",
    "test  = torch.tensor(test)\n",
    "\n",
    "# train = rolling_data[:train_batch_count, :, :, :]\n",
    "# test  = rolling_data[train_batch_count:, :, :, :]\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_SEQ_LEN:], train[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_SEQ_LEN:],  test[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y_seq_len]  to  [test_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x:  {}\".format(test_x.shape))\n",
    "print(\"test_y:  {}\".format(test_y.shape))\n",
    "print(\"train_batch_count: {}\".format(train.shape[0]))\n",
    "print(\"test_batch_count:  {}\".format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3033e44d-4dfa-4a7f-8fa6-b96783b5ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 LSTM 模型\n",
    "\n",
    "np.random.seed(1027)\n",
    "torch.manual_seed(1027)\n",
    "torch.cuda.manual_seed(1027)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TIME_STEP = SEQ_LENGTH                                        # 一般这个单独设定，这里为了简单，还是直接就等于seq_len的方便。其实也就是等于最长的那个sequence length\n",
    "INPUT_SIZE = dataset.shape[1]-1\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_layer_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        # self.h0 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size).double().to(device)\n",
    "        # self.c0 = torch.zeros(NUM_LAYERS, BATCH_SIZE, hidden_layer_size).double().to(device)\n",
    "        \n",
    "        self.init_weights2()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "\n",
    "    def init_weights2(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.kaiming_normal_(param)    \n",
    "    \n",
    "    def init_weights3(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "    def forward(self, x, hidden, cell):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # layer 1\n",
    "        # x = self.linear_1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # lstm_out, (h_n, c_n) = self.lstm(x, (self.h0.detach(), self.c0.detach()))\n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, (hidden, cell))\n",
    "        \n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        predictions = self.linear_2(lstm_out)\n",
    "        return predictions, h_n, c_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847eb8d0-f5f3-4943-ad9e-b9f2a7ba5c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 LSTM 模型 ---- 这里的损失函数是计算Sequence最后一个元素的预测数据和真实数据差异\n",
    "model = LSTMModel(input_size=INPUT_SIZE, hidden_layer_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1)\n",
    "model = model.double().to(device)\n",
    "LR = 1e-5\n",
    "loss_func = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "533d9251-af68-4657-a16a-e406fb4a1f39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 1000 epoch loss: 0.0242 with lr: 1e-05\n",
      "1 of 1000 epoch loss: 0.0178 with lr: 1e-05\n",
      "2 of 1000 epoch loss: 0.0149 with lr: 1e-05\n",
      "3 of 1000 epoch loss: 0.0140 with lr: 1e-05\n",
      "4 of 1000 epoch loss: 0.0137 with lr: 1e-05\n",
      "5 of 1000 epoch loss: 0.0148 with lr: 1e-05\n",
      "6 of 1000 epoch loss: 0.0150 with lr: 1e-05\n",
      "7 of 1000 epoch loss: 0.0131 with lr: 1e-05\n",
      "8 of 1000 epoch loss: 0.0144 with lr: 1e-05\n",
      "9 of 1000 epoch loss: 0.0133 with lr: 1e-05\n",
      "10 of 1000 epoch loss: 0.0121 with lr: 1e-05\n",
      "11 of 1000 epoch loss: 0.0120 with lr: 1e-05\n",
      "12 of 1000 epoch loss: 0.0119 with lr: 1e-05\n",
      "13 of 1000 epoch loss: 0.0129 with lr: 1e-05\n",
      "14 of 1000 epoch loss: 0.0128 with lr: 1e-05\n",
      "15 of 1000 epoch loss: 0.0124 with lr: 1e-05\n",
      "16 of 1000 epoch loss: 0.0133 with lr: 1e-05\n",
      "17 of 1000 epoch loss: 0.0123 with lr: 1e-05\n",
      "18 of 1000 epoch loss: 0.0122 with lr: 1e-05\n",
      "19 of 1000 epoch loss: 0.0122 with lr: 1e-05\n",
      "20 of 1000 epoch loss: 0.0124 with lr: 1e-05\n",
      "21 of 1000 epoch loss: 0.0116 with lr: 1e-05\n",
      "22 of 1000 epoch loss: 0.0103 with lr: 1e-05\n",
      "23 of 1000 epoch loss: 0.0122 with lr: 1e-05\n",
      "24 of 1000 epoch loss: 0.0118 with lr: 1e-05\n",
      "25 of 1000 epoch loss: 0.0117 with lr: 1e-05\n",
      "26 of 1000 epoch loss: 0.0118 with lr: 1e-05\n",
      "27 of 1000 epoch loss: 0.0120 with lr: 1e-05\n",
      "28 of 1000 epoch loss: 0.0122 with lr: 1e-05\n",
      "29 of 1000 epoch loss: 0.0113 with lr: 1e-05\n",
      "30 of 1000 epoch loss: 0.0114 with lr: 1e-05\n",
      "31 of 1000 epoch loss: 0.0124 with lr: 1e-05\n",
      "32 of 1000 epoch loss: 0.0112 with lr: 1e-05\n",
      "33 of 1000 epoch loss: 0.0110 with lr: 1e-05\n",
      "34 of 1000 epoch loss: 0.0110 with lr: 1e-05\n",
      "35 of 1000 epoch loss: 0.0120 with lr: 1e-05\n",
      "36 of 1000 epoch loss: 0.0108 with lr: 1e-05\n",
      "37 of 1000 epoch loss: 0.0116 with lr: 1e-05\n",
      "38 of 1000 epoch loss: 0.0108 with lr: 1e-05\n",
      "39 of 1000 epoch loss: 0.0116 with lr: 1e-05\n",
      "40 of 1000 epoch loss: 0.0113 with lr: 1e-05\n",
      "41 of 1000 epoch loss: 0.0107 with lr: 1e-05\n",
      "42 of 1000 epoch loss: 0.0109 with lr: 1e-05\n",
      "43 of 1000 epoch loss: 0.0113 with lr: 1e-05\n",
      "44 of 1000 epoch loss: 0.0104 with lr: 1e-05\n",
      "45 of 1000 epoch loss: 0.0114 with lr: 1e-05\n",
      "46 of 1000 epoch loss: 0.0110 with lr: 1e-05\n",
      "47 of 1000 epoch loss: 0.0102 with lr: 1e-05\n",
      "48 of 1000 epoch loss: 0.0108 with lr: 1e-05\n",
      "49 of 1000 epoch loss: 0.0108 with lr: 1e-05\n",
      "50 of 1000 epoch loss: 0.0104 with lr: 1e-05\n",
      "51 of 1000 epoch loss: 0.0106 with lr: 1e-05\n",
      "52 of 1000 epoch loss: 0.0107 with lr: 1e-05\n",
      "53 of 1000 epoch loss: 0.0106 with lr: 1e-05\n",
      "54 of 1000 epoch loss: 0.0107 with lr: 1e-05\n",
      "55 of 1000 epoch loss: 0.0104 with lr: 1e-05\n",
      "56 of 1000 epoch loss: 0.0106 with lr: 1e-05\n",
      "57 of 1000 epoch loss: 0.0103 with lr: 1e-05\n",
      "58 of 1000 epoch loss: 0.0109 with lr: 1e-05\n",
      "59 of 1000 epoch loss: 0.0107 with lr: 1e-05\n",
      "60 of 1000 epoch loss: 0.0109 with lr: 1e-05\n",
      "61 of 1000 epoch loss: 0.0109 with lr: 1e-05\n",
      "62 of 1000 epoch loss: 0.0111 with lr: 1e-05\n",
      "63 of 1000 epoch loss: 0.0101 with lr: 1e-05\n",
      "64 of 1000 epoch loss: 0.0103 with lr: 1e-05\n",
      "65 of 1000 epoch loss: 0.0106 with lr: 1e-05\n",
      "66 of 1000 epoch loss: 0.0108 with lr: 1e-05\n",
      "67 of 1000 epoch loss: 0.0103 with lr: 1e-05\n",
      "68 of 1000 epoch loss: 0.0102 with lr: 1e-05\n",
      "69 of 1000 epoch loss: 0.0100 with lr: 1e-05\n",
      "70 of 1000 epoch loss: 0.0110 with lr: 1e-05\n",
      "71 of 1000 epoch loss: 0.0113 with lr: 1e-05\n",
      "72 of 1000 epoch loss: 0.0114 with lr: 1e-05\n",
      "73 of 1000 epoch loss: 0.0095 with lr: 1e-05\n",
      "74 of 1000 epoch loss: 0.0109 with lr: 1e-05\n",
      "75 of 1000 epoch loss: 0.0108 with lr: 1e-05\n",
      "76 of 1000 epoch loss: 0.0100 with lr: 1e-05\n",
      "77 of 1000 epoch loss: 0.0107 with lr: 1e-05\n",
      "78 of 1000 epoch loss: 0.0108 with lr: 1e-05\n",
      "79 of 1000 epoch loss: 0.0110 with lr: 1e-05\n",
      "80 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "81 of 1000 epoch loss: 0.0105 with lr: 1e-05\n",
      "82 of 1000 epoch loss: 0.0098 with lr: 1e-05\n",
      "83 of 1000 epoch loss: 0.0105 with lr: 1e-05\n",
      "84 of 1000 epoch loss: 0.0103 with lr: 1e-05\n",
      "85 of 1000 epoch loss: 0.0103 with lr: 1e-05\n",
      "86 of 1000 epoch loss: 0.0104 with lr: 1e-05\n",
      "87 of 1000 epoch loss: 0.0100 with lr: 1e-05\n",
      "88 of 1000 epoch loss: 0.0104 with lr: 1e-05\n",
      "89 of 1000 epoch loss: 0.0100 with lr: 1e-05\n",
      "90 of 1000 epoch loss: 0.0101 with lr: 1e-05\n",
      "91 of 1000 epoch loss: 0.0106 with lr: 1e-05\n",
      "92 of 1000 epoch loss: 0.0098 with lr: 1e-05\n",
      "93 of 1000 epoch loss: 0.0101 with lr: 1e-05\n",
      "94 of 1000 epoch loss: 0.0104 with lr: 1e-05\n",
      "95 of 1000 epoch loss: 0.0101 with lr: 1e-05\n",
      "96 of 1000 epoch loss: 0.0104 with lr: 1e-05\n",
      "97 of 1000 epoch loss: 0.0099 with lr: 1e-05\n",
      "98 of 1000 epoch loss: 0.0095 with lr: 1e-05\n",
      "99 of 1000 epoch loss: 0.0101 with lr: 1e-05\n",
      "100 of 1000 epoch loss: 0.0100 with lr: 1e-05\n",
      "101 of 1000 epoch loss: 0.0109 with lr: 1e-05\n",
      "102 of 1000 epoch loss: 0.0099 with lr: 1e-05\n",
      "103 of 1000 epoch loss: 0.0102 with lr: 1e-05\n",
      "104 of 1000 epoch loss: 0.0098 with lr: 1e-05\n",
      "105 of 1000 epoch loss: 0.0095 with lr: 1e-05\n",
      "106 of 1000 epoch loss: 0.0096 with lr: 1e-05\n",
      "107 of 1000 epoch loss: 0.0097 with lr: 1e-05\n",
      "108 of 1000 epoch loss: 0.0101 with lr: 1e-05\n",
      "109 of 1000 epoch loss: 0.0104 with lr: 1e-05\n",
      "110 of 1000 epoch loss: 0.0096 with lr: 1e-05\n",
      "111 of 1000 epoch loss: 0.0090 with lr: 1e-05\n",
      "112 of 1000 epoch loss: 0.0099 with lr: 1e-05\n",
      "113 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "114 of 1000 epoch loss: 0.0101 with lr: 1e-05\n",
      "115 of 1000 epoch loss: 0.0101 with lr: 1e-05\n",
      "116 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "117 of 1000 epoch loss: 0.0103 with lr: 1e-05\n",
      "118 of 1000 epoch loss: 0.0102 with lr: 1e-05\n",
      "119 of 1000 epoch loss: 0.0109 with lr: 1e-05\n",
      "120 of 1000 epoch loss: 0.0097 with lr: 1e-05\n",
      "121 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "122 of 1000 epoch loss: 0.0095 with lr: 1e-05\n",
      "123 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "124 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "125 of 1000 epoch loss: 0.0103 with lr: 1e-05\n",
      "126 of 1000 epoch loss: 0.0095 with lr: 1e-05\n",
      "127 of 1000 epoch loss: 0.0098 with lr: 1e-05\n",
      "128 of 1000 epoch loss: 0.0097 with lr: 1e-05\n",
      "129 of 1000 epoch loss: 0.0107 with lr: 1e-05\n",
      "130 of 1000 epoch loss: 0.0100 with lr: 1e-05\n",
      "131 of 1000 epoch loss: 0.0096 with lr: 1e-05\n",
      "132 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "133 of 1000 epoch loss: 0.0099 with lr: 1e-05\n",
      "134 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "135 of 1000 epoch loss: 0.0098 with lr: 1e-05\n",
      "136 of 1000 epoch loss: 0.0102 with lr: 1e-05\n",
      "137 of 1000 epoch loss: 0.0104 with lr: 1e-05\n",
      "138 of 1000 epoch loss: 0.0098 with lr: 1e-05\n",
      "139 of 1000 epoch loss: 0.0098 with lr: 1e-05\n",
      "140 of 1000 epoch loss: 0.0095 with lr: 1e-05\n",
      "141 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "142 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "143 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "144 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "145 of 1000 epoch loss: 0.0097 with lr: 1e-05\n",
      "146 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "147 of 1000 epoch loss: 0.0102 with lr: 1e-05\n",
      "148 of 1000 epoch loss: 0.0099 with lr: 1e-05\n",
      "149 of 1000 epoch loss: 0.0096 with lr: 1e-05\n",
      "150 of 1000 epoch loss: 0.0097 with lr: 1e-05\n",
      "151 of 1000 epoch loss: 0.0090 with lr: 1e-05\n",
      "152 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "153 of 1000 epoch loss: 0.0100 with lr: 1e-05\n",
      "154 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "155 of 1000 epoch loss: 0.0099 with lr: 1e-05\n",
      "156 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "157 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "158 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "159 of 1000 epoch loss: 0.0096 with lr: 1e-05\n",
      "160 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "161 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "162 of 1000 epoch loss: 0.0095 with lr: 1e-05\n",
      "163 of 1000 epoch loss: 0.0099 with lr: 1e-05\n",
      "164 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "165 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "166 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "167 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "168 of 1000 epoch loss: 0.0095 with lr: 1e-05\n",
      "169 of 1000 epoch loss: 0.0095 with lr: 1e-05\n",
      "170 of 1000 epoch loss: 0.0100 with lr: 1e-05\n",
      "171 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "172 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "173 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "174 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "175 of 1000 epoch loss: 0.0098 with lr: 1e-05\n",
      "176 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "177 of 1000 epoch loss: 0.0097 with lr: 1e-05\n",
      "178 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "179 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "180 of 1000 epoch loss: 0.0097 with lr: 1e-05\n",
      "181 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "182 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "183 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "184 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "185 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "186 of 1000 epoch loss: 0.0090 with lr: 1e-05\n",
      "187 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "188 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "189 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "190 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "191 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "192 of 1000 epoch loss: 0.0090 with lr: 1e-05\n",
      "193 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "194 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "195 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "196 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "197 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "198 of 1000 epoch loss: 0.0090 with lr: 1e-05\n",
      "199 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "200 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "201 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "202 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "203 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "204 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "205 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "206 of 1000 epoch loss: 0.0097 with lr: 1e-05\n",
      "207 of 1000 epoch loss: 0.0098 with lr: 1e-05\n",
      "208 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "209 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "210 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "211 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "212 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "213 of 1000 epoch loss: 0.0090 with lr: 1e-05\n",
      "214 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "215 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "216 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "217 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "218 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "219 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "220 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "221 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "222 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "223 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "224 of 1000 epoch loss: 0.0095 with lr: 1e-05\n",
      "225 of 1000 epoch loss: 0.0090 with lr: 1e-05\n",
      "226 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "227 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "228 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "229 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "230 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "231 of 1000 epoch loss: 0.0090 with lr: 1e-05\n",
      "232 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "233 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "234 of 1000 epoch loss: 0.0093 with lr: 1e-05\n",
      "235 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "236 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "237 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "238 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "239 of 1000 epoch loss: 0.0090 with lr: 1e-05\n",
      "240 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "241 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "242 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "243 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "244 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "245 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "246 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "247 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "248 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "249 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "250 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "251 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "252 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "253 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "254 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "255 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "256 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "257 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "258 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "259 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "260 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "261 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "262 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "263 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "264 of 1000 epoch loss: 0.0091 with lr: 1e-05\n",
      "265 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "266 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "267 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "268 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "269 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "270 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "271 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "272 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "273 of 1000 epoch loss: 0.0094 with lr: 1e-05\n",
      "274 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "275 of 1000 epoch loss: 0.0092 with lr: 1e-05\n",
      "276 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "277 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "278 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "279 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "280 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "281 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "282 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "283 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "284 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "285 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "286 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "287 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "288 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "289 of 1000 epoch loss: 0.0090 with lr: 1e-05\n",
      "290 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "291 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "292 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "293 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "294 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "295 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "296 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "297 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "298 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "299 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "300 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "301 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "302 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "303 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "304 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "305 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "306 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "307 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "308 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "309 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "310 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "311 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "312 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "313 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "314 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "315 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "316 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "317 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "318 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "319 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "320 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "321 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "322 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "323 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "324 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "325 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "326 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "327 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "328 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "329 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "330 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "331 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "332 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "333 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "334 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "335 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "336 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "337 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "338 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "339 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "340 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "341 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "342 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "343 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "344 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "345 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "346 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "347 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "348 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "349 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "350 of 1000 epoch loss: 0.0088 with lr: 1e-05\n",
      "351 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "352 of 1000 epoch loss: 0.0087 with lr: 1e-05\n",
      "353 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "354 of 1000 epoch loss: 0.0086 with lr: 1e-05\n",
      "355 of 1000 epoch loss: 0.0089 with lr: 1e-05\n",
      "356 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "357 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "358 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "359 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "360 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "361 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "362 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "363 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "364 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "365 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "366 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "367 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "368 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "369 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "370 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "371 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "372 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "373 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "374 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "375 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "376 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "377 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "378 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "379 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "380 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "381 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "382 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "383 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "384 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "385 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "386 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "387 of 1000 epoch loss: 0.0085 with lr: 1e-05\n",
      "388 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "389 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "390 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "391 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "392 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "393 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "394 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "395 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "396 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "397 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "398 of 1000 epoch loss: 0.0084 with lr: 1e-05\n",
      "399 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "400 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "401 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "402 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "403 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "404 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "405 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "406 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "407 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "408 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "409 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "410 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "411 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "412 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "413 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "414 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "415 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "416 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "417 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "418 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "419 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "420 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "421 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "422 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "423 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "424 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "425 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "426 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "427 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "428 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "429 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "430 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "431 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "432 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "433 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "434 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "435 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "436 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "437 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "438 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "439 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "440 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "441 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "442 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "443 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "444 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "445 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "446 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "447 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "448 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "449 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "450 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "451 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "452 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "453 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "454 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "455 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "456 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "457 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "458 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "459 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "460 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "461 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "462 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "463 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "464 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "465 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "466 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "467 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "468 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "469 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "470 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "471 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "472 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "473 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "474 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "475 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "476 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "477 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "478 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "479 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "480 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "481 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "482 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "483 of 1000 epoch loss: 0.0083 with lr: 1e-05\n",
      "484 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "485 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "486 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "487 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "488 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "489 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "490 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "491 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "492 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "493 of 1000 epoch loss: 0.0071 with lr: 1e-05\n",
      "494 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "495 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "496 of 1000 epoch loss: 0.0070 with lr: 1e-05\n",
      "497 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "498 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "499 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "500 of 1000 epoch loss: 0.0071 with lr: 1e-05\n",
      "501 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "502 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "503 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "504 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "505 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "506 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "507 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "508 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "509 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "510 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "511 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "512 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "513 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "514 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "515 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "516 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "517 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "518 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "519 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "520 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "521 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "522 of 1000 epoch loss: 0.0070 with lr: 1e-05\n",
      "523 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "524 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "525 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "526 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "527 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "528 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "529 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "530 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "531 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "532 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "533 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "534 of 1000 epoch loss: 0.0081 with lr: 1e-05\n",
      "535 of 1000 epoch loss: 0.0082 with lr: 1e-05\n",
      "536 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "537 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "538 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "539 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "540 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "541 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "542 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "543 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "544 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "545 of 1000 epoch loss: 0.0068 with lr: 1e-05\n",
      "546 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "547 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "548 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "549 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "550 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "551 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "552 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "553 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "554 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "555 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "556 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "557 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "558 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "559 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "560 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "561 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "562 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "563 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "564 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "565 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "566 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "567 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "568 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "569 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "570 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "571 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "572 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "573 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "574 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "575 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "576 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "577 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "578 of 1000 epoch loss: 0.0070 with lr: 1e-05\n",
      "579 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "580 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "581 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "582 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "583 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "584 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "585 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "586 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "587 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "588 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "589 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "590 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "591 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "592 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "593 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "594 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "595 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "596 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "597 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "598 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "599 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "600 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "601 of 1000 epoch loss: 0.0071 with lr: 1e-05\n",
      "602 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "603 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "604 of 1000 epoch loss: 0.0078 with lr: 1e-05\n",
      "605 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "606 of 1000 epoch loss: 0.0080 with lr: 1e-05\n",
      "607 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "608 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "609 of 1000 epoch loss: 0.0077 with lr: 1e-05\n",
      "610 of 1000 epoch loss: 0.0071 with lr: 1e-05\n",
      "611 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "612 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "613 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "614 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "615 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "616 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "617 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "618 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "619 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "620 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "621 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "622 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "623 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "624 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "625 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "626 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "627 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "628 of 1000 epoch loss: 0.0071 with lr: 1e-05\n",
      "629 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "630 of 1000 epoch loss: 0.0076 with lr: 1e-05\n",
      "631 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "632 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "633 of 1000 epoch loss: 0.0070 with lr: 1e-05\n",
      "634 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "635 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "636 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "637 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "638 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "639 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "640 of 1000 epoch loss: 0.0066 with lr: 1e-05\n",
      "641 of 1000 epoch loss: 0.0069 with lr: 1e-05\n",
      "642 of 1000 epoch loss: 0.0071 with lr: 1e-05\n",
      "643 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "644 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "645 of 1000 epoch loss: 0.0070 with lr: 1e-05\n",
      "646 of 1000 epoch loss: 0.0071 with lr: 1e-05\n",
      "647 of 1000 epoch loss: 0.0071 with lr: 1e-05\n",
      "648 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "649 of 1000 epoch loss: 0.0070 with lr: 1e-05\n",
      "650 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "651 of 1000 epoch loss: 0.0079 with lr: 1e-05\n",
      "652 of 1000 epoch loss: 0.0071 with lr: 1e-05\n",
      "653 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "654 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "655 of 1000 epoch loss: 0.0075 with lr: 1e-05\n",
      "656 of 1000 epoch loss: 0.0068 with lr: 1e-05\n",
      "657 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "658 of 1000 epoch loss: 0.0072 with lr: 1e-05\n",
      "659 of 1000 epoch loss: 0.0074 with lr: 1e-05\n",
      "660 of 1000 epoch loss: 0.0073 with lr: 1e-05\n",
      "661 of 1000 epoch loss: 0.0076 with lr: 1e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17596\\1214034847.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练 LSTM 模型;  ---- 这里的损失函数是计算Sequence最后一个元素的预测数据和真实数据差异\n",
    "model.train()\n",
    "epoches = 1000\n",
    "epoch_loss = 0\n",
    "epoch_loss_list = []\n",
    "train_batch_count = train_x.shape[0]\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, TRAIN_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, TRAIN_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        pred, hn, cn = model(train_x[step], h0, c0)\n",
    "        # h0, c0 = hn.detach(), cn.detach()\n",
    "        loss = loss_func(pred[:,-1], train_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data.cpu()\n",
    "        \n",
    "    if epoch_loss.item() < 1e-4:\n",
    "        print('Epoch [{}/{}], Loss: {:.5f}'.format(epoch+1, epoches, loss.item()))\n",
    "        print(\"The loss value is reached\")\n",
    "        break\n",
    "\n",
    "    print(\"{} of {} epoch loss: {:.4f} with lr: {}\".format(epoch, epoches, epoch_loss.item(), optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    if (epoch+1) % 2000 ==0:\n",
    "        scheduler.step()\n",
    "    # print(\"learning rate: {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    # for p in optimizer.param_groups:\n",
    "    #     p['lr'] *= 0.99\n",
    "    \n",
    "plt.plot(epoch_loss_list)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab98543-e67b-4112-918f-6731252e65f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model, 'e:\\\\Model_LSTM1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e146b-9f2a-4822-8d7e-c71ee74a6730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model = torch.load('e:\\\\Model_LSTM1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602f9272-9fd8-40a2-ae45-f92f793bbe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Loss average:0.006330\n",
      "Prediction: -0.05\n",
      "Actual:     0.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 用模型预测数据\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_batch_count = test_x.shape[0]\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "actual_line=[]\n",
    "pred_line=[]\n",
    "\n",
    "for step in range(test_batch_count):\n",
    "    pred, hn, cn = model(test_x[step], h0, c0)\n",
    "    \n",
    "    h0, c0 = hn.detach(), cn.detach()\n",
    "\n",
    "    loss = loss_func(pred[:,-1], test_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "    \n",
    "    test_loss += loss.cpu()\n",
    "    \n",
    "    actual_line.append(test_y[step][-1,-1].item())\n",
    "    pred_line.append(pred[-1,-1].item())\n",
    "        \n",
    "print(\"Prediction Loss average:{:.6f}\".format(test_loss.data/(step+1)))\n",
    "print(\"Prediction: {:.2f}\".format(float(pred[-1,-1].data)))\n",
    "print(\"Actual:     {:.2f}\".format(float(test_y[step][-1,-1].data)))\n",
    "\n",
    "# actual_line = test_y[step][-1].cpu().detach().flatten().numpy()        # Only plot the last sequence of test batch\n",
    "# pred_line   = pred[-1].cpu().detach().flatten().numpy()                # Only plot the last sequence of test batch\n",
    "plt.plot(actual_line, 'r--')\n",
    "plt.plot(pred_line, 'b-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9631a87-28ac-4b26-9b2b-24d900ffec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面Train除了最后一个Sequence，再用最后一个Sequence来预测\n",
    "\n",
    "# 加载数据\n",
    "dataset = pd.read_csv(\"601229.csv\", index_col=0)\n",
    "dataset = dataset.drop(['date'], axis=1)\n",
    "\n",
    "dataset = dataset.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# 将数据按照BATCH_SIZE的窗口进行滑动，每个窗口数据做一组\n",
    "# # 数据转成sequence的格式，这里定义每个seq的长度\n",
    "SEQ_LENGTH = 60\n",
    "TRAIN_BATCH_SIZE = 599                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_SIZE = 1                                                        # 注意：BATCH_SIZE是要能够整除(total_seq_count-1)的\n",
    "TEST_BATCH_COUNT = 1\n",
    "Y_SEQ_LEN = 1                                                         # 要用2个y来表示预测的第一天和预测的第二天，对应 \"future\" 和 \"future2\",每个y都是1-D的，y的seq_len是2\n",
    "Y_DIM = 1\n",
    "X_DIM = dataset.shape[1]-Y_SEQ_LEN                                    # 表示输入的sequence里每个element有122维度，也是encoder的input_dim\n",
    "\n",
    "# 把数据切换成 BATCH_SIZE 的一个个batch\n",
    "rolling_data = pd.DataFrame()\n",
    "for i in dataset.rolling(SEQ_LENGTH):\n",
    "    if i.shape[0] == SEQ_LENGTH:\n",
    "        rolling_data = rolling_data.append(i)\n",
    "\n",
    "rolling_data = rolling_data.values.reshape(-1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                   # 数据一共是 seq_count x seq_len x (x_in_dim+Y_SEQ_LEN) \n",
    "\n",
    "test_seq_count = TEST_BATCH_COUNT * TEST_BATCH_SIZE\n",
    "\n",
    "train = rolling_data[:-test_seq_count].reshape(1, -1, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)                    # 把数据转成 tain_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "test  = rolling_data[-test_seq_count:].reshape(-1, TEST_BATCH_SIZE, SEQ_LENGTH, X_DIM+Y_SEQ_LEN)      # 把数据转成 test_batch_count x BATCH_SIZE x seq_len x in_dim 格式\n",
    "\n",
    "TRAIN_BATCH_SIZE = train.shape[1]\n",
    "TRAIN_BATCH_COUNT = train.shape[0]\n",
    "TEST_BATCH_SIZE = test.shape[1]\n",
    "TEST_BATCH_COUNT = test.shape[0]\n",
    "\n",
    "train = torch.tensor(train)\n",
    "test  = torch.tensor(test)\n",
    "\n",
    "\n",
    "train_x, train_y = train[:,:,:,Y_SEQ_LEN:], train[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "test_x,  test_y  = test[:,:,:, Y_SEQ_LEN:],  test[:,:,-1:,0:Y_SEQ_LEN]           # [train_batch_count, batch_size, sequence_length, XorY dimission]\n",
    "\n",
    "train_y = train_y.permute(0, 1, 3, 2)                                    # conver from [train_batch_count, batch_size, seq_length, y_seq_len]  to [train_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "test_y  =  test_y.permute(0, 1, 3, 2)                                    # conver from [test_batch_count, batch_size, seq_length, y_seq_len]  to  [test_batch_count, batch_size, y_seq_len, 1-dim]\n",
    "\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "\n",
    "\n",
    "model = LSTMModel(input_size=X_DIM, hidden_layer_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1)\n",
    "model = model.double().to(device)\n",
    "LR = 1e-5\n",
    "loss_func = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "epoches = 200\n",
    "epoch_loss = 0\n",
    "epoch_loss_list = []\n",
    "train_batch_count = train_x.shape[0]\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, 599, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, 599, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for step in range(train_batch_count):\n",
    "        pred, hn, cn = model(train_x[step], h0, c0)\n",
    "        # h0, c0 = hn.detach(), cn.detach()\n",
    "        loss = loss_func(pred[:,-1], train_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data.cpu()\n",
    "        \n",
    "    if epoch_loss.item() < 1e-4:\n",
    "        print('Epoch [{}/{}], Loss: {:.5f}'.format(epoch+1, epoches, loss.item()))\n",
    "        print(\"The loss value is reached\")\n",
    "        break\n",
    "\n",
    "    print(\"{} of {} epoch loss: {:.4f} with lr: {}\".format(epoch, epoches, epoch_loss.item(), optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "\n",
    "    \n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_batch_count = test_x.shape[0]\n",
    "\n",
    "h0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "c0 = torch.zeros(NUM_LAYERS, TEST_BATCH_SIZE, HIDDEN_SIZE).double().to(device)\n",
    "\n",
    "actual_line=[]\n",
    "pred_line=[]\n",
    "\n",
    "for step in range(test_batch_count):\n",
    "    pred, hn, cn = model(test_x[step], h0, c0)\n",
    "    \n",
    "    h0, c0 = hn.detach(), cn.detach()\n",
    "\n",
    "    loss = loss_func(pred[:,-1], test_y[step][:,-1])                # Compare the all sequences' last element in one batch\n",
    "    \n",
    "    test_loss += loss.cpu()\n",
    "    \n",
    "    actual_line.append(test_y[step][-1,-1].item())\n",
    "    pred_line.append(pred[-1,-1].item())\n",
    "        \n",
    "print(\"Prediction Loss average:{:.6f}\".format(test_loss.data/(step+1)))\n",
    "print(\"Prediction: {:.2f}\".format(float(pred[-1,-1].data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3c66f-e2ca-4818-9856-1c396c9319c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
